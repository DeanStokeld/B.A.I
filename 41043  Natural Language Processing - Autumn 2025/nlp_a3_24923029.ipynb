{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Multilingual Content Moderation Pipeline"
      ],
      "metadata": {
        "id": "Qno5AU_xN-w3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://i.imgur.com/KNiZaxC.jpeg\" width=\"1500\">\n",
        "\n",
        "*Image generated using AI*"
      ],
      "metadata": {
        "id": "51gHb-RnOrCg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This project focuses on building a robust pipeline for **detecting offensive content in multilingual tweets**, particularly in scenarios where users **mix languages** (e.g. English and Hindi) within the same text ‚Äî a phenomenon common in social media platforms.\n",
        "\n",
        "The pipeline includes:\n",
        "- **Preprocessing**: Text cleaning, hashtag expansion, and back-translation for data augmentation\n",
        "- **Modelling**: Fine-tuning and evaluating multilingual transformer models\n",
        "\n",
        "### Models Compared\n",
        "- `xlm-roberta-base`\n",
        "- `distilbert-base-multilingual-cased`\n",
        "- `ai4bharat/indic-bert`\n",
        "\n",
        "The aim is to evaluate how well these models handle **code-mixed language** and **low-resource multilingual text** in the context of **offensive language detection**.\n",
        "\n",
        "**Data Source**: [HASOC 2021 - Offensive Language Identification](https://github.com/Kalit31/HASOC-2021/tree/main)"
      ],
      "metadata": {
        "id": "m4MFcR3HN-IV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive so we can access files stored there\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ak6rpUmBcZIh",
        "outputId": "37c52ed3-5c7b-4525-e70f-325203b5abd6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6hZZX-ccF-g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1fb84db-6021-48b0-ac94-d720eddbadba"
      },
      "source": [
        "# Install Google Cloud Translate client library\n",
        "!pip install google-cloud-translate -q\n",
        "\n",
        "# Install transformers, datasets, huggingface and optuna\n",
        "!pip install transformers datasets huggingface_hub optuna -q\n",
        "\n",
        "# Install contractions library to convert words from \"don't\" to \"do not\"\n",
        "!pip install contractions -q\n",
        "\n",
        "# Set the environment variable to authenticate with Google Cloud Translate (deleting key after assignment submission so don't even think about it üò° )\n",
        "import os\n",
        "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '/content/gdrive/Shareddrives/NLP Assignment 3/data/sinuous-bedrock-459007-c0-0d066719d88a.json'\n",
        "\n",
        "# Translation client\n",
        "from google.cloud import translate_v2 as translate\n",
        "\n",
        "# Data handling, modeling, and evaluation\n",
        "import re\n",
        "import time\n",
        "import html\n",
        "import random\n",
        "import shutil\n",
        "import contractions\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display\n",
        "\n",
        "# NLP utils and eval\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    precision_recall_fscore_support,\n",
        "    classification_report,\n",
        "    confusion_matrix,\n",
        "    ConfusionMatrixDisplay\n",
        ")\n",
        "\n",
        "# PyTorch\n",
        "import torch\n",
        "from torch.nn import CrossEntropyLoss\n",
        "\n",
        "# HuggingFace\n",
        "from datasets import Dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    DataCollatorWithPadding,\n",
        "    EarlyStoppingCallback\n",
        ")\n",
        "\n",
        "# Optuna\n",
        "import optuna\n",
        "\n",
        "# Start the Google Translate API client\n",
        "translate_client = translate.Client()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/289.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[90m‚ï∫\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m256.0/289.9 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/118.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m118.3/118.3 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading and proprocessing data"
      ],
      "metadata": {
        "id": "DBWuo639Sm3V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load and Preview English and Hindi Datasets\n",
        "\n",
        "This code loads two CSV files containing English and Hindi datasets using `pandas`, and combines the row counts to report the total number of data instances available. It then previews the first few rows of each dataset using `display()` to ensure the data has been loaded correctly.\n"
      ],
      "metadata": {
        "id": "QfWL4CywTWwS"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "U6xrWNnQcF-h",
        "outputId": "23c4787b-aada-49aa-c3de-2038e4f6e084"
      },
      "source": [
        "# Paths to CSV data files\n",
        "eng_data = \"/content/gdrive/Shareddrives/NLP Assignment 3/data/english_2021.csv\"\n",
        "hin_data = \"/content/gdrive/Shareddrives/NLP Assignment 3/data/hindi_2021.csv\"\n",
        "\n",
        "# Load datasets into pandas DataFrames\n",
        "eng_df = pd.read_csv(eng_data)\n",
        "hin_df = pd.read_csv(hin_data)\n",
        "\n",
        "# Counting number of rows of data available\n",
        "hin_data_count = len(hin_df)\n",
        "eng_data_count = len(eng_df)\n",
        "data_count = len(eng_df) + len(hin_df)\n",
        "print(f\"Loaded {data_count} rows of data. English data rows: {eng_data_count}. Hindi data rows: {hin_data_count}\")\n",
        "print(\"\\n\")\n",
        "\n",
        "# Count labels (HOF/NOT)\n",
        "hof_count = (hin_df['task_1'] == 'HOF').sum() + (eng_df['task_1'] == 'HOF').sum()\n",
        "not_count = (hin_df['task_1'] == 'NOT').sum() + (eng_df['task_1'] == 'NOT').sum()\n",
        "print(f\"HOF (Hateful or Offensive) count: {hof_count}\")\n",
        "print(f\"NOT (Not HOF) count: {not_count}\")\n",
        "print(\"\\n\")\n",
        "\n",
        "# Check for missing values in both datasets\n",
        "missing_hin = hin_df.isnull().sum()\n",
        "missing_eng = eng_df.isnull().sum()\n",
        "print(\"Missing values in Hindi dataset:\\\\n\", missing_hin)\n",
        "print(\"\\\\nMissing values in English dataset:\\\\n\", missing_eng)\n",
        "print(\"\\n\")\n",
        "print(\"Unique labels in Hindi:\", hin_df['task_1'].unique())\n",
        "print(\"Unique labels in English:\", eng_df['task_1'].unique())\n",
        "print(\"\\n\")\n",
        "print(\"Empty/whitespace rows in Hindi:\", hin_df['text'].str.strip().eq(\"\").sum())\n",
        "print(\"Empty/whitespace rows in English:\", eng_df['text'].str.strip().eq(\"\").sum())\n",
        "print(\"\\n\")\n",
        "print(\"Duplicate rows in Hindi:\", hin_df.duplicated().sum())\n",
        "print(\"Duplicate rows in English:\", eng_df.duplicated().sum())\n",
        "print(\"\\n\")\n",
        "print(\"Non-string values in Hindi:\", hin_df[~hin_df['text'].apply(lambda x: isinstance(x, str))].shape[0])\n",
        "print(\"Non-string values in English:\", eng_df[~eng_df['text'].apply(lambda x: isinstance(x, str))].shape[0])\n",
        "print(\"\\n\")\n",
        "\n",
        "# Quick preview of each dataset\n",
        "display(eng_df.head())\n",
        "display(hin_df.head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 8437 rows of data. English data rows: 3843. Hindi data rows: 4594\n",
            "\n",
            "\n",
            "HOF (Hateful or Offensive) count: 3934\n",
            "NOT (Not HOF) count: 4503\n",
            "\n",
            "\n",
            "Missing values in Hindi dataset:\\n Unnamed: 0    0\n",
            "_id           0\n",
            "tweet_id      0\n",
            "text          0\n",
            "task_1        0\n",
            "task_2        0\n",
            "dtype: int64\n",
            "\\nMissing values in English dataset:\\n Unnamed: 0    0\n",
            "_id           0\n",
            "text          0\n",
            "task_1        0\n",
            "task_2        0\n",
            "dtype: int64\n",
            "\n",
            "\n",
            "Unique labels in Hindi: ['NOT' 'HOF']\n",
            "Unique labels in English: ['HOF' 'NOT']\n",
            "\n",
            "\n",
            "Empty/whitespace rows in Hindi: 0\n",
            "Empty/whitespace rows in English: 0\n",
            "\n",
            "\n",
            "Duplicate rows in Hindi: 0\n",
            "Duplicate rows in English: 0\n",
            "\n",
            "\n",
            "Non-string values in Hindi: 0\n",
            "Non-string values in English: 0\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   Unnamed: 0                       _id  \\\n",
              "0        4986  60c5d6bf5659ea5e55defa2c   \n",
              "1        3394  60c5d6bf5659ea5e55def461   \n",
              "2        1310  60c5d6bf5659ea5e55defaad   \n",
              "3        3390  60c5d6bf5659ea5e55def419   \n",
              "4        4626  60c5d6bf5659ea5e55def7fa   \n",
              "\n",
              "                                                text task_1 task_2  \n",
              "0  @wealth if you made it through this &amp;&amp;...    HOF   PRFN  \n",
              "1  Technically that's still turning back the cloc...    HOF   OFFN  \n",
              "2  @VMBJP @BJP4Bengal @BJP4India @narendramodi @J...    NOT   NONE  \n",
              "3  @krtoprak_yigit Soldier of Japan Who has dick ...    HOF   OFFN  \n",
              "4  @blueheartedly You'd be better off asking who ...    HOF   OFFN  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9d383bfe-5138-44c3-b375-29561eabef9e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>_id</th>\n",
              "      <th>text</th>\n",
              "      <th>task_1</th>\n",
              "      <th>task_2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4986</td>\n",
              "      <td>60c5d6bf5659ea5e55defa2c</td>\n",
              "      <td>@wealth if you made it through this &amp;amp;&amp;amp;...</td>\n",
              "      <td>HOF</td>\n",
              "      <td>PRFN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3394</td>\n",
              "      <td>60c5d6bf5659ea5e55def461</td>\n",
              "      <td>Technically that's still turning back the cloc...</td>\n",
              "      <td>HOF</td>\n",
              "      <td>OFFN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1310</td>\n",
              "      <td>60c5d6bf5659ea5e55defaad</td>\n",
              "      <td>@VMBJP @BJP4Bengal @BJP4India @narendramodi @J...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>NONE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3390</td>\n",
              "      <td>60c5d6bf5659ea5e55def419</td>\n",
              "      <td>@krtoprak_yigit Soldier of Japan Who has dick ...</td>\n",
              "      <td>HOF</td>\n",
              "      <td>OFFN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4626</td>\n",
              "      <td>60c5d6bf5659ea5e55def7fa</td>\n",
              "      <td>@blueheartedly You'd be better off asking who ...</td>\n",
              "      <td>HOF</td>\n",
              "      <td>OFFN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9d383bfe-5138-44c3-b375-29561eabef9e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-9d383bfe-5138-44c3-b375-29561eabef9e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-9d383bfe-5138-44c3-b375-29561eabef9e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-98bb3259-b605-4bc8-8724-bea5464b32f6\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-98bb3259-b605-4bc8-8724-bea5464b32f6')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-98bb3259-b605-4bc8-8724-bea5464b32f6 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"display(hin_df\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"Unnamed: 0\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1439,\n        \"min\": 1310,\n        \"max\": 4986,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          3394,\n          4626,\n          1310\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"60c5d6bf5659ea5e55def461\",\n          \"60c5d6bf5659ea5e55def7fa\",\n          \"60c5d6bf5659ea5e55defaad\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Technically that's still turning back the clock, dick head https://t.co/jbKaPJmpt1\",\n          \"@blueheartedly You'd be better off asking who DOESN'T think he's a sleazy shitbag lmao.\",\n          \"@VMBJP @BJP4Bengal @BJP4India @narendramodi @JPNadda @AmitShah @DilipGhoshBJP @RahulSinhaBJP And you're the govt?!?! Stop thinking about world media, liberal gangs or any optics whatsoever and ACT NOW already.  If this is what a person at your level is facing then shudder to think the plight of common people in Bengal. #BengalBurning\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"task_1\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"NOT\",\n          \"HOF\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"task_2\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"PRFN\",\n          \"OFFN\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   Unnamed: 0                       _id            tweet_id  \\\n",
              "0         998  60c5d7495659ea5e55df0b7b   hi_hasoc_2021_998   \n",
              "1        4049  60c5d7495659ea5e55df1b73  hi_hasoc_2021_4049   \n",
              "2        1757  60c5d7495659ea5e55df0d1b  hi_hasoc_2021_1757   \n",
              "3        5175  60c5d7495659ea5e55df0e45  hi_hasoc_2021_5178   \n",
              "4        1825  60c5d7495659ea5e55df0ee1  hi_hasoc_2021_1825   \n",
              "\n",
              "                                                text task_1 task_2  \n",
              "0  @rssurjewala #Hindus DYING #HindusLivesMatter ...    NOT   NONE  \n",
              "1  ‡§∏‡§¨ ‡§≤‡•ã‡§ó ‡§á‡§§‡§®‡•á ‡§™‡•à‡§∏‡•á ‡§°‡•ã‡§®‡•á‡§ü ‡§π‡•Ä ‡§ï‡§∞ ‡§∞‡§π‡•á ‡§π‡•à‡§Ç ‡§´‡§ø‡§∞ ‡§≠‡•Ä ‡§Ü‡§Æ...    NOT   NONE  \n",
              "2  ‡§∂‡•á‡§∞-‡§è- ‡§∏‡§ø‡§µ‡§æ‡§® ‡§∂‡§π‡§æ‡§¨‡•Å‡§¶‡•ç‡§¶‡•Ä‡§® ‡§∏‡§æ‡§π‡§¨ ‡§∏‡•á ‡§∞‡§ø‡§∂‡•ç‡§§‡§æ ‡§ï‡•ç‡§Ø‡§æ  ŸÑ...    NOT   NONE  \n",
              "3         @AskAnshul ‡§Ü‡§∏‡§Æ‡§æ‡§®‡•Ä ‡§ï‡§ø‡§§‡§æ‡§¨ ‡§ï‡•á ‡§®‡§æ‡§ú‡§æ‡§Ø‡§ú ‡§î‡§≤‡§æ‡§¶ ‡§π‡•à‡•§    HOF   OFFN  \n",
              "4  @Shikha0222 ‡§á‡§∏‡•á ‡§ï‡§π‡§§‡•á ‡§π‡•à‡§Ç ‡§¶‡•ã‡§ó‡§≤‡§æ ‡§™‡§Ç‡§§‡•Ä ‡§ú‡§ø‡§∏ ‡§∏‡§™‡§æ ‡§ï‡•Ä...    NOT   NONE  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d2d7f010-0474-429f-9216-ecd946707e18\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>_id</th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>text</th>\n",
              "      <th>task_1</th>\n",
              "      <th>task_2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>998</td>\n",
              "      <td>60c5d7495659ea5e55df0b7b</td>\n",
              "      <td>hi_hasoc_2021_998</td>\n",
              "      <td>@rssurjewala #Hindus DYING #HindusLivesMatter ...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>NONE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4049</td>\n",
              "      <td>60c5d7495659ea5e55df1b73</td>\n",
              "      <td>hi_hasoc_2021_4049</td>\n",
              "      <td>‡§∏‡§¨ ‡§≤‡•ã‡§ó ‡§á‡§§‡§®‡•á ‡§™‡•à‡§∏‡•á ‡§°‡•ã‡§®‡•á‡§ü ‡§π‡•Ä ‡§ï‡§∞ ‡§∞‡§π‡•á ‡§π‡•à‡§Ç ‡§´‡§ø‡§∞ ‡§≠‡•Ä ‡§Ü‡§Æ...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>NONE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1757</td>\n",
              "      <td>60c5d7495659ea5e55df0d1b</td>\n",
              "      <td>hi_hasoc_2021_1757</td>\n",
              "      <td>‡§∂‡•á‡§∞-‡§è- ‡§∏‡§ø‡§µ‡§æ‡§® ‡§∂‡§π‡§æ‡§¨‡•Å‡§¶‡•ç‡§¶‡•Ä‡§® ‡§∏‡§æ‡§π‡§¨ ‡§∏‡•á ‡§∞‡§ø‡§∂‡•ç‡§§‡§æ ‡§ï‡•ç‡§Ø‡§æ  ŸÑ...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>NONE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5175</td>\n",
              "      <td>60c5d7495659ea5e55df0e45</td>\n",
              "      <td>hi_hasoc_2021_5178</td>\n",
              "      <td>@AskAnshul ‡§Ü‡§∏‡§Æ‡§æ‡§®‡•Ä ‡§ï‡§ø‡§§‡§æ‡§¨ ‡§ï‡•á ‡§®‡§æ‡§ú‡§æ‡§Ø‡§ú ‡§î‡§≤‡§æ‡§¶ ‡§π‡•à‡•§</td>\n",
              "      <td>HOF</td>\n",
              "      <td>OFFN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1825</td>\n",
              "      <td>60c5d7495659ea5e55df0ee1</td>\n",
              "      <td>hi_hasoc_2021_1825</td>\n",
              "      <td>@Shikha0222 ‡§á‡§∏‡•á ‡§ï‡§π‡§§‡•á ‡§π‡•à‡§Ç ‡§¶‡•ã‡§ó‡§≤‡§æ ‡§™‡§Ç‡§§‡•Ä ‡§ú‡§ø‡§∏ ‡§∏‡§™‡§æ ‡§ï‡•Ä...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>NONE</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d2d7f010-0474-429f-9216-ecd946707e18')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-d2d7f010-0474-429f-9216-ecd946707e18 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-d2d7f010-0474-429f-9216-ecd946707e18');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-67c08c6d-3b1e-4f37-96e1-d8b9a73d07b3\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-67c08c6d-3b1e-4f37-96e1-d8b9a73d07b3')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-67c08c6d-3b1e-4f37-96e1-d8b9a73d07b3 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"display(hin_df\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"Unnamed: 0\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1766,\n        \"min\": 998,\n        \"max\": 5175,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          4049,\n          1825,\n          1757\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"60c5d7495659ea5e55df1b73\",\n          \"60c5d7495659ea5e55df0ee1\",\n          \"60c5d7495659ea5e55df0d1b\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"tweet_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"hi_hasoc_2021_4049\",\n          \"hi_hasoc_2021_1825\",\n          \"hi_hasoc_2021_1757\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"\\u0938\\u092c \\u0932\\u094b\\u0917 \\u0907\\u0924\\u0928\\u0947 \\u092a\\u0948\\u0938\\u0947 \\u0921\\u094b\\u0928\\u0947\\u091f \\u0939\\u0940 \\u0915\\u0930 \\u0930\\u0939\\u0947 \\u0939\\u0948\\u0902 \\u092b\\u093f\\u0930 \\u092d\\u0940 \\u0906\\u092e \\u0906\\u0926\\u092e\\u0940 \\u0915\\u094b \\u0938\\u093f\\u0932\\u0947\\u0902\\u0921\\u0930 \\u0914\\u0930 \\u0915\\u0928\\u094d\\u0938\\u0947\\u0902\\u091f\\u094d\\u0930\\u0947\\u091f\\u0930 \\u0916\\u093c\\u0941\\u0926 \\u0938\\u0947 \\u0939\\u0940 \\u0916\\u093c\\u0930\\u0940\\u0926\\u0928\\u093e \\u092a\\u0921\\u093c \\u0930\\u0939\\u093e \\u0939\\u0948 , \\u0924\\u094b \\u092a\\u0948\\u0938\\u0947 \\u0915\\u0939\\u093e\\u0901 \\u091c\\u093e \\u0930\\u0939\\u0947 \\u0939\\u0948\\u0902?   \\u092c\\u0940\\u092f\\u0930\\u094d\\u0921 \\u0911\\u092f\\u0932 \\u092e\\u0947\\u0902 ?   #IndiaFightsCorona  #indianeedoxygen  #IndiaCovidCrisis\",\n          \"@Shikha0222 \\u0907\\u0938\\u0947 \\u0915\\u0939\\u0924\\u0947 \\u0939\\u0948\\u0902 \\u0926\\u094b\\u0917\\u0932\\u093e \\u092a\\u0902\\u0924\\u0940 \\u091c\\u093f\\u0938 \\u0938\\u092a\\u093e \\u0915\\u0940 \\u0926\\u092e \\u092a\\u0930 0 \\u0938\\u0947 10 \\u0915\\u093e \\u0938\\u092b\\u0930 \\u0915\\u093f\\u092f\\u093e \\u0914\\u0930 \\u092e\\u093e\\u092f\\u093e \\u0905\\u092a\\u0928\\u0940 \\u0907\\u091c\\u094d\\u091c\\u093c\\u0924 \\u092c\\u091a\\u093e \\u092a\\u093e\\u0908,, \\u0906\\u091c \\u0909\\u0938\\u0940 \\u0938\\u092a\\u093e \\u0915\\u094b \\u092e\\u093f\\u091f\\u093e\\u0928\\u0947 \\u0915\\u0940 \\u092c\\u093e\\u0924 \\u0915\\u0930 \\u0930\\u0939\\u0940 \\u0939\\u0948\\u0902,,,\",\n          \"\\u0936\\u0947\\u0930-\\u090f- \\u0938\\u093f\\u0935\\u093e\\u0928 \\u0936\\u0939\\u093e\\u092c\\u0941\\u0926\\u094d\\u0926\\u0940\\u0928 \\u0938\\u093e\\u0939\\u092c \\u0938\\u0947 \\u0930\\u093f\\u0936\\u094d\\u0924\\u093e \\u0915\\u094d\\u092f\\u093e  \\u0644\\u0627 \\u0625\\u0644\\u0647 \\u0625\\u0644\\u0627 \\u0627\\u0644\\u0644\\u0647 \\u0645\\u062d\\u0645\\u062f \\u0631\\u0633\\u0648\\u0644 \\u0627\\u0644\\u0644\\u0647  #JusticeForShahabuddin\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"task_1\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"HOF\",\n          \"NOT\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"task_2\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"OFFN\",\n          \"NONE\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Clean Dataset by Dropping Irrelevant Columns\n",
        "\n",
        "This code removes columns that are not useful for training the model from both English and Hindi datasets. These include identifiers like `_id`, `tweet_id`, and metadata such as `Unnamed: 0` or `task_2`, which are redundant to the classification task.\n"
      ],
      "metadata": {
        "id": "opGOHD4TTZ8V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove unnecessary columns that aren't relevant for training\n",
        "eng_df.drop(columns=['Unnamed: 0', '_id', 'task_2'], inplace=True)\n",
        "hin_df.drop(columns=['Unnamed: 0', '_id', 'tweet_id', 'task_2'], inplace=True)\n",
        "\n",
        "# Preview after cleanup\n",
        "display(eng_df.head())\n",
        "display(hin_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "id": "VOgCR17Dm8mE",
        "outputId": "17bef141-e944-46df-dfc2-5336d41f9caa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                                text task_1\n",
              "0  @wealth if you made it through this &amp;&amp;...    HOF\n",
              "1  Technically that's still turning back the cloc...    HOF\n",
              "2  @VMBJP @BJP4Bengal @BJP4India @narendramodi @J...    NOT\n",
              "3  @krtoprak_yigit Soldier of Japan Who has dick ...    HOF\n",
              "4  @blueheartedly You'd be better off asking who ...    HOF"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-bf5746f6-06b7-4f4a-9740-c6c87c68637d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>task_1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>@wealth if you made it through this &amp;amp;&amp;amp;...</td>\n",
              "      <td>HOF</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Technically that's still turning back the cloc...</td>\n",
              "      <td>HOF</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>@VMBJP @BJP4Bengal @BJP4India @narendramodi @J...</td>\n",
              "      <td>NOT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>@krtoprak_yigit Soldier of Japan Who has dick ...</td>\n",
              "      <td>HOF</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>@blueheartedly You'd be better off asking who ...</td>\n",
              "      <td>HOF</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bf5746f6-06b7-4f4a-9740-c6c87c68637d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-bf5746f6-06b7-4f4a-9740-c6c87c68637d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-bf5746f6-06b7-4f4a-9740-c6c87c68637d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-76a190f2-45c6-4b5e-976c-6e481a80e198\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-76a190f2-45c6-4b5e-976c-6e481a80e198')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-76a190f2-45c6-4b5e-976c-6e481a80e198 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"display(hin_df\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Technically that's still turning back the clock, dick head https://t.co/jbKaPJmpt1\",\n          \"@blueheartedly You'd be better off asking who DOESN'T think he's a sleazy shitbag lmao.\",\n          \"@VMBJP @BJP4Bengal @BJP4India @narendramodi @JPNadda @AmitShah @DilipGhoshBJP @RahulSinhaBJP And you're the govt?!?! Stop thinking about world media, liberal gangs or any optics whatsoever and ACT NOW already.  If this is what a person at your level is facing then shudder to think the plight of common people in Bengal. #BengalBurning\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"task_1\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"NOT\",\n          \"HOF\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                                text task_1\n",
              "0  @rssurjewala #Hindus DYING #HindusLivesMatter ...    NOT\n",
              "1  ‡§∏‡§¨ ‡§≤‡•ã‡§ó ‡§á‡§§‡§®‡•á ‡§™‡•à‡§∏‡•á ‡§°‡•ã‡§®‡•á‡§ü ‡§π‡•Ä ‡§ï‡§∞ ‡§∞‡§π‡•á ‡§π‡•à‡§Ç ‡§´‡§ø‡§∞ ‡§≠‡•Ä ‡§Ü‡§Æ...    NOT\n",
              "2  ‡§∂‡•á‡§∞-‡§è- ‡§∏‡§ø‡§µ‡§æ‡§® ‡§∂‡§π‡§æ‡§¨‡•Å‡§¶‡•ç‡§¶‡•Ä‡§® ‡§∏‡§æ‡§π‡§¨ ‡§∏‡•á ‡§∞‡§ø‡§∂‡•ç‡§§‡§æ ‡§ï‡•ç‡§Ø‡§æ  ŸÑ...    NOT\n",
              "3         @AskAnshul ‡§Ü‡§∏‡§Æ‡§æ‡§®‡•Ä ‡§ï‡§ø‡§§‡§æ‡§¨ ‡§ï‡•á ‡§®‡§æ‡§ú‡§æ‡§Ø‡§ú ‡§î‡§≤‡§æ‡§¶ ‡§π‡•à‡•§    HOF\n",
              "4  @Shikha0222 ‡§á‡§∏‡•á ‡§ï‡§π‡§§‡•á ‡§π‡•à‡§Ç ‡§¶‡•ã‡§ó‡§≤‡§æ ‡§™‡§Ç‡§§‡•Ä ‡§ú‡§ø‡§∏ ‡§∏‡§™‡§æ ‡§ï‡•Ä...    NOT"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7831aaec-5f1a-4f7c-a999-b293b92c924e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>task_1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>@rssurjewala #Hindus DYING #HindusLivesMatter ...</td>\n",
              "      <td>NOT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>‡§∏‡§¨ ‡§≤‡•ã‡§ó ‡§á‡§§‡§®‡•á ‡§™‡•à‡§∏‡•á ‡§°‡•ã‡§®‡•á‡§ü ‡§π‡•Ä ‡§ï‡§∞ ‡§∞‡§π‡•á ‡§π‡•à‡§Ç ‡§´‡§ø‡§∞ ‡§≠‡•Ä ‡§Ü‡§Æ...</td>\n",
              "      <td>NOT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>‡§∂‡•á‡§∞-‡§è- ‡§∏‡§ø‡§µ‡§æ‡§® ‡§∂‡§π‡§æ‡§¨‡•Å‡§¶‡•ç‡§¶‡•Ä‡§® ‡§∏‡§æ‡§π‡§¨ ‡§∏‡•á ‡§∞‡§ø‡§∂‡•ç‡§§‡§æ ‡§ï‡•ç‡§Ø‡§æ  ŸÑ...</td>\n",
              "      <td>NOT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>@AskAnshul ‡§Ü‡§∏‡§Æ‡§æ‡§®‡•Ä ‡§ï‡§ø‡§§‡§æ‡§¨ ‡§ï‡•á ‡§®‡§æ‡§ú‡§æ‡§Ø‡§ú ‡§î‡§≤‡§æ‡§¶ ‡§π‡•à‡•§</td>\n",
              "      <td>HOF</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>@Shikha0222 ‡§á‡§∏‡•á ‡§ï‡§π‡§§‡•á ‡§π‡•à‡§Ç ‡§¶‡•ã‡§ó‡§≤‡§æ ‡§™‡§Ç‡§§‡•Ä ‡§ú‡§ø‡§∏ ‡§∏‡§™‡§æ ‡§ï‡•Ä...</td>\n",
              "      <td>NOT</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7831aaec-5f1a-4f7c-a999-b293b92c924e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-7831aaec-5f1a-4f7c-a999-b293b92c924e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-7831aaec-5f1a-4f7c-a999-b293b92c924e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-5b0f442a-cda8-4d36-81ef-16c5b2e25ccd\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-5b0f442a-cda8-4d36-81ef-16c5b2e25ccd')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-5b0f442a-cda8-4d36-81ef-16c5b2e25ccd button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"display(hin_df\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"\\u0938\\u092c \\u0932\\u094b\\u0917 \\u0907\\u0924\\u0928\\u0947 \\u092a\\u0948\\u0938\\u0947 \\u0921\\u094b\\u0928\\u0947\\u091f \\u0939\\u0940 \\u0915\\u0930 \\u0930\\u0939\\u0947 \\u0939\\u0948\\u0902 \\u092b\\u093f\\u0930 \\u092d\\u0940 \\u0906\\u092e \\u0906\\u0926\\u092e\\u0940 \\u0915\\u094b \\u0938\\u093f\\u0932\\u0947\\u0902\\u0921\\u0930 \\u0914\\u0930 \\u0915\\u0928\\u094d\\u0938\\u0947\\u0902\\u091f\\u094d\\u0930\\u0947\\u091f\\u0930 \\u0916\\u093c\\u0941\\u0926 \\u0938\\u0947 \\u0939\\u0940 \\u0916\\u093c\\u0930\\u0940\\u0926\\u0928\\u093e \\u092a\\u0921\\u093c \\u0930\\u0939\\u093e \\u0939\\u0948 , \\u0924\\u094b \\u092a\\u0948\\u0938\\u0947 \\u0915\\u0939\\u093e\\u0901 \\u091c\\u093e \\u0930\\u0939\\u0947 \\u0939\\u0948\\u0902?   \\u092c\\u0940\\u092f\\u0930\\u094d\\u0921 \\u0911\\u092f\\u0932 \\u092e\\u0947\\u0902 ?   #IndiaFightsCorona  #indianeedoxygen  #IndiaCovidCrisis\",\n          \"@Shikha0222 \\u0907\\u0938\\u0947 \\u0915\\u0939\\u0924\\u0947 \\u0939\\u0948\\u0902 \\u0926\\u094b\\u0917\\u0932\\u093e \\u092a\\u0902\\u0924\\u0940 \\u091c\\u093f\\u0938 \\u0938\\u092a\\u093e \\u0915\\u0940 \\u0926\\u092e \\u092a\\u0930 0 \\u0938\\u0947 10 \\u0915\\u093e \\u0938\\u092b\\u0930 \\u0915\\u093f\\u092f\\u093e \\u0914\\u0930 \\u092e\\u093e\\u092f\\u093e \\u0905\\u092a\\u0928\\u0940 \\u0907\\u091c\\u094d\\u091c\\u093c\\u0924 \\u092c\\u091a\\u093e \\u092a\\u093e\\u0908,, \\u0906\\u091c \\u0909\\u0938\\u0940 \\u0938\\u092a\\u093e \\u0915\\u094b \\u092e\\u093f\\u091f\\u093e\\u0928\\u0947 \\u0915\\u0940 \\u092c\\u093e\\u0924 \\u0915\\u0930 \\u0930\\u0939\\u0940 \\u0939\\u0948\\u0902,,,\",\n          \"\\u0936\\u0947\\u0930-\\u090f- \\u0938\\u093f\\u0935\\u093e\\u0928 \\u0936\\u0939\\u093e\\u092c\\u0941\\u0926\\u094d\\u0926\\u0940\\u0928 \\u0938\\u093e\\u0939\\u092c \\u0938\\u0947 \\u0930\\u093f\\u0936\\u094d\\u0924\\u093e \\u0915\\u094d\\u092f\\u093e  \\u0644\\u0627 \\u0625\\u0644\\u0647 \\u0625\\u0644\\u0627 \\u0627\\u0644\\u0644\\u0647 \\u0645\\u062d\\u0645\\u062f \\u0631\\u0633\\u0648\\u0644 \\u0627\\u0644\\u0644\\u0647  #JusticeForShahabuddin\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"task_1\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"HOF\",\n          \"NOT\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Add Unique Identifiers and Standardise Language/Label Columns\n",
        "\n",
        "To ensure traceability and facilitate multilingual processing, this block adds a unique `id` to each row and assigns a new `lang` column indicating the language of the entry (`'en'` for English, `'hi'` for Hindi). The columns are then reordered to place `id` and `lang` first for consistency. The label column (`task_1`) is also renamed to `label` in both datasets, preparing the data for modelling.\n"
      ],
      "metadata": {
        "id": "inKHUcoqTdh-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add unique IDs and a language column to track language source\n",
        "eng_df['id'] = range(0, len(eng_df))\n",
        "hin_df['id'] = range(len(eng_df), len(eng_df) + len(hin_df))\n",
        "eng_df['lang'] = 'en'\n",
        "hin_df['lang'] = 'hi'\n",
        "\n",
        "# Reorder columns so 'id' and 'lang' appear first\n",
        "eng_df = eng_df[['id', 'lang'] + [col for col in eng_df.columns if col not in ['id', 'lang']]]\n",
        "hin_df = hin_df[['id', 'lang'] + [col for col in hin_df.columns if col not in ['id', 'lang']]]\n",
        "\n",
        "# Rename label column to something consistent\n",
        "eng_df.rename(columns={'task_1': 'label'}, inplace=True)\n",
        "hin_df.rename(columns={'task_1': 'label'}, inplace=True)\n",
        "\n",
        "# Check that the format is what we expect\n",
        "display(eng_df.head())\n",
        "display(hin_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "id": "GHlM79CdpSye",
        "outputId": "7fb330ff-3818-4daa-8b1b-77673bd8f7a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   id lang                                               text label\n",
              "0   0   en  @wealth if you made it through this &amp;&amp;...   HOF\n",
              "1   1   en  Technically that's still turning back the cloc...   HOF\n",
              "2   2   en  @VMBJP @BJP4Bengal @BJP4India @narendramodi @J...   NOT\n",
              "3   3   en  @krtoprak_yigit Soldier of Japan Who has dick ...   HOF\n",
              "4   4   en  @blueheartedly You'd be better off asking who ...   HOF"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c38bab5a-3f90-4a55-a368-8d65c6cb4265\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>lang</th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>en</td>\n",
              "      <td>@wealth if you made it through this &amp;amp;&amp;amp;...</td>\n",
              "      <td>HOF</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>en</td>\n",
              "      <td>Technically that's still turning back the cloc...</td>\n",
              "      <td>HOF</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>en</td>\n",
              "      <td>@VMBJP @BJP4Bengal @BJP4India @narendramodi @J...</td>\n",
              "      <td>NOT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>en</td>\n",
              "      <td>@krtoprak_yigit Soldier of Japan Who has dick ...</td>\n",
              "      <td>HOF</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>en</td>\n",
              "      <td>@blueheartedly You'd be better off asking who ...</td>\n",
              "      <td>HOF</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c38bab5a-3f90-4a55-a368-8d65c6cb4265')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c38bab5a-3f90-4a55-a368-8d65c6cb4265 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c38bab5a-3f90-4a55-a368-8d65c6cb4265');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-a89377dd-7275-4ee6-b4b9-d13563beea55\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a89377dd-7275-4ee6-b4b9-d13563beea55')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-a89377dd-7275-4ee6-b4b9-d13563beea55 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"display(hin_df\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 4,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          1,\n          4,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"lang\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"en\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Technically that's still turning back the clock, dick head https://t.co/jbKaPJmpt1\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"NOT\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "     id lang                                               text label\n",
              "0  3843   hi  @rssurjewala #Hindus DYING #HindusLivesMatter ...   NOT\n",
              "1  3844   hi  ‡§∏‡§¨ ‡§≤‡•ã‡§ó ‡§á‡§§‡§®‡•á ‡§™‡•à‡§∏‡•á ‡§°‡•ã‡§®‡•á‡§ü ‡§π‡•Ä ‡§ï‡§∞ ‡§∞‡§π‡•á ‡§π‡•à‡§Ç ‡§´‡§ø‡§∞ ‡§≠‡•Ä ‡§Ü‡§Æ...   NOT\n",
              "2  3845   hi  ‡§∂‡•á‡§∞-‡§è- ‡§∏‡§ø‡§µ‡§æ‡§® ‡§∂‡§π‡§æ‡§¨‡•Å‡§¶‡•ç‡§¶‡•Ä‡§® ‡§∏‡§æ‡§π‡§¨ ‡§∏‡•á ‡§∞‡§ø‡§∂‡•ç‡§§‡§æ ‡§ï‡•ç‡§Ø‡§æ  ŸÑ...   NOT\n",
              "3  3846   hi         @AskAnshul ‡§Ü‡§∏‡§Æ‡§æ‡§®‡•Ä ‡§ï‡§ø‡§§‡§æ‡§¨ ‡§ï‡•á ‡§®‡§æ‡§ú‡§æ‡§Ø‡§ú ‡§î‡§≤‡§æ‡§¶ ‡§π‡•à‡•§   HOF\n",
              "4  3847   hi  @Shikha0222 ‡§á‡§∏‡•á ‡§ï‡§π‡§§‡•á ‡§π‡•à‡§Ç ‡§¶‡•ã‡§ó‡§≤‡§æ ‡§™‡§Ç‡§§‡•Ä ‡§ú‡§ø‡§∏ ‡§∏‡§™‡§æ ‡§ï‡•Ä...   NOT"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a95eec45-ef8c-49bf-a6ed-3f50c0363393\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>lang</th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3843</td>\n",
              "      <td>hi</td>\n",
              "      <td>@rssurjewala #Hindus DYING #HindusLivesMatter ...</td>\n",
              "      <td>NOT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3844</td>\n",
              "      <td>hi</td>\n",
              "      <td>‡§∏‡§¨ ‡§≤‡•ã‡§ó ‡§á‡§§‡§®‡•á ‡§™‡•à‡§∏‡•á ‡§°‡•ã‡§®‡•á‡§ü ‡§π‡•Ä ‡§ï‡§∞ ‡§∞‡§π‡•á ‡§π‡•à‡§Ç ‡§´‡§ø‡§∞ ‡§≠‡•Ä ‡§Ü‡§Æ...</td>\n",
              "      <td>NOT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3845</td>\n",
              "      <td>hi</td>\n",
              "      <td>‡§∂‡•á‡§∞-‡§è- ‡§∏‡§ø‡§µ‡§æ‡§® ‡§∂‡§π‡§æ‡§¨‡•Å‡§¶‡•ç‡§¶‡•Ä‡§® ‡§∏‡§æ‡§π‡§¨ ‡§∏‡•á ‡§∞‡§ø‡§∂‡•ç‡§§‡§æ ‡§ï‡•ç‡§Ø‡§æ  ŸÑ...</td>\n",
              "      <td>NOT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3846</td>\n",
              "      <td>hi</td>\n",
              "      <td>@AskAnshul ‡§Ü‡§∏‡§Æ‡§æ‡§®‡•Ä ‡§ï‡§ø‡§§‡§æ‡§¨ ‡§ï‡•á ‡§®‡§æ‡§ú‡§æ‡§Ø‡§ú ‡§î‡§≤‡§æ‡§¶ ‡§π‡•à‡•§</td>\n",
              "      <td>HOF</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3847</td>\n",
              "      <td>hi</td>\n",
              "      <td>@Shikha0222 ‡§á‡§∏‡•á ‡§ï‡§π‡§§‡•á ‡§π‡•à‡§Ç ‡§¶‡•ã‡§ó‡§≤‡§æ ‡§™‡§Ç‡§§‡•Ä ‡§ú‡§ø‡§∏ ‡§∏‡§™‡§æ ‡§ï‡•Ä...</td>\n",
              "      <td>NOT</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a95eec45-ef8c-49bf-a6ed-3f50c0363393')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a95eec45-ef8c-49bf-a6ed-3f50c0363393 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a95eec45-ef8c-49bf-a6ed-3f50c0363393');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-4a0a16b3-614c-4677-857b-85113f23cecc\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-4a0a16b3-614c-4677-857b-85113f23cecc')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-4a0a16b3-614c-4677-857b-85113f23cecc button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"display(hin_df\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 3843,\n        \"max\": 3847,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          3844,\n          3847,\n          3845\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"lang\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"hi\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"\\u0938\\u092c \\u0932\\u094b\\u0917 \\u0907\\u0924\\u0928\\u0947 \\u092a\\u0948\\u0938\\u0947 \\u0921\\u094b\\u0928\\u0947\\u091f \\u0939\\u0940 \\u0915\\u0930 \\u0930\\u0939\\u0947 \\u0939\\u0948\\u0902 \\u092b\\u093f\\u0930 \\u092d\\u0940 \\u0906\\u092e \\u0906\\u0926\\u092e\\u0940 \\u0915\\u094b \\u0938\\u093f\\u0932\\u0947\\u0902\\u0921\\u0930 \\u0914\\u0930 \\u0915\\u0928\\u094d\\u0938\\u0947\\u0902\\u091f\\u094d\\u0930\\u0947\\u091f\\u0930 \\u0916\\u093c\\u0941\\u0926 \\u0938\\u0947 \\u0939\\u0940 \\u0916\\u093c\\u0930\\u0940\\u0926\\u0928\\u093e \\u092a\\u0921\\u093c \\u0930\\u0939\\u093e \\u0939\\u0948 , \\u0924\\u094b \\u092a\\u0948\\u0938\\u0947 \\u0915\\u0939\\u093e\\u0901 \\u091c\\u093e \\u0930\\u0939\\u0947 \\u0939\\u0948\\u0902?   \\u092c\\u0940\\u092f\\u0930\\u094d\\u0921 \\u0911\\u092f\\u0932 \\u092e\\u0947\\u0902 ?   #IndiaFightsCorona  #indianeedoxygen  #IndiaCovidCrisis\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"HOF\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocess Text: Clean Noise and Expand Hashtags\n",
        "\n",
        "This code cleans tweets from both English and Hindi datasets. It includes:\n",
        "\n",
        "- **`split_hashtag`**: Transforms hashtags into readable text by inserting spaces between camel case transitions, digits and letters, and replacing underscores  \n",
        "  (e.g. `#StopTheHate123` ‚Üí `Stop The Hate 123`).\n",
        "\n",
        "- **`clean_text`**:\n",
        "  - Decodes HTML entities (e.g. `&amp;` ‚Üí `&`)\n",
        "  - Expands contractions (e.g. `don't` ‚Üí `do not`)\n",
        "  - Collapses repeated special characters (e.g. `!!` ‚Üí `!`)\n",
        "  - Replaces ampersands (`&`) with the word \"and\"\n",
        "  - Removes URLs, user mentions (`@username`), and any non-ASCII or non-Devanagari characters\n",
        "  - Expands hashtags using `split_hashtag`\n",
        "  - Converts all text to lowercase\n",
        "  - Removes punctuation\n",
        "  - Adds a trailing space after \"covid\" to separate it in merged hashtags (e.g. `#CovidSucks` ‚Üí `covid sucks`)\n",
        "  - Normalises underscores and collapses extra whitespace\n",
        "\n",
        "These cleaning steps reduce noise, handle social media-specific artifacts, and standardise inputs, improving interpretability.\n"
      ],
      "metadata": {
        "id": "xhYqwBtVTpis"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split hashtags into readable words (e.g. #NotCool ‚Üí \"Not Cool\")\n",
        "def split_hashtag(tag):\n",
        "    \"\"\"Split camelCase or snake_case hashtag into readable words.\"\"\"\n",
        "    tag = tag.replace(\"_\", \" \")\n",
        "    tag = re.sub(r\"([a-z])([A-Z])\", r\"\\1 \\2\", tag)\n",
        "    tag = re.sub(r\"([A-Za-z])(\\d)\", r\"\\1 \\2\", tag)\n",
        "    tag = re.sub(r\"(\\d)([A-Za-z])\", r\"\\1 \\2\", tag)\n",
        "    return tag\n",
        "\n",
        "# Clean each tweet: expand contractins, decode html entities, remove non-roman characters, links, mentions, punctuations, and expand hashtags\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Clean tweet text: decode HTML, expand contractions, remove links/mentions,\n",
        "    normalise punctuation/casing, and split hashtags.\n",
        "    \"\"\"\n",
        "    text = html.unescape(text) # decodes HTML entities (i.e. '&amp;' -> '&')\n",
        "    text = contractions.fix(text) # expands contractions (i.e. \"don't\" -> \"do not\")\n",
        "    text = re.sub(r\"([^\\w\\s])\\1+\", r\"\\1\", text) # collapses repeated special characters (i.e. && -> &)\n",
        "    text = re.sub(r\"&\", \" and \", text) # replaces ampersands (&) with 'and'\n",
        "    text = re.sub(r\"[^\\x20-\\x7E\\u0900-\\u097F]\", \"\", text) # removes any non-ASCII and non-Devanagari characters (mojibake characters were present)\n",
        "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text) # removes URLs\n",
        "    text = re.sub(r\"@\\w+\", \"\", text) # removes mentions (@'s)\n",
        "\n",
        "    # expands hastags by splitting camelCase or snake_case instances (camelCase: \"#CovidSucks\" | snake_case: \"#covid_sucks\")\n",
        "    hashtags = sorted(re.findall(r\"#\\w+\", text), key=len, reverse=True)\n",
        "    for tag in hashtags:\n",
        "        tag_body = tag[1:]\n",
        "        split = split_hashtag(tag_body)\n",
        "        text = text.replace(tag, split)\n",
        "\n",
        "    text = text.lower() # converts text to lowercase (e.g. \"COVID\" -> \"covid\")\n",
        "    text = re.sub(r\"[^\\w\\s]\", \"\", text) # removes punctuation (e.g. \"price: $1000\" -> \"price 1000\")\n",
        "\n",
        "    # Adds a trailing space after every instance of \"covid\" for hastags\n",
        "    # that originally included \"covid\" or \"COVID\" before converting to lower case. Solving edge cases such as:\n",
        "    # \"#covidsucks\" - hashtag splitting would result in \"covidsucks\", now --> \"covid sucks\"\n",
        "    # \"#COVIDSucks\" - hashtag splitting would result in \"COVIDSucks\" (\"covidsucks\" after applying lowercase), now --> \"covid sucks\"\n",
        "    text = re.sub(r\"covid\", \"covid \", text)\n",
        "\n",
        "    text = re.sub(r\"_\", \" \", text) # normalises underscores (e.g. \"covid_sucks\" -> \"covid sucks\")\n",
        "    text = re.sub(r\"\\s+\", \" \", text) # collapses multiple spaces (e.g. \"covid   sucks\" -> \"covid sucks\")\n",
        "    text = text.strip() # strips edges (e.g. \" covid \" -> \"covid\")\n",
        "    return text\n",
        "\n",
        "# Apply cleaning to English and Hindi text\n",
        "eng_df['text'] = eng_df['text'].astype(str).apply(clean_text)\n",
        "hin_df['text'] = hin_df['text'].astype(str).apply(clean_text)\n",
        "\n",
        "# Preview the cleaned text\n",
        "display(eng_df.head())\n",
        "display(hin_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "id": "ODFnHizMlrJS",
        "outputId": "366c780a-34ac-4ebd-cbdc-be615437e4e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   id lang                                               text label\n",
              "0   0   en  if you made it through this and were not only ...   HOF\n",
              "1   1   en  technically that is still turning back the clo...   HOF\n",
              "2   2   en  and you are the govt stop thinking about world...   NOT\n",
              "3   3   en                 soldier of japan who has dick head   HOF\n",
              "4   4   en  you would be better off asking who does not th...   HOF"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ec8fb32e-3c1e-4a86-95ca-8aca091e670e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>lang</th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>en</td>\n",
              "      <td>if you made it through this and were not only ...</td>\n",
              "      <td>HOF</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>en</td>\n",
              "      <td>technically that is still turning back the clo...</td>\n",
              "      <td>HOF</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>en</td>\n",
              "      <td>and you are the govt stop thinking about world...</td>\n",
              "      <td>NOT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>en</td>\n",
              "      <td>soldier of japan who has dick head</td>\n",
              "      <td>HOF</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>en</td>\n",
              "      <td>you would be better off asking who does not th...</td>\n",
              "      <td>HOF</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ec8fb32e-3c1e-4a86-95ca-8aca091e670e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ec8fb32e-3c1e-4a86-95ca-8aca091e670e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ec8fb32e-3c1e-4a86-95ca-8aca091e670e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-4aaa2c12-3a14-49d3-9126-1a606fdca7af\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-4aaa2c12-3a14-49d3-9126-1a606fdca7af')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-4aaa2c12-3a14-49d3-9126-1a606fdca7af button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"display(hin_df\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 4,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          1,\n          4,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"lang\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"en\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"technically that is still turning back the clock dick head\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"NOT\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "     id lang                                               text label\n",
              "0  3843   hi  hindus dying hindus lives matter kind attn pmn...   NOT\n",
              "1  3844   hi  ‡§∏‡§¨ ‡§≤‡§ó ‡§á‡§§‡§® ‡§™‡§∏ ‡§°‡§®‡§ü ‡§π ‡§ï‡§∞ ‡§∞‡§π ‡§π ‡§´‡§∞ ‡§≠ ‡§Ü‡§Æ ‡§Ü‡§¶‡§Æ ‡§ï ‡§∏‡§≤‡§°‡§∞ ...   NOT\n",
              "2  3845   hi  ‡§∂‡§∞‡§è ‡§∏‡§µ‡§® ‡§∂‡§π‡§¨‡§¶‡§¶‡§® ‡§∏‡§π‡§¨ ‡§∏ ‡§∞‡§∂‡§§ ‡§ï‡§Ø justice for shahab...   NOT\n",
              "3  3846   hi                              ‡§Ü‡§∏‡§Æ‡§® ‡§ï‡§§‡§¨ ‡§ï ‡§®‡§ú‡§Ø‡§ú ‡§î‡§≤‡§¶ ‡§π   HOF\n",
              "4  3847   hi  ‡§á‡§∏ ‡§ï‡§π‡§§ ‡§π ‡§¶‡§ó‡§≤ ‡§™‡§§ ‡§ú‡§∏ ‡§∏‡§™ ‡§ï ‡§¶‡§Æ ‡§™‡§∞ 0 ‡§∏ 10 ‡§ï ‡§∏‡§´‡§∞ ‡§ï‡§Ø ...   NOT"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2e994b54-1941-410b-ad29-85051a2ee1ac\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>lang</th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3843</td>\n",
              "      <td>hi</td>\n",
              "      <td>hindus dying hindus lives matter kind attn pmn...</td>\n",
              "      <td>NOT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3844</td>\n",
              "      <td>hi</td>\n",
              "      <td>‡§∏‡§¨ ‡§≤‡§ó ‡§á‡§§‡§® ‡§™‡§∏ ‡§°‡§®‡§ü ‡§π ‡§ï‡§∞ ‡§∞‡§π ‡§π ‡§´‡§∞ ‡§≠ ‡§Ü‡§Æ ‡§Ü‡§¶‡§Æ ‡§ï ‡§∏‡§≤‡§°‡§∞ ...</td>\n",
              "      <td>NOT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3845</td>\n",
              "      <td>hi</td>\n",
              "      <td>‡§∂‡§∞‡§è ‡§∏‡§µ‡§® ‡§∂‡§π‡§¨‡§¶‡§¶‡§® ‡§∏‡§π‡§¨ ‡§∏ ‡§∞‡§∂‡§§ ‡§ï‡§Ø justice for shahab...</td>\n",
              "      <td>NOT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3846</td>\n",
              "      <td>hi</td>\n",
              "      <td>‡§Ü‡§∏‡§Æ‡§® ‡§ï‡§§‡§¨ ‡§ï ‡§®‡§ú‡§Ø‡§ú ‡§î‡§≤‡§¶ ‡§π</td>\n",
              "      <td>HOF</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3847</td>\n",
              "      <td>hi</td>\n",
              "      <td>‡§á‡§∏ ‡§ï‡§π‡§§ ‡§π ‡§¶‡§ó‡§≤ ‡§™‡§§ ‡§ú‡§∏ ‡§∏‡§™ ‡§ï ‡§¶‡§Æ ‡§™‡§∞ 0 ‡§∏ 10 ‡§ï ‡§∏‡§´‡§∞ ‡§ï‡§Ø ...</td>\n",
              "      <td>NOT</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2e994b54-1941-410b-ad29-85051a2ee1ac')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-2e994b54-1941-410b-ad29-85051a2ee1ac button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-2e994b54-1941-410b-ad29-85051a2ee1ac');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-66d09d14-1ec4-4999-a92a-6ef18b28b7f0\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-66d09d14-1ec4-4999-a92a-6ef18b28b7f0')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-66d09d14-1ec4-4999-a92a-6ef18b28b7f0 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"display(hin_df\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 3843,\n        \"max\": 3847,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          3844,\n          3847,\n          3845\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"lang\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"hi\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"\\u0938\\u092c \\u0932\\u0917 \\u0907\\u0924\\u0928 \\u092a\\u0938 \\u0921\\u0928\\u091f \\u0939 \\u0915\\u0930 \\u0930\\u0939 \\u0939 \\u092b\\u0930 \\u092d \\u0906\\u092e \\u0906\\u0926\\u092e \\u0915 \\u0938\\u0932\\u0921\\u0930 \\u0914\\u0930 \\u0915\\u0928\\u0938\\u091f\\u0930\\u091f\\u0930 \\u0916\\u0926 \\u0938 \\u0939 \\u0916\\u0930\\u0926\\u0928 \\u092a\\u0921 \\u0930\\u0939 \\u0939 \\u0924 \\u092a\\u0938 \\u0915\\u0939 \\u091c \\u0930\\u0939 \\u0939 \\u092c\\u092f\\u0930\\u0921 \\u0911\\u092f\\u0932 \\u092e india fights corona indianeedoxygen india covid crisis\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"HOF\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Back-Translation for Data Augmentation\n",
        "\n",
        "This block performs **back-translation**, a technique used to paraphrase text while preserving its semantic meaning. It uses the Google Translate API to translate each tweet from:\n",
        "- English ‚Üí Hindi ‚Üí English\n",
        "- Hindi ‚Üí English ‚Üí Hindi (only if Devanagari script is detected, to avoid translating Hinglish content)\n",
        "\n",
        "A helper function `devanagari_detection` checks if the Hindi tweet is written in Devanagari script before translating it ‚Äî this avoids corrupting Romanised Hindi. Each dataset is augmented by replacing the original text with its back-translated version to introduce linguistic diversity and potentially improve generalisation during model training.\n",
        "\n",
        "Finally, the augmented datasets are saved as new CSV files for reuse.\n"
      ],
      "metadata": {
        "id": "dSOE9v3CTwkD"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GbBKN0bhcF-h"
      },
      "source": [
        "def devanagari_detection(text):\n",
        "  \"\"\"Check if the given text contains any Devanagari script characters.\"\"\"\n",
        "    return any('\\u0900' <= ch <= '\\u097F' for ch in text)\n",
        "\n",
        "def back_translate(text, src_lang):\n",
        "    \"\"\"\n",
        "    Perform back-translation for paraphrasing via Google Translate API.\n",
        "\n",
        "    If the source language is Hindi ('hi'), only translate if Devanagari script is detected.\n",
        "    - 'hi': HI ‚Üí EN ‚Üí HI\n",
        "    - 'en': EN ‚Üí HI ‚Üí EN\n",
        "\n",
        "    Returns the back-translated text. If translation fails, returns the original text.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if src_lang == 'hi':\n",
        "            if not devanagari_detection(text):\n",
        "                return text\n",
        "            en = translate_client.translate(text, source_language='hi', target_language='en')['translatedText']\n",
        "            return translate_client.translate(en, source_language='en', target_language='hi')['translatedText'].strip()\n",
        "        else:\n",
        "            hi = translate_client.translate(text, source_language='en', target_language='hi')['translatedText']\n",
        "            return translate_client.translate(hi, source_language='hi', target_language='en')['translatedText'].strip()\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        return text\n",
        "\n",
        "# Back translate English tweets\n",
        "backtrans_eng_texts = []\n",
        "for text in eng_df['text'].astype(str).tolist():\n",
        "    translated_text = back_translate(text, 'en')\n",
        "    backtrans_eng_texts.append(translated_text)\n",
        "eng_df['text'] = backtrans_eng_texts\n",
        "\n",
        "# Back translate Hindi tweets\n",
        "backtrans_hin_texts = []\n",
        "for text in hin_df['text'].astype(str).tolist():\n",
        "    translated_text = back_translate(text, 'hi')\n",
        "    backtrans_hin_texts.append(translated_text)\n",
        "hin_df['text'] = backtrans_hin_texts\n",
        "\n",
        "# Save these augmented datasets for reuse later\n",
        "eng_df.to_csv(\"/content/gdrive/Shareddrives/NLP Assignment 3/data/english_preprocessed.csv\", index=False)\n",
        "hin_df.to_csv(\"/content/gdrive/Shareddrives/NLP Assignment 3/data/hindi_preprocessed.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load saved CSV's to skip back-translation in case runtime disconnects\n",
        "eng_df = pd.read_csv(\"/content/gdrive/Shareddrives/NLP Assignment 3/data/english_preprocessed.csv\")\n",
        "hin_df = pd.read_csv(\"/content/gdrive/Shareddrives/NLP Assignment 3/data/hindi_preprocessed.csv\")"
      ],
      "metadata": {
        "id": "okOQ73IUS1h3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare Dataset: Merge, Encode Labels, Balance Classes, and Split\n",
        "\n",
        "This section prepares the final dataset for training, validation, and testing. These steps include:\n",
        "\n",
        "- **Random seed** is set for reproducibility across data splits.\n",
        "- **English and Hindi datasets** are merged into a unified DataFrame to create a multilingual corpus.\n",
        "- **Label encoding** converts categorical labels (`'HOF'`, `'NOT'`) into integers using `LabelEncoder`, enabling compatibility with machine learning models.\n",
        "- **Class weights** are computed using `compute_class_weight` to mitigate label imbalance, ensuring the model doesn't favour majority classes. These weights are converted into a tensor for later use in loss functions.\n",
        "- The dataset is split:\n",
        "  - **60% for training, 20% for validation** and **20% for testing**, stratifying on the label class to ensure even label distribution amongst the splits.\n",
        "- The final text-label splits are converted into HuggingFace `Dataset` objects for integration with transformer models.\n"
      ],
      "metadata": {
        "id": "VS0MfPjPUGS8"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBBHIxqEcF-i"
      },
      "source": [
        "# Set a fixed random seed for reproducibility\n",
        "random_seed = 42\n",
        "\n",
        "# Combine English and Hindi data into one unified DataFrame\n",
        "# This will be used as the full dataset for training and evaluation\n",
        "df = pd.concat([eng_df, hin_df], ignore_index=True)\n",
        "\n",
        "# Encode labels into numerical format (e.g., 'HOF' ‚Üí 0, 'NOT' ‚Üí 1)\n",
        "le = LabelEncoder()\n",
        "df['label'] = le.fit_transform(df['label'])\n",
        "\n",
        "# Compute class weights to address potential label imbalance in the dataset\n",
        "labels = df['label'].values\n",
        "weights = compute_class_weight(class_weight='balanced', classes=np.unique(labels), y=labels)\n",
        "class_weights = torch.tensor(weights, dtype=torch.float).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "\n",
        "# Separate features and labels\n",
        "X = df['text']\n",
        "y = df['label']\n",
        "\n",
        "# Split into train+val and test (80% train_val, 20% test)\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=random_seed)\n",
        "\n",
        "# Further split train+val into 75% train and 25% validation ‚Üí 60% train, 20% val, 20% test\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.25, stratify=y_train_val, random_state=random_seed)\n",
        "\n",
        "# Convert text and labels into HuggingFace Datasets\n",
        "raw_train_ds = Dataset.from_dict({'text': X_train.tolist(), 'label': y_train.tolist()})\n",
        "raw_val_ds = Dataset.from_dict({'text': X_val.tolist(), 'label': y_val.tolist()})\n",
        "raw_test_ds = Dataset.from_dict({'text': X_test.tolist(), 'label': y_test.tolist()})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modelling"
      ],
      "metadata": {
        "id": "IP0VPKULUKZn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Custom Trainer with Class Weights and Evaluation Metrics\n",
        "\n",
        "To address class imbalance during training, a custom `WeightedTrainer` class was defined by subclassing HuggingFace‚Äôs `Trainer`. It overrides the `compute_loss` method to incorporate **class weights** into the cross-entropy loss function. This ensures that the model doesn't learn any unnecessary bias during optimisation.\n",
        "\n",
        "Additionally, the `compute_metrics` function calculates key evaluation metrics:\n",
        "- **Accuracy**: Overall correctness.\n",
        "- **Precision**: Proportion of correctly predicted positive instances.\n",
        "- **Recall**: Proportion of actual positives correctly identified.\n",
        "- **F1 Score**: Harmonic mean of precision and recall.\n"
      ],
      "metadata": {
        "id": "HbAJafA4LMZ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class WeightedTrainer(Trainer):\n",
        "    \"\"\"\n",
        "    Custom Trainer that applies class weights to the CrossEntropyLoss\n",
        "    for imbalanced classification tasks.\n",
        "    \"\"\"\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
        "        labels = inputs.get(\"labels\")\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.get(\"logits\")\n",
        "        loss_fct = CrossEntropyLoss(weight=class_weights.to(logits.device))\n",
        "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    \"\"\"\n",
        "    Compute evaluation metrics for classification:\n",
        "    returns macro-averaged accuracy, precision, recall, and F1.\n",
        "    \"\"\"\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    labels = pred.label_ids\n",
        "    p, r, f1, _ = precision_recall_fscore_support(labels, preds, average='macro')\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {\"accuracy\": acc, \"f1\": f1, \"precision\": p, \"recall\": r}"
      ],
      "metadata": {
        "id": "04fcXLJPIBpT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hyperparameter Tuning with Optuna for Transformer Models\n",
        "\n",
        "This function implements **hyperparameter tuning** using **Optuna** with HuggingFace transformer models. It attempts to maximise classification performance on the data by tuning training parameters.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "- **Tokenisation**: The selected model's tokeniser is applied to both training and validation datasets, with automatic padding and truncation for variable-length input.\n",
        "- **Dynamic Data Collation**: `DataCollatorWithPadding` enables dynamic padding during batching to ensure all the data is of the same length when inputting into the model.\n",
        "- **Model Initialisation**: A fresh instance of a pre-trained transformer is loaded and adapted for sequence classification using a `model_init()` function.\n",
        "- **Search Space**: Hyperparameters such as learning rate, batch size, warm-up steps, number of epochs, optimiser settings (`beta1`, `beta2`, `epsilon`), gradient accumulation steps, and label smoothing are explored.\n",
        "- **WeightedTrainer**: A custom subclass of `Trainer` is used to apply class weights during training, compensating for dataset imbalance.\n",
        "- **Early Stopping**: Built-in early stopping is applied in each trial and final training to reduce overfitting.\n",
        "- **Per-Trial Cleanup**: Non-best trial checkpoints are deleted immediately after training and evaluation to prevent Colab disk overflow, ensuring only the best model is retained.\n",
        "- **Final Training**: After tuning, the best configuration is used to retrain the model from scratch and save the final model and tokeniser to disk.\n",
        "- **Result Reporting**: The final macro F1 score and evaluation metrics are printed after training for benchmarking.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "This pipeline is ideal for **offensive multilingual content classification** or other NLP tasks **content classification**. The cleanup protocols are particularly useful as I was running into memory issues.\n",
        "\n",
        "It produces a fully trained, optimised model along with detailed evaluation metrics.\n",
        "\n",
        "---\n",
        "\n",
        "Generative AI was used to find a suitable framework to complete a grid search of hyperparameters. AI suggested optuna and gave a general guide on how to set it up. Here is a link to the AI prompt: https://chatgpt.com/share/682437fb-7b0c-8006-99f9-bdf63ee902e7\n"
      ],
      "metadata": {
        "id": "qZLhWOhpUSyU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_optuna_search(model_name, raw_train_ds, raw_val_ds, label_list, predefined_params=None):\n",
        "    \"\"\"\n",
        "    Run Optuna hyperparameter search for a HuggingFace Transformer model.\n",
        "    \"\"\"\n",
        "    # Load tokeniser and define padding logic\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "    # Tokenisation function for input text\n",
        "    def tokenize(batch):\n",
        "        return tokenizer(batch[\"text\"], padding=True, truncation=True)\n",
        "\n",
        "    # Apply tokenization to training and validation datasets\n",
        "    train_ds = raw_train_ds.map(tokenize, batched=True)\n",
        "    val_ds = raw_val_ds.map(tokenize, batched=True)\n",
        "\n",
        "    # Function to initialise the model for each trial\n",
        "    def model_init():\n",
        "        return AutoModelForSequenceClassification.from_pretrained(\n",
        "            model_name, num_labels=len(label_list)\n",
        "        )\n",
        "\n",
        "    # If no predefined hyperparameters are provided, run Optuna search\n",
        "    if predefined_params is None:\n",
        "        best_f1 = 0.0\n",
        "        best_trial_number = None\n",
        "        best_hyperparams = None\n",
        "\n",
        "        # Objective function to be optimised by Optuna\n",
        "        def objective(trial):\n",
        "            nonlocal best_f1, best_trial_number, best_hyperparams\n",
        "\n",
        "            # Define hyperparameter search space\n",
        "            hp = {\n",
        "                \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-5, 5e-5, log=True),\n",
        "                \"weight_decay\": trial.suggest_categorical(\"weight_decay\", [0.0, 0.01, 0.1]),\n",
        "                \"per_device_train_batch_size\": trial.suggest_categorical(\"per_device_train_batch_size\", [8, 16, 32]),\n",
        "                \"warmup_steps\": trial.suggest_categorical(\"warmup_steps\", [0, 100, 250, 500]),\n",
        "                \"num_train_epochs\": trial.suggest_int(\"num_train_epochs\", 2, 5),\n",
        "                \"adam_beta1\": trial.suggest_float(\"adam_beta1\", 0.85, 0.95),\n",
        "                \"adam_beta2\": trial.suggest_float(\"adam_beta2\", 0.95, 0.999),\n",
        "                \"adam_epsilon\": trial.suggest_float(\"adam_epsilon\", 1e-8, 1e-6, log=True),\n",
        "                \"gradient_accumulation_steps\": trial.suggest_categorical(\"gradient_accumulation_steps\", [1, 2, 4]),\n",
        "                \"lr_scheduler_type\": trial.suggest_categorical(\"lr_scheduler_type\", [\"linear\", \"cosine\", \"cosine_with_restarts\", \"polynomial\"]),\n",
        "                \"label_smoothing_factor\": trial.suggest_float(\"label_smoothing_factor\", 0.0, 0.2),\n",
        "            }\n",
        "\n",
        "            # Output folder for trial\n",
        "            output_dir = f\"optuna_search/trial-{trial.number}\"\n",
        "\n",
        "            # Training arguments for current trial\n",
        "            training_args = TrainingArguments(\n",
        "                output_dir=output_dir,\n",
        "                eval_strategy=\"epoch\",\n",
        "                save_strategy=\"epoch\",\n",
        "                logging_strategy=\"epoch\",\n",
        "                **hp,\n",
        "                save_total_limit=1,\n",
        "                load_best_model_at_end=True,\n",
        "                metric_for_best_model=\"f1\",\n",
        "                greater_is_better=True,\n",
        "                seed=random_seed,\n",
        "                fp16=torch.cuda.is_available(),\n",
        "                disable_tqdm=True,\n",
        "                per_device_eval_batch_size=64,\n",
        "                logging_dir=f\"{output_dir}/logs\"\n",
        "            )\n",
        "\n",
        "            # Initialise trainer with training args and datasets\n",
        "            trainer = WeightedTrainer(\n",
        "                model_init=model_init,\n",
        "                args=training_args,\n",
        "                train_dataset=train_ds,\n",
        "                eval_dataset=val_ds,\n",
        "                data_collator=data_collator,\n",
        "                compute_metrics=compute_metrics,\n",
        "                callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
        "            )\n",
        "\n",
        "            # Train and evaluate the model\n",
        "            trainer.train()\n",
        "            metrics = trainer.evaluate()\n",
        "            f1 = metrics[\"eval_f1\"]\n",
        "\n",
        "            # Save best results\n",
        "            if f1 > best_f1:\n",
        "                best_f1 = f1\n",
        "                best_trial_number = trial.number\n",
        "                best_hyperparams = hp\n",
        "\n",
        "            # Clean up models from unsuccessful trials\n",
        "            if trial.number != best_trial_number:\n",
        "                shutil.rmtree(output_dir, ignore_errors=True)\n",
        "\n",
        "            return f1  # Objective to maximise\n",
        "\n",
        "        # Create TPESampler for reproducibility, Optuna study and start optimisation\n",
        "        sampler = optuna.samplers.TPESampler(seed=42, multivariate=True, group=True)\n",
        "        study = optuna.create_study(direction=\"maximize\", sampler=sampler)\n",
        "        study.optimize(objective, n_trials=40)\n",
        "\n",
        "        # Print best results\n",
        "        print(\"\\nBest Hyperparameters:\")\n",
        "        for k, v in best_hyperparams.items():\n",
        "            print(f\"{k}: {v}\")\n",
        "        print(f\"Best F1: {best_f1:.4f}\")\n",
        "\n",
        "    else:\n",
        "        # Use predefined parameters if provided\n",
        "        best_hyperparams = predefined_params\n",
        "\n",
        "    # Define final output directory\n",
        "    best_output_dir = f\"{model_name}_best\"\n",
        "\n",
        "    # Set up final training arguments using best hyperparameters\n",
        "    final_args = TrainingArguments(\n",
        "        output_dir=best_output_dir,\n",
        "        **best_hyperparams,\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        logging_strategy=\"epoch\",\n",
        "        save_total_limit=1,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"f1\",\n",
        "        greater_is_better=True,\n",
        "        seed=random_seed,\n",
        "        fp16=torch.cuda.is_available(),\n",
        "        logging_steps=50,\n",
        "        per_device_eval_batch_size=64,\n",
        "        logging_dir=f\"{best_output_dir}/logs\"\n",
        "    )\n",
        "\n",
        "    # Final training using best configuration\n",
        "    final_trainer = WeightedTrainer(\n",
        "        model_init=model_init,\n",
        "        args=final_args,\n",
        "        train_dataset=train_ds,\n",
        "        eval_dataset=val_ds,\n",
        "        data_collator=data_collator,\n",
        "        compute_metrics=compute_metrics,\n",
        "        callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
        "    )\n",
        "\n",
        "    final_trainer.train()\n",
        "    final_eval = final_trainer.evaluate()\n",
        "\n",
        "    # Print final evaluation results\n",
        "    print(\"\\nValidation Set Results:\")\n",
        "    for key, value in final_eval.items():\n",
        "        print(f\"{key}: {value:.4f}\" if isinstance(value, float) else f\"{key}: {value}\")\n",
        "\n",
        "    # Save final model and tokeniser\n",
        "    final_trainer.save_model(best_output_dir)\n",
        "    tokenizer.save_pretrained(best_output_dir)\n",
        "\n",
        "    return {\n",
        "        \"best_hyperparameters\": best_hyperparams,\n",
        "        \"evaluation_metrics\": final_eval\n",
        "    }\n"
      ],
      "metadata": {
        "id": "2IH439LPynn3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fine-Tuning Multiple Transformer Models with Optuna Search\n",
        "\n",
        "A list of multilingual transformer models is defined to evaluate, including:\n",
        "- `xlm-roberta-base`: A strong general-purpose multilingual model.\n",
        "- `distilbert-base-multilingual-cased`: A lightweight, efficient version of multilingual BERT.\n",
        "- `ai4bharat/indic-bert`: A model specifically pre-trained on Indic languages (as well as english).\n",
        "\n",
        "The loop iterates over each model name and performs:\n",
        "- **Back-to-back fine-tuning** using the previously defined `run_optuna_search` function.\n",
        "- **Automated hyperparameter optimisation** via Optuna for each model separately.\n",
        "- **Evaluation and saving** of the best-performing model per architecture.\n",
        "\n",
        "This allows for a comparative analysis of model performance across different multilingual pre-trained transformers.\n"
      ],
      "metadata": {
        "id": "gQx6LgM0UaGc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List of model names to try fine-tuning on\n",
        "models = [\n",
        "    \"xlm-roberta-base\",\n",
        "    \"distilbert-base-multilingual-cased\",\n",
        "    \"ai4bharat/indic-bert\"\n",
        "]\n",
        "\n",
        "# Run fine-tuning loop across all models\n",
        "for model_name in models:\n",
        "    print(f\"\\n{model_name}\")\n",
        "    best = run_optuna_search(model_name, raw_train_ds, raw_val_ds, le.classes_.tolist())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "fe6bd52c4c2a40bb9127918c09783f18",
            "a98cb6b5da3d4fe387293a487ac9f5a1",
            "e92bfde4820345439d11987c5806b4d0",
            "86fc061753964feab47caf263d400fe6",
            "6cc1478a57fc4c1eba111345f3f24650",
            "23b6e4ab33454d64aaa4bd31c1297834",
            "d0ddbb21444e40a5b5b35943e4d505ba",
            "d47c2dfb26e54d35b742bf1366ddc6dc",
            "634d0d6b33554d298c4bd32fcbd5cf2f",
            "18228a13d6be47289dcc4fe6796bcbc3",
            "7b93c018e5df4831b42b9ac2889e1361",
            "effc78b4fb264398b2a1bef70b96f186",
            "48770d75120b48bf89fa4d3906b1f234",
            "4bc08f27ddbd450489ffa5add49754a5",
            "4f13a298eb0741a5ada27460d69b69d4",
            "b5149ad6d5f3434298c0c3aa16a25025",
            "888476001c064a35b060ad280f1bf28b",
            "26a3654ccc1b4ad1b5477886e73acb36",
            "d9aba3def53f4ae7b0b129bfcb15f093",
            "eb668b77cf6b45d9b8fcc233762ecee0",
            "32fe610c960f4001a1e504bafeca702b",
            "d64487a7b9584e13b050c4ce25a9bcdf",
            "a9dabccf2e10420ca8d840fbd088699c",
            "97c29864e98b4a9b8110ceef29f1fe6f",
            "c9626ae8b2b343c89e5115f588cc1033",
            "7a05e20560b246e5818f0d8f4d9fa698",
            "54cc2e75d93d46e6be6fb1b2d42139a9",
            "be68f7611f434241a93bac4258a676d0",
            "2bcd11da8887476db9bef6388496711c",
            "2ed67f4b172f4aa49048c83d803c279f",
            "153c2e300f764d6eba6a3c5deb27ef51",
            "88876d83852e4d51a2845f62aa6401c6",
            "c7e7011782cb41e4b83020069b215f6f",
            "c2ba02a52ce246bab82701c20db0827c",
            "25990a6b60f646b59adc1185a7426d5e",
            "5d88d2d1a4b142638c058bc1a9b63aaf",
            "d30fa226feed494cb8e98e084ec279c5",
            "dbad17785f9d4c7a9b6cb14da432e006",
            "53b4430bf04e42ac81c8794057731fce",
            "973d45fc06fa43789d6479bf877a5066",
            "30a2ab949b1746a3b901c143c2c753eb",
            "c9a9dc93207b4291ab30c2beb81879d1",
            "c1b6b4675a0b4277a3e3e40005c7f6af",
            "b1439e3a2ea44899902f11c932e7a016",
            "c3a69afb5e104bedb07a4c2e76ad4ce1",
            "e5a79ee0d029460c90f9e9a247419bb0",
            "662174339bbb4781be93064d2ed8c6ea",
            "b0e6837934f44c03a70273c9cd6bd9cd",
            "53a399538e8e4fd69b8aa36350055e4e",
            "8d5778deefd74cd2b010c8699f89a084",
            "9289d2ffd4954db592dc2cc335a432c7",
            "8fea450cc60f49f8a171405305ddb030",
            "573236716aeb4341b053cf8aeab564bb",
            "2b6a391d3c0a4f95859221191e024d44",
            "88dd983859bf4d5d8ce03661e868158b",
            "c4401417ce7c4af7b8079879ffb1e73b",
            "0d5a97cb2a7445c9b5c17f1a8db35b09",
            "6b7fb008107b4079a3f4837d70e31c52",
            "eeaf9c8cb3d44680a89477445fb88889",
            "12cd4ab294c243e68d191a76738e2cb9",
            "db48b5d708034266a1e59f8b650ac8fa",
            "1a11f3ee47f8457c89e67887e34faf3c",
            "7585ddcb9cdd46c7b53f1b3ea0d1ce90",
            "12691b5740f74f4d8ddec66456dc7bdd",
            "d2725c8643c64b93858583dfed088dc4",
            "33c84a9a53c244648310c2a3edea2cd4",
            "76e840103f294c56b78f507572f3940f",
            "e48c2cdb63c148d3a00c529f3d21d242",
            "43fc607a99ac4118bbf65d54626927e3",
            "a40d62a401354421a0869b6bb81c5273",
            "5252cfdac84840599a25467fe52d8220",
            "d60f90e2a9784c1fa371fdfac2a79bd1",
            "81a458a9be764b1a9c98fd2c775a46a3",
            "885932a7d4574ff0a22204e8cbc12390",
            "5438467975d246c5be049c2f5c160a32",
            "aa2c9ba71b224a2f821db0b41221cfae",
            "3c69661f891a49d7ac71341cb0f72c59",
            "13a51d39d8714d448c8ee24a46f78133",
            "4511153c0d7e42f5a8f170eb58878b7c",
            "462e538469d44c5899a6f074e0189ff8",
            "aac59c5abb3d4f4eb861233c412df5ca",
            "bd2d2f7926974bcba42abf93e4530b1e",
            "aa6b864100da4cc0808431fbb2916d53",
            "989e8470796e45de89cfe9947bee86f8",
            "d233a961e44040e49f04612ee2aa20cf",
            "4efd5df802df44f691c0b980daca2641",
            "5292dc4e55564663a04773cbc322b354",
            "05bec770242649cc8a845897fc513ff7",
            "87b05c0ea7b74000899eefb3985ef789",
            "fcd31ac615c346faa22afcef14611768",
            "183e9194a40b4a188e390d569d9d24c6",
            "1dd6ef7ae6ec42438411d1f25df8e049",
            "6f7893782787405db2e9af2602a0963d",
            "65c093b33268454990574908e5bb4487",
            "e1a1aa42972f42a888dc166a82b24eb2",
            "772bb059cac545668f2f455ca52d6ada",
            "280e0516adae405882d53add10ca715f",
            "756cb46dce3f4487bd8fa5255b4bce10",
            "34765305af074a0bba6ec633bb274958",
            "1b2bfb75c30c4edfb9bbe7055d14d051",
            "9a7ebb64df644027b58cb4c3fe5655f5",
            "8f8a19ae19084de48ad060fafd0d4fbc",
            "ee46778baa914c7bad1af31c83edef4b",
            "9551f4b4920c4ed0ad4aade9042ab3a7",
            "c9f4b27af1544f5f840418603d5f0817",
            "85064a9509464e04b6f4671f1984044e",
            "5f4d964ee16e4cd1a5709a9a86fcc17a",
            "ff818a277fdf4e698f34104128e998a6",
            "414dbb3896b54ab8b1db7a36e4eee253",
            "aadc00b7c4e24e03903e18705a7568b2",
            "9bb0dcad18f449a194bb55a3112e2dd9",
            "2c5039fb98ef42da9924b3d7e21f4794",
            "f2b81b6f74af451281c6f00495391fd5",
            "14733c2dce3d4779b7f1f00a9f86c15b",
            "7159c7d2e463436ca9dd102122d9c106",
            "99f5db70737a447eb7beabbdee84088d",
            "be6ae6d15ac6495883e42db6c0f6f679",
            "49999fb51a054ae4ba2a8d02454b0219",
            "f3a46d6e07204b9794591fb616a943af",
            "78c9e46c417949f38bf060ef2c77aa31",
            "3a51d8bb5dca458fbd77b275f6b85e36",
            "7175583ad87247a9bbbdd801c95312ea",
            "1766c3d0d4534582bf129ef03894c884",
            "63544a7358d54be8be447f825e247978",
            "4f699288f3544a2d934f71d750b40752",
            "b3b4694073604be5bf001ec01987b138",
            "2474d517975e4e8abffa153932103961",
            "91f0cf0027a847e7b99addfbbb3f6e1d",
            "0aa18d9643f541f887ecb15f79a19a9e",
            "342e1dc3fac34c8aaf52736f2360e68e",
            "c27f6993d00f4876b4c69b3c0ba67b44",
            "50b542274f4f4455bd108c4cffb05a7d",
            "e2adcdbd991e41bc8d04a5b685e24656",
            "8eb54e94206a492499b4372e39e9aedd",
            "6c673f75e09b4f6ba3d7b708a3afc20e",
            "91afa049bdff4e83b740f1e3708b15b0",
            "fed0616f0b074328b16c705ea81222ba",
            "683f3a58406e49b0a54d85d1fe6c7dfb",
            "11be0698056644638922d26ce0f50a5b",
            "b543caf6343a40f3aa7167bdd95b0adb",
            "64f9448b75e64bcc872f289d676e65ae",
            "998067e4a7e040969780a18d61c04fcb",
            "51352df5de1b445ab8bd072ec67d4905",
            "9853fedeaf934e1499efcf9aa32a6cef",
            "6bf4ccd51fe242b8b8ec769b06bbc7e9",
            "c66d8e471adb4a378231247056b531de",
            "c3fbf23ae34248e78c7aa064ccc23c52",
            "5f9775be021d4dad91325d6be1b6f35c",
            "cd9c361fc6784b7fab56de49bb42b8ed",
            "31e1054422384c948428491517fffe88",
            "510603e8f0b34bfdb0095ff964333379",
            "276943eb571f4b4785cb7d9156bb43f3",
            "b69fdbf5cacb4ef28360ac1350191c2b",
            "6b675e61c7414247aa0eefcc28017ddd",
            "f730744cb6e84684a883250a1fbb89cd",
            "ae062950308c4d779f095385d8cf9889",
            "a070102ab39c4ac7aac03b02e533813d",
            "42d2b2e821ce43bf97aec0b3cbdd4753",
            "d69c3e15c5174305965f9ec333a50a25",
            "d2d86728ea22435694fb490c41bb988d",
            "d194bd8842c743279d6b53047f90877d",
            "5cc67ab935814dd29c3f8768fc85e118",
            "8a38123f5261449691720c7bb6df3724",
            "2d77bc9809bb403e996c86aa5889a8bd",
            "e27f1040bacb4e2da45e1698c9bda49a"
          ]
        },
        "id": "NdwzPLyJag7Y",
        "outputId": "bedf491e-05ca-4b2a-d9bd-602efffa3a57",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "xlm-roberta-base\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/5061 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fe6bd52c4c2a40bb9127918c09783f18"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/1688 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "effc78b4fb264398b2a1bef70b96f186"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-12 14:03:27,937] A new study created in memory with name: no-name-8b187d37-e2c5-44af-9873-3532f751d0b2\n",
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.6288, 'grad_norm': 9.421112060546875, 'learning_rate': 1.1402485571255739e-05, 'epoch': 1.0}\n",
            "{'eval_loss': 0.5130487084388733, 'eval_accuracy': 0.7713270142180095, 'eval_f1': 0.7713010087679273, 'eval_precision': 0.776511190571535, 'eval_recall': 0.7755846602744092, 'eval_runtime': 0.6502, 'eval_samples_per_second': 2596.21, 'eval_steps_per_second': 41.527, 'epoch': 1.0}\n",
            "{'loss': 0.5164, 'grad_norm': 5.885914325714111, 'learning_rate': 7.666716656124895e-06, 'epoch': 2.0}\n",
            "{'eval_loss': 0.4762418270111084, 'eval_accuracy': 0.7813981042654028, 'eval_f1': 0.7813421610150898, 'eval_precision': 0.7876077586206897, 'eval_recall': 0.7860636282994893, 'eval_runtime': 0.6655, 'eval_samples_per_second': 2536.486, 'eval_steps_per_second': 40.572, 'epoch': 2.0}\n",
            "{'loss': 0.4584, 'grad_norm': 10.285442352294922, 'learning_rate': 3.895255681295349e-06, 'epoch': 3.0}\n",
            "{'eval_loss': 0.491141140460968, 'eval_accuracy': 0.7879146919431279, 'eval_f1': 0.7878384451185261, 'eval_precision': 0.7891686372699032, 'eval_recall': 0.7902387154185593, 'eval_runtime': 0.6562, 'eval_samples_per_second': 2572.32, 'eval_steps_per_second': 41.145, 'epoch': 3.0}\n",
            "{'loss': 0.4039, 'grad_norm': 23.306535720825195, 'learning_rate': 1.7138411939740504e-07, 'epoch': 3.9889415481832544}\n",
            "{'eval_loss': 0.5021085143089294, 'eval_accuracy': 0.7837677725118484, 'eval_f1': 0.7837039315036706, 'eval_precision': 0.78520192480662, 'eval_recall': 0.7861933726044901, 'eval_runtime': 0.6383, 'eval_samples_per_second': 2644.652, 'eval_steps_per_second': 42.302, 'epoch': 3.9889415481832544}\n",
            "{'train_runtime': 181.3704, 'train_samples_per_second': 111.617, 'train_steps_per_second': 6.969, 'train_loss': 0.5021797252606742, 'epoch': 3.9889415481832544}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-12 14:06:31,753] Trial 0 finished with value: 0.7878384451185261 and parameters: {'learning_rate': 1.2163916178161389e-05, 'weight_decay': 0.0, 'per_device_train_batch_size': 8, 'warmup_steps': 250, 'num_train_epochs': 4, 'adam_beta1': 0.8755859523020287, 'adam_beta2': 0.957936037923716, 'adam_epsilon': 1.0269019949984133e-08, 'gradient_accumulation_steps': 2, 'lr_scheduler_type': 'polynomial', 'label_smoothing_factor': 0.16680111564321964}. Best is trial 0 with value: 0.7878384451185261.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.491141140460968, 'eval_accuracy': 0.7879146919431279, 'eval_f1': 0.7878384451185261, 'eval_precision': 0.7891686372699032, 'eval_recall': 0.7902387154185593, 'eval_runtime': 0.6768, 'eval_samples_per_second': 2494.138, 'eval_steps_per_second': 39.894, 'epoch': 3.9889415481832544}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.6364, 'grad_norm': 16.38515281677246, 'learning_rate': 1.2659963119326328e-05, 'epoch': 1.0}\n",
            "{'eval_loss': 0.5369513034820557, 'eval_accuracy': 0.726303317535545, 'eval_f1': 0.7226397620236525, 'eval_precision': 0.7592767985708455, 'eval_recall': 0.737428552490738, 'eval_runtime': 0.6352, 'eval_samples_per_second': 2657.301, 'eval_steps_per_second': 42.504, 'epoch': 1.0}\n",
            "{'loss': 0.5345, 'grad_norm': 6.729859352111816, 'learning_rate': 2.5730964002267148e-05, 'epoch': 2.0}\n",
            "{'eval_loss': 0.4722779393196106, 'eval_accuracy': 0.7890995260663507, 'eval_f1': 0.7873294007562213, 'eval_precision': 0.7890364148766515, 'eval_recall': 0.786525489820008, 'eval_runtime': 0.653, 'eval_samples_per_second': 2585.066, 'eval_steps_per_second': 41.349, 'epoch': 2.0}\n",
            "{'loss': 0.4803, 'grad_norm': 15.692007064819336, 'learning_rate': 3.871975733248507e-05, 'epoch': 3.0}\n",
            "{'eval_loss': 0.46508902311325073, 'eval_accuracy': 0.7796208530805687, 'eval_f1': 0.7793231418028466, 'eval_precision': 0.7794306138751191, 'eval_recall': 0.7807010987368264, 'eval_runtime': 0.6389, 'eval_samples_per_second': 2641.864, 'eval_steps_per_second': 42.257, 'epoch': 3.0}\n",
            "{'loss': 0.42, 'grad_norm': 21.006187438964844, 'learning_rate': 2.299024779538679e-05, 'epoch': 4.0}\n",
            "{'eval_loss': 0.5070526003837585, 'eval_accuracy': 0.773696682464455, 'eval_f1': 0.7722180774100199, 'eval_precision': 0.7729271556943471, 'eval_recall': 0.7717755367112922, 'eval_runtime': 0.6542, 'eval_samples_per_second': 2580.312, 'eval_steps_per_second': 41.273, 'epoch': 4.0}\n",
            "{'train_runtime': 80.9665, 'train_samples_per_second': 312.537, 'train_steps_per_second': 9.819, 'train_loss': 0.5178123929965421, 'epoch': 4.0}\n",
            "{'eval_loss': 0.4722779393196106, 'eval_accuracy': 0.7890995260663507, 'eval_f1': 0.7873294007562213, 'eval_precision': 0.7890364148766515, 'eval_recall': 0.786525489820008, 'eval_runtime': 0.6508, 'eval_samples_per_second': 2593.881, 'eval_steps_per_second': 41.49, 'epoch': 4.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-12 14:07:56,015] Trial 1 finished with value: 0.7873294007562213 and parameters: {'learning_rate': 4.110377636144912e-05, 'weight_decay': 0.1, 'per_device_train_batch_size': 32, 'warmup_steps': 500, 'num_train_epochs': 5, 'adam_beta1': 0.8548370140763988, 'adam_beta2': 0.9852657466025685, 'adam_epsilon': 1.149450752279312e-07, 'gradient_accumulation_steps': 1, 'lr_scheduler_type': 'linear', 'label_smoothing_factor': 0.17214955828119935}. Best is trial 0 with value: 0.7878384451185261.\n",
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.6151, 'grad_norm': 6.323467254638672, 'learning_rate': 2.2302989897181716e-05, 'epoch': 1.0}\n",
            "{'eval_loss': 0.5041490197181702, 'eval_accuracy': 0.7754739336492891, 'eval_f1': 0.7754643985395316, 'eval_precision': 0.7781405761594998, 'eval_recall': 0.7785849973275494, 'eval_runtime': 0.6429, 'eval_samples_per_second': 2625.677, 'eval_steps_per_second': 41.998, 'epoch': 1.0}\n",
            "{'loss': 0.5284, 'grad_norm': 1.7780476808547974, 'learning_rate': 2.5288094920101255e-05, 'epoch': 2.0}\n",
            "{'eval_loss': 0.46907326579093933, 'eval_accuracy': 0.7956161137440758, 'eval_f1': 0.7953310196371304, 'eval_precision': 0.7953909869437035, 'eval_recall': 0.7967294563290541, 'eval_runtime': 0.6345, 'eval_samples_per_second': 2660.454, 'eval_steps_per_second': 42.555, 'epoch': 2.0}\n",
            "{'loss': 0.4464, 'grad_norm': 5.536614894866943, 'learning_rate': 3.963651241395181e-07, 'epoch': 2.9921011058451814}\n",
            "{'eval_loss': 0.4568566083908081, 'eval_accuracy': 0.7968009478672986, 'eval_f1': 0.795853417998328, 'eval_precision': 0.7958776933362672, 'eval_recall': 0.7958297077791583, 'eval_runtime': 0.6365, 'eval_samples_per_second': 2652.127, 'eval_steps_per_second': 42.421, 'epoch': 2.9921011058451814}\n",
            "{'train_runtime': 135.5239, 'train_samples_per_second': 112.032, 'train_steps_per_second': 6.995, 'train_loss': 0.5302342282065863, 'epoch': 2.9921011058451814}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-12 14:10:14,007] Trial 2 finished with value: 0.795853417998328 and parameters: {'learning_rate': 3.551431512290082e-05, 'weight_decay': 0.01, 'per_device_train_batch_size': 8, 'warmup_steps': 500, 'num_train_epochs': 3, 'adam_beta1': 0.8901108230579624, 'adam_beta2': 0.9534875934230428, 'adam_epsilon': 1.948953838642585e-08, 'gradient_accumulation_steps': 2, 'lr_scheduler_type': 'linear', 'label_smoothing_factor': 0.10874746201247015}. Best is trial 2 with value: 0.795853417998328.\n",
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.4568566083908081, 'eval_accuracy': 0.7968009478672986, 'eval_f1': 0.795853417998328, 'eval_precision': 0.7958776933362672, 'eval_recall': 0.7958297077791583, 'eval_runtime': 0.6836, 'eval_samples_per_second': 2469.339, 'eval_steps_per_second': 39.498, 'epoch': 2.9921011058451814}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.5936, 'grad_norm': 13.129279136657715, 'learning_rate': 1.4963015546236943e-05, 'epoch': 1.0}\n",
            "{'eval_loss': 0.48906558752059937, 'eval_accuracy': 0.7748815165876777, 'eval_f1': 0.7739214533540295, 'eval_precision': 0.7738408827289166, 'eval_recall': 0.7740108054441839, 'eval_runtime': 0.6656, 'eval_samples_per_second': 2535.961, 'eval_steps_per_second': 40.563, 'epoch': 1.0}\n",
            "{'loss': 0.5016, 'grad_norm': 3.136742115020752, 'learning_rate': 1.004510599049678e-05, 'epoch': 2.0}\n",
            "{'eval_loss': 0.46547290682792664, 'eval_accuracy': 0.7885071090047393, 'eval_f1': 0.7884981273842027, 'eval_precision': 0.7931305389763068, 'eval_recall': 0.7925621256630004, 'eval_runtime': 0.6438, 'eval_samples_per_second': 2622.051, 'eval_steps_per_second': 41.94, 'epoch': 2.0}\n",
            "{'loss': 0.4319, 'grad_norm': 11.555007934570312, 'learning_rate': 5.095971612180484e-06, 'epoch': 3.0}\n",
            "{'eval_loss': 0.5014742612838745, 'eval_accuracy': 0.7968009478672986, 'eval_f1': 0.7964202017218616, 'eval_precision': 0.7962672733400742, 'eval_recall': 0.7975177940083515, 'eval_runtime': 0.6558, 'eval_samples_per_second': 2574.126, 'eval_steps_per_second': 41.174, 'epoch': 3.0}\n",
            "{'loss': 0.3622, 'grad_norm': 33.32527160644531, 'learning_rate': 2.092868790164477e-07, 'epoch': 3.9889415481832544}\n",
            "{'eval_loss': 0.5182494521141052, 'eval_accuracy': 0.7902843601895735, 'eval_f1': 0.7899632617079579, 'eval_precision': 0.7899572484573643, 'eval_recall': 0.7912526953674232, 'eval_runtime': 0.629, 'eval_samples_per_second': 2683.452, 'eval_steps_per_second': 42.923, 'epoch': 3.9889415481832544}\n",
            "{'train_runtime': 178.3798, 'train_samples_per_second': 113.488, 'train_steps_per_second': 7.086, 'train_loss': 0.4726660704310936, 'epoch': 3.9889415481832544}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-12 14:13:14,788] Trial 3 finished with value: 0.7964202017218616 and parameters: {'learning_rate': 1.8272846739306512e-05, 'weight_decay': 0.01, 'per_device_train_batch_size': 8, 'warmup_steps': 100, 'num_train_epochs': 4, 'adam_beta1': 0.8644669807046899, 'adam_beta2': 0.9539514555435706, 'adam_epsilon': 2.691864108319155e-08, 'gradient_accumulation_steps': 2, 'lr_scheduler_type': 'polynomial', 'label_smoothing_factor': 0.07632439793510226}. Best is trial 3 with value: 0.7964202017218616.\n",
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.5014742612838745, 'eval_accuracy': 0.7968009478672986, 'eval_f1': 0.7964202017218616, 'eval_precision': 0.7962672733400742, 'eval_recall': 0.7975177940083515, 'eval_runtime': 0.6543, 'eval_samples_per_second': 2579.826, 'eval_steps_per_second': 41.265, 'epoch': 3.9889415481832544}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.5875, 'grad_norm': 2.556758403778076, 'learning_rate': 2.5094181767489082e-05, 'epoch': 1.0}\n",
            "{'eval_loss': 0.52120441198349, 'eval_accuracy': 0.7677725118483413, 'eval_f1': 0.7677672955974844, 'eval_precision': 0.770645199912668, 'eval_recall': 0.7709688655975924, 'eval_runtime': 0.6434, 'eval_samples_per_second': 2623.616, 'eval_steps_per_second': 41.965, 'epoch': 1.0}\n",
            "{'loss': 0.5009, 'grad_norm': 5.722947597503662, 'learning_rate': 1.3342805811105728e-06, 'epoch': 1.9842271293375395}\n",
            "{'eval_loss': 0.4722486734390259, 'eval_accuracy': 0.784952606635071, 'eval_f1': 0.7846332613855518, 'eval_precision': 0.7846581081384639, 'eval_recall': 0.7859367045228582, 'eval_runtime': 0.64, 'eval_samples_per_second': 2637.559, 'eval_steps_per_second': 42.188, 'epoch': 1.9842271293375395}\n",
            "{'train_runtime': 52.7311, 'train_samples_per_second': 191.955, 'train_steps_per_second': 2.996, 'train_loss': 0.5447193580337718, 'epoch': 1.9842271293375395}\n",
            "{'eval_loss': 0.4722486734390259, 'eval_accuracy': 0.784952606635071, 'eval_f1': 0.7846332613855518, 'eval_precision': 0.7846581081384639, 'eval_recall': 0.7859367045228582, 'eval_runtime': 0.6524, 'eval_samples_per_second': 2587.303, 'eval_steps_per_second': 41.385, 'epoch': 1.9842271293375395}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-12 14:14:09,930] Trial 4 finished with value: 0.7846332613855518 and parameters: {'learning_rate': 4.885408295386759e-05, 'weight_decay': 0.0, 'per_device_train_batch_size': 16, 'warmup_steps': 0, 'num_train_epochs': 2, 'adam_beta1': 0.8901194611355209, 'adam_beta2': 0.9881145520208362, 'adam_epsilon': 5.92838369945402e-07, 'gradient_accumulation_steps': 4, 'lr_scheduler_type': 'polynomial', 'label_smoothing_factor': 0.03041398955195338}. Best is trial 3 with value: 0.7964202017218616.\n",
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.6731, 'grad_norm': 7.027419090270996, 'learning_rate': 9.75404143933562e-06, 'epoch': 1.0}\n",
            "{'eval_loss': 0.577264130115509, 'eval_accuracy': 0.7008293838862559, 'eval_f1': 0.7007990368736301, 'eval_precision': 0.7028854595799284, 'eval_recall': 0.7033572749183105, 'eval_runtime': 0.6301, 'eval_samples_per_second': 2678.751, 'eval_steps_per_second': 42.847, 'epoch': 1.0}\n",
            "{'loss': 0.5499, 'grad_norm': 5.944705009460449, 'learning_rate': 1.0746850233780877e-05, 'epoch': 2.0}\n",
            "{'eval_loss': 0.4829968214035034, 'eval_accuracy': 0.7701421800947867, 'eval_f1': 0.7700375836362309, 'eval_precision': 0.7772290805429118, 'eval_recall': 0.7751178628292438, 'eval_runtime': 0.6507, 'eval_samples_per_second': 2593.973, 'eval_steps_per_second': 41.491, 'epoch': 2.0}\n",
            "{'loss': 0.504, 'grad_norm': 8.851909637451172, 'learning_rate': 3.909955442791782e-06, 'epoch': 3.0}\n",
            "{'eval_loss': 0.4668590724468231, 'eval_accuracy': 0.7861374407582938, 'eval_f1': 0.7859853427030832, 'eval_precision': 0.7866755032350827, 'eval_recall': 0.787930818080151, 'eval_runtime': 0.6367, 'eval_samples_per_second': 2651.32, 'eval_steps_per_second': 42.409, 'epoch': 3.0}\n",
            "{'loss': 0.4638, 'grad_norm': 10.04995346069336, 'learning_rate': 1.6740742876775452e-08, 'epoch': 3.9559748427672954}\n",
            "{'eval_loss': 0.47167953848838806, 'eval_accuracy': 0.7808056872037915, 'eval_f1': 0.7807749116865759, 'eval_precision': 0.7828295828076408, 'eval_recall': 0.7835794479379823, 'eval_runtime': 0.6332, 'eval_samples_per_second': 2665.65, 'eval_steps_per_second': 42.638, 'epoch': 3.9559748427672954}\n",
            "{'train_runtime': 75.4715, 'train_samples_per_second': 268.234, 'train_steps_per_second': 4.187, 'train_loss': 0.5487558751166621, 'epoch': 3.9559748427672954}\n",
            "{'eval_loss': 0.4668590724468231, 'eval_accuracy': 0.7861374407582938, 'eval_f1': 0.7859853427030832, 'eval_precision': 0.7866755032350827, 'eval_recall': 0.787930818080151, 'eval_runtime': 0.671, 'eval_samples_per_second': 2515.477, 'eval_steps_per_second': 40.236, 'epoch': 3.9559748427672954}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-12 14:15:28,308] Trial 5 finished with value: 0.7859853427030832 and parameters: {'learning_rate': 1.2667586284851453e-05, 'weight_decay': 0.01, 'per_device_train_batch_size': 32, 'warmup_steps': 100, 'num_train_epochs': 4, 'adam_beta1': 0.9229861382866147, 'adam_beta2': 0.9555321109587673, 'adam_epsilon': 8.378574036234562e-07, 'gradient_accumulation_steps': 2, 'lr_scheduler_type': 'cosine', 'label_smoothing_factor': 0.0976475883983766}. Best is trial 3 with value: 0.7964202017218616.\n",
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.6168, 'grad_norm': 11.168665885925293, 'learning_rate': 2.0996358039011034e-05, 'epoch': 1.0}\n",
            "{'eval_loss': 0.5206325054168701, 'eval_accuracy': 0.7630331753554502, 'eval_f1': 0.7601206218105493, 'eval_precision': 0.7640402762704921, 'eval_recall': 0.7590535435003039, 'eval_runtime': 0.6701, 'eval_samples_per_second': 2519.169, 'eval_steps_per_second': 40.295, 'epoch': 1.0}\n",
            "{'loss': 0.508, 'grad_norm': 7.752590656280518, 'learning_rate': 9.435266911759403e-06, 'epoch': 2.0}\n",
            "{'eval_loss': 0.469700425863266, 'eval_accuracy': 0.792654028436019, 'eval_f1': 0.7926537373567213, 'eval_precision': 0.7964633105501125, 'eval_recall': 0.7963663133014708, 'eval_runtime': 0.6364, 'eval_samples_per_second': 2652.243, 'eval_steps_per_second': 42.423, 'epoch': 2.0}\n",
            "{'loss': 0.425, 'grad_norm': 19.23312759399414, 'learning_rate': 5.266446950090669e-09, 'epoch': 3.0}\n",
            "{'eval_loss': 0.4686954617500305, 'eval_accuracy': 0.79739336492891, 'eval_f1': 0.7969598278032962, 'eval_precision': 0.7967368783317765, 'eval_recall': 0.7979119628480003, 'eval_runtime': 0.6414, 'eval_samples_per_second': 2631.564, 'eval_steps_per_second': 42.093, 'epoch': 3.0}\n",
            "{'train_runtime': 90.1915, 'train_samples_per_second': 168.342, 'train_steps_per_second': 10.544, 'train_loss': 0.5166021240747815, 'epoch': 3.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-12 14:17:00,570] Trial 6 finished with value: 0.7969598278032962 and parameters: {'learning_rate': 2.140688782970674e-05, 'weight_decay': 0.0, 'per_device_train_batch_size': 16, 'warmup_steps': 250, 'num_train_epochs': 3, 'adam_beta1': 0.8622909944106082, 'adam_beta2': 0.9965067186372787, 'adam_epsilon': 8.597199157697101e-08, 'gradient_accumulation_steps': 1, 'lr_scheduler_type': 'cosine_with_restarts', 'label_smoothing_factor': 0.13818274127494543}. Best is trial 6 with value: 0.7969598278032962.\n",
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.4686954617500305, 'eval_accuracy': 0.79739336492891, 'eval_f1': 0.7969598278032962, 'eval_precision': 0.7967368783317765, 'eval_recall': 0.7979119628480003, 'eval_runtime': 0.6594, 'eval_samples_per_second': 2560.01, 'eval_steps_per_second': 40.948, 'epoch': 3.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.6041, 'grad_norm': 7.113738059997559, 'learning_rate': 3.0166438512078353e-05, 'epoch': 1.0}\n",
            "{'eval_loss': 0.5223957896232605, 'eval_accuracy': 0.7381516587677726, 'eval_f1': 0.7351399246843388, 'eval_precision': 0.7688317150969188, 'eval_recall': 0.7487684868006323, 'eval_runtime': 0.6299, 'eval_samples_per_second': 2679.921, 'eval_steps_per_second': 42.866, 'epoch': 1.0}\n",
            "{'loss': 0.5419, 'grad_norm': 7.6694512367248535, 'learning_rate': 1.8886466460736733e-06, 'epoch': 2.0}\n",
            "{'eval_loss': 0.4964049160480499, 'eval_accuracy': 0.7695497630331753, 'eval_f1': 0.7690692354088305, 'eval_precision': 0.7820199314496386, 'eval_recall': 0.7761706250431892, 'eval_runtime': 0.6417, 'eval_samples_per_second': 2630.341, 'eval_steps_per_second': 42.073, 'epoch': 2.0}\n",
            "{'train_runtime': 62.4613, 'train_samples_per_second': 162.052, 'train_steps_per_second': 10.15, 'train_loss': 0.5729656038976242, 'epoch': 2.0}\n",
            "{'eval_loss': 0.4964049160480499, 'eval_accuracy': 0.7695497630331753, 'eval_f1': 0.7690692354088305, 'eval_precision': 0.7820199314496386, 'eval_recall': 0.7761706250431892, 'eval_runtime': 0.6484, 'eval_samples_per_second': 2603.478, 'eval_steps_per_second': 41.643, 'epoch': 2.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-12 14:18:05,417] Trial 7 finished with value: 0.7690692354088305 and parameters: {'learning_rate': 4.803573011477445e-05, 'weight_decay': 0.01, 'per_device_train_batch_size': 16, 'warmup_steps': 500, 'num_train_epochs': 2, 'adam_beta1': 0.8839579697099269, 'adam_beta2': 0.9834695227765562, 'adam_epsilon': 3.851844870123975e-08, 'gradient_accumulation_steps': 1, 'lr_scheduler_type': 'polynomial', 'label_smoothing_factor': 0.16828874890359763}. Best is trial 6 with value: 0.7969598278032962.\n",
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.6913, 'grad_norm': 2.564788818359375, 'learning_rate': 9.163295394063527e-06, 'epoch': 0.9811320754716981}\n",
            "{'eval_loss': 0.6223734617233276, 'eval_accuracy': 0.6789099526066351, 'eval_f1': 0.6757743671742881, 'eval_precision': 0.6775096187478139, 'eval_recall': 0.6753487230762938, 'eval_runtime': 0.6382, 'eval_samples_per_second': 2644.996, 'eval_steps_per_second': 42.307, 'epoch': 0.9811320754716981}\n",
            "{'loss': 0.587, 'grad_norm': 7.652460098266602, 'learning_rate': 1.7844312083176342e-05, 'epoch': 1.9811320754716981}\n",
            "{'eval_loss': 0.5176981687545776, 'eval_accuracy': 0.7494075829383886, 'eval_f1': 0.7482795272467677, 'eval_precision': 0.7670432930076847, 'eval_recall': 0.7573830855734205, 'eval_runtime': 0.6398, 'eval_samples_per_second': 2638.172, 'eval_steps_per_second': 42.198, 'epoch': 1.9811320754716981}\n",
            "{'train_runtime': 34.6881, 'train_samples_per_second': 291.8, 'train_steps_per_second': 2.249, 'train_loss': 0.6391780804365109, 'epoch': 1.9811320754716981}\n",
            "{'eval_loss': 0.5176981687545776, 'eval_accuracy': 0.7494075829383886, 'eval_f1': 0.7482795272467677, 'eval_precision': 0.7670432930076847, 'eval_recall': 0.7573830855734205, 'eval_runtime': 0.6814, 'eval_samples_per_second': 2477.326, 'eval_steps_per_second': 39.625, 'epoch': 1.9811320754716981}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-12 14:18:42,542] Trial 8 finished with value: 0.7482795272467677 and parameters: {'learning_rate': 2.41139352475356e-05, 'weight_decay': 0.01, 'per_device_train_batch_size': 32, 'warmup_steps': 100, 'num_train_epochs': 2, 'adam_beta1': 0.8602578932686146, 'adam_beta2': 0.9507081825293817, 'adam_epsilon': 1.6252687981634883e-07, 'gradient_accumulation_steps': 4, 'lr_scheduler_type': 'linear', 'label_smoothing_factor': 0.158959826545784}. Best is trial 6 with value: 0.7969598278032962.\n",
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.5808, 'grad_norm': 3.621155023574829, 'learning_rate': 1.591587723263422e-05, 'epoch': 1.0}\n",
            "{'eval_loss': 0.4968704879283905, 'eval_accuracy': 0.768957345971564, 'eval_f1': 0.7664180121261667, 'eval_precision': 0.7695549110180113, 'eval_recall': 0.7654067836527817, 'eval_runtime': 0.6642, 'eval_samples_per_second': 2541.277, 'eval_steps_per_second': 40.648, 'epoch': 1.0}\n",
            "{'loss': 0.4914, 'grad_norm': 8.035633087158203, 'learning_rate': 5.894769345420082e-07, 'epoch': 1.9842271293375395}\n",
            "{'eval_loss': 0.46589475870132446, 'eval_accuracy': 0.7754739336492891, 'eval_f1': 0.7750532258092873, 'eval_precision': 0.7749325918436131, 'eval_recall': 0.7760930605130258, 'eval_runtime': 0.6442, 'eval_samples_per_second': 2620.481, 'eval_steps_per_second': 41.915, 'epoch': 1.9842271293375395}\n",
            "{'train_runtime': 49.3642, 'train_samples_per_second': 205.047, 'train_steps_per_second': 3.201, 'train_loss': 0.5366967358166659, 'epoch': 1.9842271293375395}\n",
            "{'eval_loss': 0.46589475870132446, 'eval_accuracy': 0.7754739336492891, 'eval_f1': 0.7750532258092873, 'eval_precision': 0.7749325918436131, 'eval_recall': 0.7760930605130258, 'eval_runtime': 0.6451, 'eval_samples_per_second': 2616.659, 'eval_steps_per_second': 41.854, 'epoch': 1.9842271293375395}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-12 14:19:34,287] Trial 9 finished with value: 0.7750532258092873 and parameters: {'learning_rate': 3.104578521921243e-05, 'weight_decay': 0.1, 'per_device_train_batch_size': 16, 'warmup_steps': 0, 'num_train_epochs': 2, 'adam_beta1': 0.8748376599179193, 'adam_beta2': 0.9774929374364021, 'adam_epsilon': 5.208058189721321e-07, 'gradient_accumulation_steps': 4, 'lr_scheduler_type': 'linear', 'label_smoothing_factor': 0.17385083458287454}. Best is trial 6 with value: 0.7969598278032962.\n",
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.6089, 'grad_norm': 9.870415687561035, 'learning_rate': 1.78846324641358e-05, 'epoch': 1.0}\n",
            "{'eval_loss': 0.5054540634155273, 'eval_accuracy': 0.7630331753554502, 'eval_f1': 0.7628250792807755, 'eval_precision': 0.7716807416887458, 'eval_recall': 0.7685389804071997, 'eval_runtime': 0.6473, 'eval_samples_per_second': 2607.581, 'eval_steps_per_second': 41.709, 'epoch': 1.0}\n",
            "{'loss': 0.5079, 'grad_norm': 9.844346046447754, 'learning_rate': 7.955832406083502e-06, 'epoch': 2.0}\n",
            "{'eval_loss': 0.4731485843658447, 'eval_accuracy': 0.7879146919431279, 'eval_f1': 0.7879146919431279, 'eval_precision': 0.7915248763550875, 'eval_recall': 0.7915248763550875, 'eval_runtime': 0.6393, 'eval_samples_per_second': 2640.444, 'eval_steps_per_second': 42.235, 'epoch': 2.0}\n",
            "{'loss': 0.4185, 'grad_norm': 26.521207809448242, 'learning_rate': 2.2888384349671893e-09, 'epoch': 3.0}\n",
            "{'eval_loss': 0.482596218585968, 'eval_accuracy': 0.7908767772511849, 'eval_f1': 0.7904849306350353, 'eval_precision': 0.7903409729243905, 'eval_recall': 0.7915664791485388, 'eval_runtime': 0.6408, 'eval_samples_per_second': 2634.019, 'eval_steps_per_second': 42.132, 'epoch': 3.0}\n",
            "{'train_runtime': 89.4935, 'train_samples_per_second': 169.655, 'train_steps_per_second': 10.626, 'train_loss': 0.5117616222233176, 'epoch': 3.0}\n",
            "{'eval_loss': 0.482596218585968, 'eval_accuracy': 0.7908767772511849, 'eval_f1': 0.7904849306350353, 'eval_precision': 0.7903409729243905, 'eval_recall': 0.7915664791485388, 'eval_runtime': 0.6634, 'eval_samples_per_second': 2544.649, 'eval_steps_per_second': 40.702, 'epoch': 3.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-12 14:21:06,220] Trial 10 finished with value: 0.7904849306350353 and parameters: {'learning_rate': 1.823432046281298e-05, 'weight_decay': 0.0, 'per_device_train_batch_size': 16, 'warmup_steps': 250, 'num_train_epochs': 3, 'adam_beta1': 0.913404632535913, 'adam_beta2': 0.9981492654138489, 'adam_epsilon': 2.3489302017709188e-07, 'gradient_accumulation_steps': 1, 'lr_scheduler_type': 'cosine_with_restarts', 'label_smoothing_factor': 0.01347453058292547}. Best is trial 6 with value: 0.7969598278032962.\n",
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.5978, 'grad_norm': 11.638314247131348, 'learning_rate': 1.744916405318301e-05, 'epoch': 1.0}\n",
            "{'eval_loss': 0.49291929602622986, 'eval_accuracy': 0.7784360189573459, 'eval_f1': 0.7762193738656987, 'eval_precision': 0.7788175967657835, 'eval_recall': 0.775250427662614, 'eval_runtime': 0.6585, 'eval_samples_per_second': 2563.306, 'eval_steps_per_second': 41.001, 'epoch': 1.0}\n",
            "{'loss': 0.5023, 'grad_norm': 4.320458889007568, 'learning_rate': 1.3175428349489476e-05, 'epoch': 2.0}\n",
            "{'eval_loss': 0.4678020775318146, 'eval_accuracy': 0.7962085308056872, 'eval_f1': 0.7961853549641631, 'eval_precision': 0.7984060550213488, 'eval_recall': 0.7991332516320282, 'eval_runtime': 0.6392, 'eval_samples_per_second': 2640.602, 'eval_steps_per_second': 42.237, 'epoch': 2.0}\n",
            "{'loss': 0.4361, 'grad_norm': 13.626559257507324, 'learning_rate': 7.152103635327195e-06, 'epoch': 3.0}\n",
            "{'eval_loss': 0.4946773946285248, 'eval_accuracy': 0.7725118483412322, 'eval_f1': 0.7724961988636603, 'eval_precision': 0.7773149817261539, 'eval_recall': 0.7766141531293058, 'eval_runtime': 0.6596, 'eval_samples_per_second': 2559.273, 'eval_steps_per_second': 40.936, 'epoch': 3.0}\n",
            "{'loss': 0.3527, 'grad_norm': 18.30449676513672, 'learning_rate': 2.013571330917967e-06, 'epoch': 4.0}\n",
            "{'eval_loss': 0.5100204944610596, 'eval_accuracy': 0.7855450236966824, 'eval_f1': 0.7850377468673266, 'eval_precision': 0.7847946708706202, 'eval_recall': 0.7858485630113089, 'eval_runtime': 0.6405, 'eval_samples_per_second': 2635.288, 'eval_steps_per_second': 42.152, 'epoch': 4.0}\n",
            "{'train_runtime': 176.7981, 'train_samples_per_second': 143.129, 'train_steps_per_second': 8.937, 'train_loss': 0.4722243528636848, 'epoch': 4.0}\n",
            "{'eval_loss': 0.4678020775318146, 'eval_accuracy': 0.7962085308056872, 'eval_f1': 0.7961853549641631, 'eval_precision': 0.7984060550213488, 'eval_recall': 0.7991332516320282, 'eval_runtime': 0.6542, 'eval_samples_per_second': 2580.431, 'eval_steps_per_second': 41.275, 'epoch': 4.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-12 14:24:05,879] Trial 11 finished with value: 0.7961853549641631 and parameters: {'learning_rate': 1.836327003219829e-05, 'weight_decay': 0.0, 'per_device_train_batch_size': 8, 'warmup_steps': 100, 'num_train_epochs': 5, 'adam_beta1': 0.9471275811434123, 'adam_beta2': 0.9661697793759623, 'adam_epsilon': 4.730126020914986e-08, 'gradient_accumulation_steps': 2, 'lr_scheduler_type': 'cosine_with_restarts', 'label_smoothing_factor': 0.08781855379953499}. Best is trial 6 with value: 0.7969598278032962.\n",
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.5999, 'grad_norm': 4.21232795715332, 'learning_rate': 2.130328533512098e-05, 'epoch': 1.0}\n",
            "{'eval_loss': 0.5281887650489807, 'eval_accuracy': 0.7624407582938388, 'eval_f1': 0.7615179970961685, 'eval_precision': 0.779208910758324, 'eval_recall': 0.7701544380308764, 'eval_runtime': 0.6483, 'eval_samples_per_second': 2603.926, 'eval_steps_per_second': 41.65, 'epoch': 1.0}\n",
            "{'loss': 0.5071, 'grad_norm': 6.520579814910889, 'learning_rate': 1.3463121736868385e-05, 'epoch': 2.0}\n",
            "{'eval_loss': 0.4907272160053253, 'eval_accuracy': 0.7914691943127962, 'eval_f1': 0.7890707092508762, 'eval_precision': 0.7927659934567496, 'eval_recall': 0.7878610100030039, 'eval_runtime': 0.6567, 'eval_samples_per_second': 2570.596, 'eval_steps_per_second': 41.117, 'epoch': 2.0}\n",
            "{'loss': 0.4255, 'grad_norm': 24.444272994995117, 'learning_rate': 4.14683453453526e-06, 'epoch': 3.0}\n",
            "{'eval_loss': 0.5427922010421753, 'eval_accuracy': 0.7867298578199052, 'eval_f1': 0.7865977426446316, 'eval_precision': 0.7874207813107788, 'eval_recall': 0.7886465271539318, 'eval_runtime': 0.6489, 'eval_samples_per_second': 2601.5, 'eval_steps_per_second': 41.612, 'epoch': 3.0}\n",
            "{'loss': 0.3562, 'grad_norm': 20.993732452392578, 'learning_rate': 5.294545198686609e-10, 'epoch': 4.0}\n",
            "{'eval_loss': 0.5795150399208069, 'eval_accuracy': 0.7920616113744076, 'eval_f1': 0.791507772430963, 'eval_precision': 0.7912292883688765, 'eval_recall': 0.7921940467107703, 'eval_runtime': 0.639, 'eval_samples_per_second': 2641.51, 'eval_steps_per_second': 42.252, 'epoch': 4.0}\n",
            "{'train_runtime': 205.2905, 'train_samples_per_second': 98.611, 'train_steps_per_second': 12.334, 'train_loss': 0.47216554614604933, 'epoch': 4.0}\n",
            "{'eval_loss': 0.5795150399208069, 'eval_accuracy': 0.7920616113744076, 'eval_f1': 0.791507772430963, 'eval_precision': 0.7912292883688765, 'eval_recall': 0.7921940467107703, 'eval_runtime': 0.7209, 'eval_samples_per_second': 2341.582, 'eval_steps_per_second': 37.454, 'epoch': 4.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-12 14:27:33,699] Trial 12 finished with value: 0.791507772430963 and parameters: {'learning_rate': 2.2804862982471552e-05, 'weight_decay': 0.01, 'per_device_train_batch_size': 8, 'warmup_steps': 250, 'num_train_epochs': 4, 'adam_beta1': 0.8509937757605762, 'adam_beta2': 0.968674516804908, 'adam_epsilon': 5.517762874066458e-08, 'gradient_accumulation_steps': 1, 'lr_scheduler_type': 'cosine_with_restarts', 'label_smoothing_factor': 0.05920349106691464}. Best is trial 6 with value: 0.7969598278032962.\n",
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.6145, 'grad_norm': 5.518176078796387, 'learning_rate': 1.5875134638609052e-05, 'epoch': 1.0}\n",
            "{'eval_loss': 0.5007212162017822, 'eval_accuracy': 0.7754739336492891, 'eval_f1': 0.7754700724214236, 'eval_precision': 0.7784516423511909, 'eval_recall': 0.7787457674446154, 'eval_runtime': 0.6522, 'eval_samples_per_second': 2588.123, 'eval_steps_per_second': 41.398, 'epoch': 1.0}\n",
            "{'loss': 0.5122, 'grad_norm': 6.160147190093994, 'learning_rate': 7.0089911296066895e-06, 'epoch': 2.0}\n",
            "{'eval_loss': 0.46660590171813965, 'eval_accuracy': 0.7914691943127962, 'eval_f1': 0.7914118010503552, 'eval_precision': 0.7978913958227853, 'eval_recall': 0.7962210560904375, 'eval_runtime': 0.6418, 'eval_samples_per_second': 2630.281, 'eval_steps_per_second': 42.072, 'epoch': 2.0}\n",
            "{'loss': 0.4354, 'grad_norm': 6.028893947601318, 'learning_rate': 2.9531223172696193e-09, 'epoch': 2.9921011058451814}\n",
            "{'eval_loss': 0.4785385727882385, 'eval_accuracy': 0.7962085308056872, 'eval_f1': 0.7959997582935525, 'eval_precision': 0.7963323024397503, 'eval_recall': 0.7976863205784339, 'eval_runtime': 0.6433, 'eval_samples_per_second': 2623.851, 'eval_steps_per_second': 41.969, 'epoch': 2.9921011058451814}\n",
            "{'train_runtime': 132.1985, 'train_samples_per_second': 114.85, 'train_steps_per_second': 7.171, 'train_loss': 0.5209560635723646, 'epoch': 2.9921011058451814}\n",
            "{'eval_loss': 0.4785385727882385, 'eval_accuracy': 0.7962085308056872, 'eval_f1': 0.7959997582935525, 'eval_precision': 0.7963323024397503, 'eval_recall': 0.7976863205784339, 'eval_runtime': 0.6555, 'eval_samples_per_second': 2575.241, 'eval_steps_per_second': 41.192, 'epoch': 2.9921011058451814}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-12 14:29:48,343] Trial 13 finished with value: 0.7959997582935525 and parameters: {'learning_rate': 1.6198560399575845e-05, 'weight_decay': 0.0, 'per_device_train_batch_size': 8, 'warmup_steps': 250, 'num_train_epochs': 3, 'adam_beta1': 0.8675709314203409, 'adam_beta2': 0.9971762893392613, 'adam_epsilon': 2.4345828013711587e-08, 'gradient_accumulation_steps': 2, 'lr_scheduler_type': 'cosine', 'label_smoothing_factor': 0.13256080258606026}. Best is trial 6 with value: 0.7969598278032962.\n",
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.5997, 'grad_norm': 15.265664100646973, 'learning_rate': 2.16702409810265e-05, 'epoch': 1.0}\n",
            "{'eval_loss': 0.4843302369117737, 'eval_accuracy': 0.7843601895734598, 'eval_f1': 0.7839449650968329, 'eval_precision': 0.7837951708243978, 'eval_recall': 0.7849798402734784, 'eval_runtime': 0.6396, 'eval_samples_per_second': 2639.109, 'eval_steps_per_second': 42.213, 'epoch': 1.0}\n",
            "{'loss': 0.4912, 'grad_norm': 7.157354354858398, 'learning_rate': 1.4532862813482654e-05, 'epoch': 2.0}\n",
            "{'eval_loss': 0.45578646659851074, 'eval_accuracy': 0.7962085308056872, 'eval_f1': 0.7961673256649893, 'eval_precision': 0.7979843851713961, 'eval_recall': 0.7988920964564292, 'eval_runtime': 0.6333, 'eval_samples_per_second': 2665.444, 'eval_steps_per_second': 42.634, 'epoch': 2.0}\n",
            "{'loss': 0.4049, 'grad_norm': 21.490514755249023, 'learning_rate': 7.3728980061680995e-06, 'epoch': 3.0}\n",
            "{'eval_loss': 0.5047149062156677, 'eval_accuracy': 0.7902843601895735, 'eval_f1': 0.7899230921754832, 'eval_precision': 0.7898330026524678, 'eval_recall': 0.7910919252503572, 'eval_runtime': 0.6283, 'eval_samples_per_second': 2686.739, 'eval_steps_per_second': 42.975, 'epoch': 3.0}\n",
            "{'loss': 0.3234, 'grad_norm': 18.2502384185791, 'learning_rate': 2.3551983862424933e-07, 'epoch': 4.0}\n",
            "{'eval_loss': 0.5497989654541016, 'eval_accuracy': 0.7873222748815166, 'eval_f1': 0.787112401568925, 'eval_precision': 0.7874887690925427, 'eval_recall': 0.7887995408179814, 'eval_runtime': 0.6477, 'eval_samples_per_second': 2606.202, 'eval_steps_per_second': 41.687, 'epoch': 4.0}\n",
            "{'train_runtime': 121.2908, 'train_samples_per_second': 166.905, 'train_steps_per_second': 10.454, 'train_loss': 0.4547851093183933, 'epoch': 4.0}\n",
            "{'eval_loss': 0.45578646659851074, 'eval_accuracy': 0.7962085308056872, 'eval_f1': 0.7961673256649893, 'eval_precision': 0.7979843851713961, 'eval_recall': 0.7988920964564292, 'eval_runtime': 0.6592, 'eval_samples_per_second': 2560.596, 'eval_steps_per_second': 40.957, 'epoch': 4.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-12 14:31:52,978] Trial 14 finished with value: 0.7961673256649893 and parameters: {'learning_rate': 2.6481195252187383e-05, 'weight_decay': 0.1, 'per_device_train_batch_size': 16, 'warmup_steps': 100, 'num_train_epochs': 4, 'adam_beta1': 0.9050522472716017, 'adam_beta2': 0.9649394914885664, 'adam_epsilon': 6.556205156285124e-08, 'gradient_accumulation_steps': 1, 'lr_scheduler_type': 'polynomial', 'label_smoothing_factor': 0.06397592477073387}. Best is trial 6 with value: 0.7969598278032962.\n",
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.6678, 'grad_norm': 6.990504264831543, 'learning_rate': 6.350584418958797e-06, 'epoch': 1.0}\n",
            "{'eval_loss': 0.5698009133338928, 'eval_accuracy': 0.7209715639810427, 'eval_f1': 0.716282779404444, 'eval_precision': 0.7225687891708021, 'eval_recall': 0.7155532395883721, 'eval_runtime': 0.6338, 'eval_samples_per_second': 2663.254, 'eval_steps_per_second': 42.599, 'epoch': 1.0}\n",
            "{'loss': 0.5484, 'grad_norm': 5.262779712677002, 'learning_rate': 8.314604064534337e-06, 'epoch': 2.0}\n",
            "{'eval_loss': 0.48812776803970337, 'eval_accuracy': 0.7742890995260664, 'eval_f1': 0.774287119132677, 'eval_precision': 0.7782713142844238, 'eval_recall': 0.7780378148238509, 'eval_runtime': 0.6338, 'eval_samples_per_second': 2663.156, 'eval_steps_per_second': 42.598, 'epoch': 2.0}\n",
            "{'loss': 0.4946, 'grad_norm': 9.625677108764648, 'learning_rate': 8.056970996769229e-09, 'epoch': 2.9842271293375395}\n",
            "{'eval_loss': 0.48595577478408813, 'eval_accuracy': 0.7790284360189573, 'eval_f1': 0.7787002933796443, 'eval_precision': 0.7787334052159849, 'eval_recall': 0.7799853896630455, 'eval_runtime': 0.6663, 'eval_samples_per_second': 2533.38, 'eval_steps_per_second': 40.522, 'epoch': 2.9842271293375395}\n",
            "{'train_runtime': 80.8457, 'train_samples_per_second': 187.802, 'train_steps_per_second': 5.863, 'train_loss': 0.570778134502942, 'epoch': 2.9842271293375395}\n",
            "{'eval_loss': 0.48595577478408813, 'eval_accuracy': 0.7790284360189573, 'eval_f1': 0.7787002933796443, 'eval_precision': 0.7787334052159849, 'eval_recall': 0.7799853896630455, 'eval_runtime': 0.6565, 'eval_samples_per_second': 2571.242, 'eval_steps_per_second': 41.128, 'epoch': 2.9842271293375395}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-12 14:33:16,276] Trial 15 finished with value: 0.7787002933796443 and parameters: {'learning_rate': 1.0242878095094833e-05, 'weight_decay': 0.01, 'per_device_train_batch_size': 16, 'warmup_steps': 250, 'num_train_epochs': 3, 'adam_beta1': 0.8641889033502269, 'adam_beta2': 0.9757642274449456, 'adam_epsilon': 1.0041555756303558e-08, 'gradient_accumulation_steps': 2, 'lr_scheduler_type': 'cosine_with_restarts', 'label_smoothing_factor': 0.12531031010471186}. Best is trial 6 with value: 0.7969598278032962.\n",
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.5958, 'grad_norm': 9.09505844116211, 'learning_rate': 1.2651186493751045e-05, 'epoch': 1.0}\n",
            "{'eval_loss': 0.5084899663925171, 'eval_accuracy': 0.7796208530805687, 'eval_f1': 0.7796010512837792, 'eval_precision': 0.784671156014014, 'eval_recall': 0.7838361160196139, 'eval_runtime': 0.637, 'eval_samples_per_second': 2649.837, 'eval_steps_per_second': 42.385, 'epoch': 1.0}\n",
            "{'loss': 0.4984, 'grad_norm': 6.8555378913879395, 'learning_rate': 9.524521194558825e-06, 'epoch': 2.0}\n",
            "{'eval_loss': 0.4885820150375366, 'eval_accuracy': 0.79739336492891, 'eval_f1': 0.7944497852976992, 'eval_precision': 0.8004602848790197, 'eval_recall': 0.7929280892189534, 'eval_runtime': 0.6364, 'eval_samples_per_second': 2652.297, 'eval_steps_per_second': 42.424, 'epoch': 2.0}\n",
            "{'loss': 0.4313, 'grad_norm': 26.84432029724121, 'learning_rate': 6.392908640146364e-06, 'epoch': 3.0}\n",
            "{'eval_loss': 0.5143423676490784, 'eval_accuracy': 0.7861374407582938, 'eval_f1': 0.7860742993776029, 'eval_precision': 0.7875742039935241, 'eval_recall': 0.7885738985484151, 'eval_runtime': 0.6515, 'eval_samples_per_second': 2591.137, 'eval_steps_per_second': 41.446, 'epoch': 3.0}\n",
            "{'loss': 0.3686, 'grad_norm': 35.56882095336914, 'learning_rate': 3.261296085733904e-06, 'epoch': 4.0}\n",
            "{'eval_loss': 0.550650954246521, 'eval_accuracy': 0.7896919431279621, 'eval_f1': 0.7885323138956262, 'eval_precision': 0.7888463103635377, 'eval_recall': 0.788286204654718, 'eval_runtime': 0.6774, 'eval_samples_per_second': 2491.793, 'eval_steps_per_second': 39.857, 'epoch': 4.0}\n",
            "{'train_runtime': 200.9209, 'train_samples_per_second': 125.945, 'train_steps_per_second': 15.752, 'train_loss': 0.4735193659343991, 'epoch': 4.0}\n",
            "{'eval_loss': 0.4885820150375366, 'eval_accuracy': 0.79739336492891, 'eval_f1': 0.7944497852976992, 'eval_precision': 0.8004602848790197, 'eval_recall': 0.7929280892189534, 'eval_runtime': 0.6483, 'eval_samples_per_second': 2603.915, 'eval_steps_per_second': 41.65, 'epoch': 4.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-12 14:36:40,243] Trial 16 finished with value: 0.7944497852976992 and parameters: {'learning_rate': 1.5263337250038215e-05, 'weight_decay': 0.0, 'per_device_train_batch_size': 8, 'warmup_steps': 100, 'num_train_epochs': 5, 'adam_beta1': 0.8788848463720886, 'adam_beta2': 0.991783450890181, 'adam_epsilon': 2.640909787490712e-07, 'gradient_accumulation_steps': 1, 'lr_scheduler_type': 'polynomial', 'label_smoothing_factor': 0.1992664724227173}. Best is trial 6 with value: 0.7969598278032962.\n",
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.5799, 'grad_norm': 4.022329807281494, 'learning_rate': 1.7237063866292352e-05, 'epoch': 1.0}\n",
            "{'eval_loss': 0.4857659339904785, 'eval_accuracy': 0.7636255924170616, 'eval_f1': 0.7631826836356348, 'eval_precision': 0.7753596305051795, 'eval_recall': 0.7700585400663107, 'eval_runtime': 0.6499, 'eval_samples_per_second': 2597.437, 'eval_steps_per_second': 41.547, 'epoch': 1.0}\n",
            "{'loss': 0.4925, 'grad_norm': 5.0728373527526855, 'learning_rate': 1.0126631272913677e-05, 'epoch': 2.0}\n",
            "{'eval_loss': 0.46345752477645874, 'eval_accuracy': 0.7873222748815166, 'eval_f1': 0.7873096598138057, 'eval_precision': 0.7921324331812347, 'eval_recall': 0.7914522477495709, 'eval_runtime': 0.6445, 'eval_samples_per_second': 2619.165, 'eval_steps_per_second': 41.894, 'epoch': 2.0}\n",
            "{'loss': 0.4069, 'grad_norm': 10.07871150970459, 'learning_rate': 2.9690821000874455e-06, 'epoch': 3.0}\n",
            "{'eval_loss': 0.507056474685669, 'eval_accuracy': 0.7855450236966824, 'eval_f1': 0.7852166687522056, 'eval_precision': 0.7852173729914146, 'eval_recall': 0.786491643479573, 'eval_runtime': 0.6385, 'eval_samples_per_second': 2643.544, 'eval_steps_per_second': 42.284, 'epoch': 3.0}\n",
            "{'loss': 0.3441, 'grad_norm': 27.7999210357666, 'learning_rate': 7.780751616180199e-10, 'epoch': 3.9889415481832544}\n",
            "{'eval_loss': 0.5366718769073486, 'eval_accuracy': 0.7825829383886256, 'eval_f1': 0.7820811507596606, 'eval_precision': 0.7818506238057772, 'eval_recall': 0.7829130981106691, 'eval_runtime': 0.6314, 'eval_samples_per_second': 2673.495, 'eval_steps_per_second': 42.763, 'epoch': 3.9889415481832544}\n",
            "{'train_runtime': 177.0582, 'train_samples_per_second': 114.335, 'train_steps_per_second': 7.139, 'train_loss': 0.4562138062489184, 'epoch': 3.9889415481832544}\n",
            "{'eval_loss': 0.46345752477645874, 'eval_accuracy': 0.7873222748815166, 'eval_f1': 0.7873096598138057, 'eval_precision': 0.7921324331812347, 'eval_recall': 0.7914522477495709, 'eval_runtime': 0.6449, 'eval_samples_per_second': 2617.468, 'eval_steps_per_second': 41.867, 'epoch': 3.9889415481832544}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-12 14:39:40,119] Trial 17 finished with value: 0.7873096598138057 and parameters: {'learning_rate': 2.015308450549332e-05, 'weight_decay': 0.0, 'per_device_train_batch_size': 8, 'warmup_steps': 0, 'num_train_epochs': 4, 'adam_beta1': 0.8990975019683104, 'adam_beta2': 0.9600809570166652, 'adam_epsilon': 2.3036332479375665e-08, 'gradient_accumulation_steps': 2, 'lr_scheduler_type': 'cosine_with_restarts', 'label_smoothing_factor': 0.07506462118116625}. Best is trial 6 with value: 0.7969598278032962.\n",
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.5907, 'grad_norm': 13.652923583984375, 'learning_rate': 2.4451565550839856e-05, 'epoch': 1.0}\n",
            "{'eval_loss': 0.4721633493900299, 'eval_accuracy': 0.7813981042654028, 'eval_f1': 0.7811122555588004, 'eval_precision': 0.7812443845462713, 'eval_recall': 0.7825266857240367, 'eval_runtime': 0.6453, 'eval_samples_per_second': 2615.976, 'eval_steps_per_second': 41.843, 'epoch': 1.0}\n",
            "{'loss': 0.5008, 'grad_norm': 26.584318161010742, 'learning_rate': 9.106052269439911e-06, 'epoch': 2.0}\n",
            "{'eval_loss': 0.48464885354042053, 'eval_accuracy': 0.7760663507109005, 'eval_f1': 0.7757830641125836, 'eval_precision': 0.7862482770268819, 'eval_recall': 0.7820337983914527, 'eval_runtime': 0.6389, 'eval_samples_per_second': 2642.062, 'eval_steps_per_second': 42.26, 'epoch': 2.0}\n",
            "{'loss': 0.4187, 'grad_norm': 15.64554214477539, 'learning_rate': 6.229427871775657e-09, 'epoch': 3.0}\n",
            "{'eval_loss': 0.47882381081581116, 'eval_accuracy': 0.7962085308056872, 'eval_f1': 0.7952415610207455, 'eval_precision': 0.7952909974536401, 'eval_recall': 0.7951943837639105, 'eval_runtime': 0.6524, 'eval_samples_per_second': 2587.199, 'eval_steps_per_second': 41.383, 'epoch': 3.0}\n",
            "{'train_runtime': 93.5995, 'train_samples_per_second': 162.212, 'train_steps_per_second': 10.16, 'train_loss': 0.503395361353797, 'epoch': 3.0}\n",
            "{'eval_loss': 0.47882381081581116, 'eval_accuracy': 0.7962085308056872, 'eval_f1': 0.7952415610207455, 'eval_precision': 0.7952909974536401, 'eval_recall': 0.7951943837639105, 'eval_runtime': 0.6596, 'eval_samples_per_second': 2559.104, 'eval_steps_per_second': 40.934, 'epoch': 3.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-12 14:41:16,157] Trial 18 finished with value: 0.7952415610207455 and parameters: {'learning_rate': 2.8570584170972665e-05, 'weight_decay': 0.01, 'per_device_train_batch_size': 16, 'warmup_steps': 100, 'num_train_epochs': 3, 'adam_beta1': 0.9269783983244738, 'adam_beta2': 0.9715799634606495, 'adam_epsilon': 7.878653002259053e-08, 'gradient_accumulation_steps': 1, 'lr_scheduler_type': 'cosine', 'label_smoothing_factor': 0.042464824669639946}. Best is trial 6 with value: 0.7969598278032962.\n",
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.7077, 'grad_norm': 2.9954519271850586, 'learning_rate': 2.2337729392544244e-06, 'epoch': 0.9811320754716981}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.6967170238494873, 'eval_accuracy': 0.4662322274881517, 'eval_f1': 0.317979797979798, 'eval_precision': 0.23311611374407584, 'eval_recall': 0.5, 'eval_runtime': 0.6423, 'eval_samples_per_second': 2628.241, 'eval_steps_per_second': 42.039, 'epoch': 0.9811320754716981}\n",
            "{'loss': 0.6752, 'grad_norm': 3.1095805168151855, 'learning_rate': 4.526329376910281e-06, 'epoch': 1.9811320754716981}\n",
            "{'eval_loss': 0.6143624782562256, 'eval_accuracy': 0.6872037914691943, 'eval_f1': 0.6871967654986523, 'eval_precision': 0.6909368728913969, 'eval_recall': 0.690674063972404, 'eval_runtime': 0.6479, 'eval_samples_per_second': 2605.353, 'eval_steps_per_second': 41.673, 'epoch': 1.9811320754716981}\n",
            "{'loss': 0.612, 'grad_norm': 4.226070880889893, 'learning_rate': 6.760102316164706e-06, 'epoch': 2.981132075471698}\n",
            "{'eval_loss': 0.5386582612991333, 'eval_accuracy': 0.7298578199052133, 'eval_f1': 0.7298544067542014, 'eval_precision': 0.7327481383896807, 'eval_recall': 0.7329608355533243, 'eval_runtime': 0.6665, 'eval_samples_per_second': 2532.795, 'eval_steps_per_second': 40.513, 'epoch': 2.981132075471698}\n",
            "{'loss': 0.5474, 'grad_norm': 5.668299674987793, 'learning_rate': 8.99387525541913e-06, 'epoch': 3.981132075471698}\n",
            "{'eval_loss': 0.5381380915641785, 'eval_accuracy': 0.7559241706161137, 'eval_f1': 0.7545125946007003, 'eval_precision': 0.7766162176988723, 'eval_recall': 0.7645324198582121, 'eval_runtime': 0.6354, 'eval_samples_per_second': 2656.568, 'eval_steps_per_second': 42.492, 'epoch': 3.981132075471698}\n",
            "{'train_runtime': 71.035, 'train_samples_per_second': 284.986, 'train_steps_per_second': 2.196, 'train_loss': 0.635579207004645, 'epoch': 3.981132075471698}\n",
            "{'eval_loss': 0.5381380915641785, 'eval_accuracy': 0.7559241706161137, 'eval_f1': 0.7545125946007003, 'eval_precision': 0.7766162176988723, 'eval_recall': 0.7645324198582121, 'eval_runtime': 0.6569, 'eval_samples_per_second': 2569.477, 'eval_steps_per_second': 41.099, 'epoch': 3.981132075471698}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-12 14:42:31,163] Trial 19 finished with value: 0.7545125946007003 and parameters: {'learning_rate': 1.4695874600358056e-05, 'weight_decay': 0.1, 'per_device_train_batch_size': 32, 'warmup_steps': 250, 'num_train_epochs': 4, 'adam_beta1': 0.8645553447571406, 'adam_beta2': 0.9613665273857692, 'adam_epsilon': 1.0730189167123248e-07, 'gradient_accumulation_steps': 4, 'lr_scheduler_type': 'cosine_with_restarts', 'label_smoothing_factor': 0.13471057632904412}. Best is trial 6 with value: 0.7969598278032962.\n",
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.5913, 'grad_norm': 8.682401657104492, 'learning_rate': 1.5685684485790686e-05, 'epoch': 1.0}\n",
            "{'eval_loss': 0.4911617040634155, 'eval_accuracy': 0.759478672985782, 'eval_f1': 0.7587304806041737, 'eval_precision': 0.7743000717875089, 'eval_recall': 0.7667366627790384, 'eval_runtime': 0.6299, 'eval_samples_per_second': 2679.637, 'eval_steps_per_second': 42.861, 'epoch': 1.0}\n",
            "{'loss': 0.4914, 'grad_norm': 8.156763076782227, 'learning_rate': 7.95381909925603e-06, 'epoch': 2.0}\n",
            "{'eval_loss': 0.45283833146095276, 'eval_accuracy': 0.7997630331753555, 'eval_f1': 0.7992419284237432, 'eval_precision': 0.7989557199980323, 'eval_recall': 0.7999709485577933, 'eval_runtime': 0.6355, 'eval_samples_per_second': 2656.344, 'eval_steps_per_second': 42.489, 'epoch': 2.0}\n",
            "{'loss': 0.4147, 'grad_norm': 21.711210250854492, 'learning_rate': 2.2195371272136673e-07, 'epoch': 3.0}\n",
            "{'eval_loss': 0.4893409013748169, 'eval_accuracy': 0.7885071090047393, 'eval_f1': 0.7879175328699739, 'eval_precision': 0.7876403041140878, 'eval_recall': 0.7885428727363497, 'eval_runtime': 0.6543, 'eval_samples_per_second': 2579.905, 'eval_steps_per_second': 41.266, 'epoch': 3.0}\n",
            "{'train_runtime': 90.7228, 'train_samples_per_second': 167.356, 'train_steps_per_second': 10.482, 'train_loss': 0.49913071432825645, 'epoch': 3.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-12 14:44:04,476] Trial 20 finished with value: 0.7992419284237432 and parameters: {'learning_rate': 2.0856521905176643e-05, 'weight_decay': 0.01, 'per_device_train_batch_size': 16, 'warmup_steps': 100, 'num_train_epochs': 3, 'adam_beta1': 0.8524638726782257, 'adam_beta2': 0.9796537634211028, 'adam_epsilon': 3.374369216426795e-08, 'gradient_accumulation_steps': 1, 'lr_scheduler_type': 'polynomial', 'label_smoothing_factor': 0.11187986994691718}. Best is trial 20 with value: 0.7992419284237432.\n",
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.45283833146095276, 'eval_accuracy': 0.7997630331753555, 'eval_f1': 0.7992419284237432, 'eval_precision': 0.7989557199980323, 'eval_recall': 0.7999709485577933, 'eval_runtime': 0.6436, 'eval_samples_per_second': 2622.845, 'eval_steps_per_second': 41.953, 'epoch': 3.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.5942, 'grad_norm': 29.854528427124023, 'learning_rate': 1.557861833990695e-05, 'epoch': 1.0}\n",
            "{'eval_loss': 0.4911644160747528, 'eval_accuracy': 0.7582938388625592, 'eval_f1': 0.7575095206976588, 'eval_precision': 0.7733997941726772, 'eval_recall': 0.7656267848656089, 'eval_runtime': 0.6358, 'eval_samples_per_second': 2654.92, 'eval_steps_per_second': 42.466, 'epoch': 1.0}\n",
            "{'loss': 0.4816, 'grad_norm': 12.93538761138916, 'learning_rate': 7.912092641771221e-06, 'epoch': 2.0}\n",
            "{'eval_loss': 0.46275845170021057, 'eval_accuracy': 0.7938388625592417, 'eval_f1': 0.7938362577861011, 'eval_precision': 0.7980199529340017, 'eval_recall': 0.7977173463904994, 'eval_runtime': 0.6351, 'eval_samples_per_second': 2657.79, 'eval_steps_per_second': 42.512, 'epoch': 2.0}\n",
            "{'loss': 0.4073, 'grad_norm': 18.7038516998291, 'learning_rate': 2.2130578636290698e-07, 'epoch': 3.0}\n",
            "{'eval_loss': 0.4841189384460449, 'eval_accuracy': 0.7867298578199052, 'eval_f1': 0.786094449607164, 'eval_precision': 0.7858210602845407, 'eval_recall': 0.7866369006906064, 'eval_runtime': 0.6315, 'eval_samples_per_second': 2672.981, 'eval_steps_per_second': 42.755, 'epoch': 3.0}\n",
            "{'train_runtime': 89.6876, 'train_samples_per_second': 169.288, 'train_steps_per_second': 10.603, 'train_loss': 0.49434044935224436, 'epoch': 3.0}\n",
            "{'eval_loss': 0.46275845170021057, 'eval_accuracy': 0.7938388625592417, 'eval_f1': 0.7938362577861011, 'eval_precision': 0.7980199529340017, 'eval_recall': 0.7977173463904994, 'eval_runtime': 0.6538, 'eval_samples_per_second': 2581.867, 'eval_steps_per_second': 41.298, 'epoch': 3.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-12 14:45:36,999] Trial 21 finished with value: 0.7938362577861011 and parameters: {'learning_rate': 2.0746244838966793e-05, 'weight_decay': 0.01, 'per_device_train_batch_size': 16, 'warmup_steps': 100, 'num_train_epochs': 3, 'adam_beta1': 0.8561803423284401, 'adam_beta2': 0.9818979173712771, 'adam_epsilon': 3.1226774156670264e-08, 'gradient_accumulation_steps': 1, 'lr_scheduler_type': 'polynomial', 'label_smoothing_factor': 0.11959921603136331}. Best is trial 20 with value: 0.7992419284237432.\n",
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.5908, 'grad_norm': 25.15877342224121, 'learning_rate': 1.3772378962669604e-05, 'epoch': 1.0}\n",
            "{'eval_loss': 0.5035487413406372, 'eval_accuracy': 0.7796208530805687, 'eval_f1': 0.7778192934782608, 'eval_precision': 0.7793593445858233, 'eval_recall': 0.7770837711028407, 'eval_runtime': 0.6472, 'eval_samples_per_second': 2607.987, 'eval_steps_per_second': 41.715, 'epoch': 1.0}\n",
            "{'loss': 0.4887, 'grad_norm': 7.787574768066406, 'learning_rate': 6.98968079183038e-06, 'epoch': 2.0}\n",
            "{'eval_loss': 0.47044637799263, 'eval_accuracy': 0.7813981042654028, 'eval_f1': 0.7813974137825141, 'eval_precision': 0.7847241408474528, 'eval_recall': 0.7848578524214942, 'eval_runtime': 0.6415, 'eval_samples_per_second': 2631.479, 'eval_steps_per_second': 42.091, 'epoch': 2.0}\n",
            "{'loss': 0.418, 'grad_norm': 28.228696823120117, 'learning_rate': 2.7117219358584735e-07, 'epoch': 3.0}\n",
            "{'eval_loss': 0.48935848474502563, 'eval_accuracy': 0.7902843601895735, 'eval_f1': 0.789788293897883, 'eval_precision': 0.7895381692850048, 'eval_recall': 0.790609614899159, 'eval_runtime': 0.6476, 'eval_samples_per_second': 2606.453, 'eval_steps_per_second': 41.691, 'epoch': 3.0}\n",
            "{'train_runtime': 93.0538, 'train_samples_per_second': 163.164, 'train_steps_per_second': 10.22, 'train_loss': 0.4991598359919246, 'epoch': 3.0}\n",
            "{'eval_loss': 0.48935848474502563, 'eval_accuracy': 0.7902843601895735, 'eval_f1': 0.789788293897883, 'eval_precision': 0.7895381692850048, 'eval_recall': 0.790609614899159, 'eval_runtime': 0.6597, 'eval_samples_per_second': 2558.925, 'eval_steps_per_second': 40.931, 'epoch': 3.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-12 14:47:13,587] Trial 22 finished with value: 0.789788293897883 and parameters: {'learning_rate': 1.8308442092694574e-05, 'weight_decay': 0.01, 'per_device_train_batch_size': 16, 'warmup_steps': 100, 'num_train_epochs': 3, 'adam_beta1': 0.8710453819595462, 'adam_beta2': 0.9930510060201282, 'adam_epsilon': 1.4775116392243076e-08, 'gradient_accumulation_steps': 1, 'lr_scheduler_type': 'polynomial', 'label_smoothing_factor': 0.14607132646416104}. Best is trial 20 with value: 0.7992419284237432.\n",
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.595, 'grad_norm': 66.34188079833984, 'learning_rate': 1.8418727065041777e-05, 'epoch': 1.0}\n",
            "{'eval_loss': 0.4884870648384094, 'eval_accuracy': 0.768957345971564, 'eval_f1': 0.7686650661479038, 'eval_precision': 0.7789999813926061, 'eval_recall': 0.7748922205596775, 'eval_runtime': 0.6542, 'eval_samples_per_second': 2580.059, 'eval_steps_per_second': 41.269, 'epoch': 1.0}\n",
            "{'loss': 0.4929, 'grad_norm': 7.187110900878906, 'learning_rate': 9.31678900921381e-06, 'epoch': 2.0}\n",
            "{'eval_loss': 0.4541196823120117, 'eval_accuracy': 0.7997630331753555, 'eval_f1': 0.7993980648064807, 'eval_precision': 0.7992574191383085, 'eval_recall': 0.8005336439675245, 'eval_runtime': 0.6393, 'eval_samples_per_second': 2640.525, 'eval_steps_per_second': 42.236, 'epoch': 2.0}\n",
            "{'loss': 0.4095, 'grad_norm': 20.513214111328125, 'learning_rate': 2.4356369173230216e-07, 'epoch': 3.0}\n",
            "{'eval_loss': 0.46902936697006226, 'eval_accuracy': 0.7962085308056872, 'eval_f1': 0.7958772743890435, 'eval_precision': 0.7958186783485324, 'eval_recall': 0.7971236251687028, 'eval_runtime': 0.6417, 'eval_samples_per_second': 2630.501, 'eval_steps_per_second': 42.076, 'epoch': 3.0}\n",
            "{'train_runtime': 88.6528, 'train_samples_per_second': 171.264, 'train_steps_per_second': 10.727, 'train_loss': 0.4991581191775927, 'epoch': 3.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-12 14:48:44,683] Trial 23 finished with value: 0.7993980648064807 and parameters: {'learning_rate': 2.4534540332837856e-05, 'weight_decay': 0.01, 'per_device_train_batch_size': 16, 'warmup_steps': 100, 'num_train_epochs': 3, 'adam_beta1': 0.8527636952750518, 'adam_beta2': 0.9790399250426908, 'adam_epsilon': 3.3795597718775974e-08, 'gradient_accumulation_steps': 1, 'lr_scheduler_type': 'polynomial', 'label_smoothing_factor': 0.09139418943354574}. Best is trial 23 with value: 0.7993980648064807.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.4541196823120117, 'eval_accuracy': 0.7997630331753555, 'eval_f1': 0.7993980648064807, 'eval_precision': 0.7992574191383085, 'eval_recall': 0.8005336439675245, 'eval_runtime': 0.656, 'eval_samples_per_second': 2573.174, 'eval_steps_per_second': 41.159, 'epoch': 3.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.5948, 'grad_norm': 22.72905731201172, 'learning_rate': 1.8773815861900077e-05, 'epoch': 1.0}\n",
            "{'eval_loss': 0.4950720965862274, 'eval_accuracy': 0.7523696682464455, 'eval_f1': 0.7509375352987688, 'eval_precision': 0.7728823157270712, 'eval_recall': 0.7609616309423244, 'eval_runtime': 0.6361, 'eval_samples_per_second': 2653.486, 'eval_steps_per_second': 42.443, 'epoch': 1.0}\n",
            "{'loss': 0.4928, 'grad_norm': 11.095316886901855, 'learning_rate': 9.55398514638515e-06, 'epoch': 2.0}\n",
            "{'eval_loss': 0.4701991081237793, 'eval_accuracy': 0.7861374407582938, 'eval_f1': 0.7860977283537189, 'eval_precision': 0.7879650721336371, 'eval_recall': 0.7888150537240142, 'eval_runtime': 0.6428, 'eval_samples_per_second': 2626.067, 'eval_steps_per_second': 42.005, 'epoch': 2.0}\n",
            "{'loss': 0.4171, 'grad_norm': 21.025463104248047, 'learning_rate': 2.75615823152664e-07, 'epoch': 3.0}\n",
            "{'eval_loss': 0.4823976159095764, 'eval_accuracy': 0.794431279620853, 'eval_f1': 0.7938320973522939, 'eval_precision': 0.7935473438510644, 'eval_recall': 0.7944138025376293, 'eval_runtime': 0.6443, 'eval_samples_per_second': 2619.898, 'eval_steps_per_second': 41.906, 'epoch': 3.0}\n",
            "{'train_runtime': 90.5891, 'train_samples_per_second': 167.603, 'train_steps_per_second': 10.498, 'train_loss': 0.501557118509094, 'epoch': 3.0}\n",
            "{'eval_loss': 0.4823976159095764, 'eval_accuracy': 0.794431279620853, 'eval_f1': 0.7938320973522939, 'eval_precision': 0.7935473438510644, 'eval_recall': 0.7944138025376293, 'eval_runtime': 0.6654, 'eval_samples_per_second': 2536.985, 'eval_steps_per_second': 40.58, 'epoch': 3.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-12 14:50:17,788] Trial 24 finished with value: 0.7938320973522939 and parameters: {'learning_rate': 2.5008177583819698e-05, 'weight_decay': 0.01, 'per_device_train_batch_size': 16, 'warmup_steps': 100, 'num_train_epochs': 3, 'adam_beta1': 0.8513308501400199, 'adam_beta2': 0.979987011813941, 'adam_epsilon': 4.25778676161665e-08, 'gradient_accumulation_steps': 1, 'lr_scheduler_type': 'polynomial', 'label_smoothing_factor': 0.10627864109409951}. Best is trial 23 with value: 0.7993980648064807.\n",
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.6071, 'grad_norm': 12.777227401733398, 'learning_rate': 2.623808009376577e-05, 'epoch': 1.0}\n",
            "{'eval_loss': 0.49238479137420654, 'eval_accuracy': 0.759478672985782, 'eval_f1': 0.7568351613415991, 'eval_precision': 0.7598839832420239, 'eval_recall': 0.7558846798770813, 'eval_runtime': 0.6502, 'eval_samples_per_second': 2595.929, 'eval_steps_per_second': 41.523, 'epoch': 1.0}\n",
            "{'loss': 0.4977, 'grad_norm': 9.563517570495605, 'learning_rate': 5.058708089094075e-07, 'epoch': 2.0}\n",
            "{'eval_loss': 0.47175121307373047, 'eval_accuracy': 0.7879146919431279, 'eval_f1': 0.7878643632743444, 'eval_precision': 0.7940794985307188, 'eval_recall': 0.7925698821160168, 'eval_runtime': 0.642, 'eval_samples_per_second': 2629.169, 'eval_steps_per_second': 42.054, 'epoch': 2.0}\n",
            "{'train_runtime': 60.9309, 'train_samples_per_second': 166.123, 'train_steps_per_second': 10.405, 'train_loss': 0.5523991629904377, 'epoch': 2.0}\n",
            "{'eval_loss': 0.47175121307373047, 'eval_accuracy': 0.7879146919431279, 'eval_f1': 0.7878643632743444, 'eval_precision': 0.7940794985307188, 'eval_recall': 0.7925698821160168, 'eval_runtime': 0.6448, 'eval_samples_per_second': 2617.884, 'eval_steps_per_second': 41.874, 'epoch': 2.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-12 14:51:21,147] Trial 25 finished with value: 0.7878643632743444 and parameters: {'learning_rate': 3.127087812424241e-05, 'weight_decay': 0.0, 'per_device_train_batch_size': 16, 'warmup_steps': 250, 'num_train_epochs': 2, 'adam_beta1': 0.8501785934486464, 'adam_beta2': 0.9726389433835627, 'adam_epsilon': 9.577666429982576e-08, 'gradient_accumulation_steps': 1, 'lr_scheduler_type': 'polynomial', 'label_smoothing_factor': 0.09240030077399394}. Best is trial 23 with value: 0.7993980648064807.\n",
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.5843, 'grad_norm': 8.336485862731934, 'learning_rate': 1.6260689287713973e-05, 'epoch': 1.0}\n",
            "{'eval_loss': 0.4923865497112274, 'eval_accuracy': 0.768957345971564, 'eval_f1': 0.7687378804485288, 'eval_precision': 0.7779543296857043, 'eval_recall': 0.7745706803255454, 'eval_runtime': 0.6336, 'eval_samples_per_second': 2664.246, 'eval_steps_per_second': 42.615, 'epoch': 1.0}\n",
            "{'loss': 0.4946, 'grad_norm': 9.538061141967773, 'learning_rate': 5.564958480506138e-06, 'epoch': 2.0}\n",
            "{'eval_loss': 0.4728742837905884, 'eval_accuracy': 0.784952606635071, 'eval_f1': 0.7848256623062491, 'eval_precision': 0.7928214454443514, 'eval_recall': 0.7901971126251081, 'eval_runtime': 0.6475, 'eval_samples_per_second': 2606.988, 'eval_steps_per_second': 41.699, 'epoch': 2.0}\n",
            "{'loss': 0.4253, 'grad_norm': 15.291380882263184, 'learning_rate': 2.860302941814707e-09, 'epoch': 3.0}\n",
            "{'eval_loss': 0.4746529757976532, 'eval_accuracy': 0.7896919431279621, 'eval_f1': 0.7896458022009191, 'eval_precision': 0.7913911195087387, 'eval_recall': 0.7923054575813687, 'eval_runtime': 0.6566, 'eval_samples_per_second': 2570.692, 'eval_steps_per_second': 41.119, 'epoch': 3.0}\n",
            "{'train_runtime': 90.3835, 'train_samples_per_second': 167.984, 'train_steps_per_second': 10.522, 'train_loss': 0.5014166445887552, 'epoch': 3.0}\n",
            "{'eval_loss': 0.4746529757976532, 'eval_accuracy': 0.7896919431279621, 'eval_f1': 0.7896458022009191, 'eval_precision': 0.7913911195087387, 'eval_recall': 0.7923054575813687, 'eval_runtime': 0.6789, 'eval_samples_per_second': 2486.257, 'eval_steps_per_second': 39.768, 'epoch': 3.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-12 14:52:54,058] Trial 26 finished with value: 0.7896458022009191 and parameters: {'learning_rate': 2.13971822202993e-05, 'weight_decay': 0.01, 'per_device_train_batch_size': 16, 'warmup_steps': 0, 'num_train_epochs': 3, 'adam_beta1': 0.8592934156163953, 'adam_beta2': 0.9868368886149652, 'adam_epsilon': 1.5798867526054843e-07, 'gradient_accumulation_steps': 1, 'lr_scheduler_type': 'cosine_with_restarts', 'label_smoothing_factor': 0.11319999697263325}. Best is trial 23 with value: 0.7993980648064807.\n",
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.6182, 'grad_norm': 8.608611106872559, 'learning_rate': 1.708541691404521e-05, 'epoch': 1.0}\n",
            "{'eval_loss': 0.5057183504104614, 'eval_accuracy': 0.768957345971564, 'eval_f1': 0.7689560485840872, 'eval_precision': 0.7728230421314819, 'eval_recall': 0.772641438920753, 'eval_runtime': 0.6341, 'eval_samples_per_second': 2661.873, 'eval_steps_per_second': 42.577, 'epoch': 1.0}\n",
            "{'loss': 0.5227, 'grad_norm': 18.05145835876465, 'learning_rate': 2.2218503289737214e-05, 'epoch': 2.0}\n",
            "{'eval_loss': 0.4667387902736664, 'eval_accuracy': 0.8033175355450237, 'eval_f1': 0.8031307084843886, 'eval_precision': 0.803528234156148, 'eval_recall': 0.8049082834687422, 'eval_runtime': 0.6418, 'eval_samples_per_second': 2630.127, 'eval_steps_per_second': 42.07, 'epoch': 2.0}\n",
            "{'loss': 0.4395, 'grad_norm': 35.938720703125, 'learning_rate': 1.191728150203548e-08, 'epoch': 3.0}\n",
            "{'eval_loss': 0.4624094069004059, 'eval_accuracy': 0.7968009478672986, 'eval_f1': 0.7961552910078058, 'eval_precision': 0.7958803868942118, 'eval_recall': 0.7966335583644883, 'eval_runtime': 0.6388, 'eval_samples_per_second': 2642.415, 'eval_steps_per_second': 42.266, 'epoch': 3.0}\n",
            "{'train_runtime': 90.9124, 'train_samples_per_second': 167.007, 'train_steps_per_second': 10.461, 'train_loss': 0.5267668201594949, 'epoch': 3.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-12 14:54:27,523] Trial 27 finished with value: 0.8031307084843886 and parameters: {'learning_rate': 2.7292998265247943e-05, 'weight_decay': 0.01, 'per_device_train_batch_size': 16, 'warmup_steps': 500, 'num_train_epochs': 3, 'adam_beta1': 0.885138018556435, 'adam_beta2': 0.9787316893607377, 'adam_epsilon': 1.4762098191497374e-08, 'gradient_accumulation_steps': 1, 'lr_scheduler_type': 'cosine', 'label_smoothing_factor': 0.14647147934802351}. Best is trial 27 with value: 0.8031307084843886.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.4667387902736664, 'eval_accuracy': 0.8033175355450237, 'eval_f1': 0.8031307084843886, 'eval_precision': 0.803528234156148, 'eval_recall': 0.8049082834687422, 'eval_runtime': 0.7129, 'eval_samples_per_second': 2367.677, 'eval_steps_per_second': 37.872, 'epoch': 3.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.6123, 'grad_norm': 17.456485748291016, 'learning_rate': 2.2351143545205924e-05, 'epoch': 1.0}\n",
            "{'eval_loss': 0.5221490263938904, 'eval_accuracy': 0.7701421800947867, 'eval_f1': 0.7701421800947867, 'eval_precision': 0.7736709317756496, 'eval_recall': 0.7736709317756496, 'eval_runtime': 0.643, 'eval_samples_per_second': 2625.233, 'eval_steps_per_second': 41.991, 'epoch': 1.0}\n",
            "{'loss': 0.5305, 'grad_norm': 5.4606709480285645, 'learning_rate': 2.414129929968429e-07, 'epoch': 2.0}\n",
            "{'eval_loss': 0.47254717350006104, 'eval_accuracy': 0.7908767772511849, 'eval_f1': 0.7907004109320067, 'eval_precision': 0.7912230159264999, 'eval_recall': 0.7925310998509351, 'eval_runtime': 0.6526, 'eval_samples_per_second': 2586.749, 'eval_steps_per_second': 41.376, 'epoch': 2.0}\n",
            "{'train_runtime': 61.5618, 'train_samples_per_second': 164.42, 'train_steps_per_second': 10.299, 'train_loss': 0.5714393399115241, 'epoch': 2.0}\n",
            "{'eval_loss': 0.47254717350006104, 'eval_accuracy': 0.7908767772511849, 'eval_f1': 0.7907004109320067, 'eval_precision': 0.7912230159264999, 'eval_recall': 0.7925310998509351, 'eval_runtime': 0.6532, 'eval_samples_per_second': 2584.192, 'eval_steps_per_second': 41.335, 'epoch': 2.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-12 14:55:31,788] Trial 28 finished with value: 0.7907004109320067 and parameters: {'learning_rate': 3.593431438136001e-05, 'weight_decay': 0.01, 'per_device_train_batch_size': 16, 'warmup_steps': 500, 'num_train_epochs': 2, 'adam_beta1': 0.8849646962589647, 'adam_beta2': 0.9788183319639149, 'adam_epsilon': 1.5735819759856956e-08, 'gradient_accumulation_steps': 1, 'lr_scheduler_type': 'cosine', 'label_smoothing_factor': 0.1501277928933192}. Best is trial 27 with value: 0.8031307084843886.\n",
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.6238, 'grad_norm': 8.948413848876953, 'learning_rate': 1.7162565553103765e-05, 'epoch': 1.0}\n",
            "{'eval_loss': 0.5018043518066406, 'eval_accuracy': 0.7760663507109005, 'eval_f1': 0.7760132103846429, 'eval_precision': 0.7776812510982253, 'eval_recall': 0.7785772408745331, 'eval_runtime': 0.6521, 'eval_samples_per_second': 2588.669, 'eval_steps_per_second': 41.406, 'epoch': 1.0}\n",
            "{'loss': 0.5199, 'grad_norm': 4.656665325164795, 'learning_rate': 2.2390364923992385e-05, 'epoch': 2.0}\n",
            "{'eval_loss': 0.502757728099823, 'eval_accuracy': 0.7796208530805687, 'eval_f1': 0.7770287832626712, 'eval_precision': 0.7807785437577417, 'eval_recall': 0.7758779952248456, 'eval_runtime': 0.6831, 'eval_samples_per_second': 2471.103, 'eval_steps_per_second': 39.526, 'epoch': 2.0}\n",
            "{'loss': 0.4391, 'grad_norm': 16.146484375, 'learning_rate': 1.6345353557337798e-08, 'epoch': 3.0}\n",
            "{'eval_loss': 0.47443366050720215, 'eval_accuracy': 0.7896919431279621, 'eval_f1': 0.7892065627239222, 'eval_precision': 0.7889653815892999, 'eval_recall': 0.7900546759424443, 'eval_runtime': 0.6351, 'eval_samples_per_second': 2657.803, 'eval_steps_per_second': 42.512, 'epoch': 3.0}\n",
            "{'train_runtime': 90.8026, 'train_samples_per_second': 167.209, 'train_steps_per_second': 10.473, 'train_loss': 0.5275778945938897, 'epoch': 3.0}\n",
            "{'eval_loss': 0.47443366050720215, 'eval_accuracy': 0.7896919431279621, 'eval_f1': 0.7892065627239222, 'eval_precision': 0.7889653815892999, 'eval_recall': 0.7900546759424443, 'eval_runtime': 0.654, 'eval_samples_per_second': 2580.977, 'eval_steps_per_second': 41.283, 'epoch': 3.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-12 14:57:05,024] Trial 29 finished with value: 0.7892065627239222 and parameters: {'learning_rate': 2.7504111463307313e-05, 'weight_decay': 0.01, 'per_device_train_batch_size': 16, 'warmup_steps': 500, 'num_train_epochs': 3, 'adam_beta1': 0.8747923655576253, 'adam_beta2': 0.973471201153547, 'adam_epsilon': 1.4352436660021764e-08, 'gradient_accumulation_steps': 1, 'lr_scheduler_type': 'cosine', 'label_smoothing_factor': 0.1930055013977967}. Best is trial 27 with value: 0.8031307084843886.\n",
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.6183, 'grad_norm': 8.531578063964844, 'learning_rate': 1.9322760216515534e-05, 'epoch': 1.0}\n",
            "{'eval_loss': 0.49057817459106445, 'eval_accuracy': 0.783175355450237, 'eval_f1': 0.7828824459673168, 'eval_precision': 0.7829852409972611, 'eval_recall': 0.7842718876527139, 'eval_runtime': 0.6364, 'eval_samples_per_second': 2652.408, 'eval_steps_per_second': 42.426, 'epoch': 1.0}\n",
            "{'loss': 0.5306, 'grad_norm': 15.874551773071289, 'learning_rate': 2.889163576542732e-05, 'epoch': 2.0}\n",
            "{'eval_loss': 0.49358534812927246, 'eval_accuracy': 0.7559241706161137, 'eval_f1': 0.7537668444047897, 'eval_precision': 0.7828830330080121, 'eval_recall': 0.7657381957362073, 'eval_runtime': 0.6678, 'eval_samples_per_second': 2527.829, 'eval_steps_per_second': 40.433, 'epoch': 2.0}\n",
            "{'loss': 0.4574, 'grad_norm': 18.140249252319336, 'learning_rate': 1.1659518242013046e-05, 'epoch': 3.0}\n",
            "{'eval_loss': 0.4625145494937897, 'eval_accuracy': 0.7908767772511849, 'eval_f1': 0.7908767038576963, 'eval_precision': 0.7945885027899551, 'eval_recall': 0.7945407263142604, 'eval_runtime': 0.6424, 'eval_samples_per_second': 2627.677, 'eval_steps_per_second': 42.03, 'epoch': 3.0}\n",
            "{'loss': 0.3523, 'grad_norm': 19.519916534423828, 'learning_rate': 6.346997024277333e-09, 'epoch': 4.0}\n",
            "{'eval_loss': 0.507794976234436, 'eval_accuracy': 0.7956161137440758, 'eval_f1': 0.7952118335180094, 'eval_precision': 0.7950301399414617, 'eval_recall': 0.796247145977856, 'eval_runtime': 0.656, 'eval_samples_per_second': 2573.003, 'eval_steps_per_second': 41.156, 'epoch': 4.0}\n",
            "{'train_runtime': 120.2985, 'train_samples_per_second': 168.281, 'train_steps_per_second': 10.54, 'train_loss': 0.4896477479663933, 'epoch': 4.0}\n",
            "{'eval_loss': 0.507794976234436, 'eval_accuracy': 0.7956161137440758, 'eval_f1': 0.7952118335180094, 'eval_precision': 0.7950301399414617, 'eval_recall': 0.796247145977856, 'eval_runtime': 0.6528, 'eval_samples_per_second': 2585.76, 'eval_steps_per_second': 41.36, 'epoch': 4.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-12 14:59:07,803] Trial 30 finished with value: 0.7952118335180094 and parameters: {'learning_rate': 3.096596188544156e-05, 'weight_decay': 0.01, 'per_device_train_batch_size': 16, 'warmup_steps': 500, 'num_train_epochs': 4, 'adam_beta1': 0.9454036311506979, 'adam_beta2': 0.9897830014431531, 'adam_epsilon': 3.1344953288070726e-08, 'gradient_accumulation_steps': 1, 'lr_scheduler_type': 'cosine', 'label_smoothing_factor': 0.08316960886257073}. Best is trial 27 with value: 0.8031307084843886.\n",
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.623, 'grad_norm': 13.920486450195312, 'learning_rate': 1.4500439247726658e-05, 'epoch': 1.0}\n",
            "{'eval_loss': 0.4840097427368164, 'eval_accuracy': 0.7837677725118484, 'eval_f1': 0.7831649845869482, 'eval_precision': 0.7828941449040646, 'eval_recall': 0.7837818208484996, 'eval_runtime': 0.6354, 'eval_samples_per_second': 2656.577, 'eval_steps_per_second': 42.493, 'epoch': 1.0}\n",
            "{'loss': 0.5245, 'grad_norm': 5.765153408050537, 'learning_rate': 1.66197305054579e-05, 'epoch': 2.0}\n",
            "{'eval_loss': 0.4716498553752899, 'eval_accuracy': 0.792654028436019, 'eval_f1': 0.7912992855850968, 'eval_precision': 0.7920520609832955, 'eval_recall': 0.7908197442626927, 'eval_runtime': 0.6466, 'eval_samples_per_second': 2610.53, 'eval_steps_per_second': 41.756, 'epoch': 2.0}\n",
            "{'loss': 0.4344, 'grad_norm': 26.534873962402344, 'learning_rate': 3.5651755443257654e-07, 'epoch': 3.0}\n",
            "{'eval_loss': 0.48021528124809265, 'eval_accuracy': 0.7902843601895735, 'eval_f1': 0.7893899854929642, 'eval_precision': 0.7893047058110544, 'eval_recall': 0.7894842240796969, 'eval_runtime': 0.6492, 'eval_samples_per_second': 2600.257, 'eval_steps_per_second': 41.592, 'epoch': 3.0}\n",
            "{'train_runtime': 89.0483, 'train_samples_per_second': 170.503, 'train_steps_per_second': 10.68, 'train_loss': 0.5273243836925359, 'epoch': 3.0}\n",
            "{'eval_loss': 0.4716498553752899, 'eval_accuracy': 0.792654028436019, 'eval_f1': 0.7912992855850968, 'eval_precision': 0.7920520609832955, 'eval_recall': 0.7908197442626927, 'eval_runtime': 0.6735, 'eval_samples_per_second': 2506.252, 'eval_steps_per_second': 40.088, 'epoch': 3.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-12 15:00:39,724] Trial 31 finished with value: 0.7912992855850968 and parameters: {'learning_rate': 2.3237883409818362e-05, 'weight_decay': 0.0, 'per_device_train_batch_size': 16, 'warmup_steps': 500, 'num_train_epochs': 3, 'adam_beta1': 0.8710947890213462, 'adam_beta2': 0.9807246575969624, 'adam_epsilon': 1.710935464384941e-08, 'gradient_accumulation_steps': 1, 'lr_scheduler_type': 'polynomial', 'label_smoothing_factor': 0.14316047886827438}. Best is trial 27 with value: 0.8031307084843886.\n",
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.6024, 'grad_norm': 14.921982765197754, 'learning_rate': 2.4916703489335568e-05, 'epoch': 1.0}\n",
            "{'eval_loss': 0.49161213636398315, 'eval_accuracy': 0.7648104265402843, 'eval_f1': 0.7647502382736875, 'eval_precision': 0.7708541288566244, 'eval_recall': 0.7693999466920138, 'eval_runtime': 0.6495, 'eval_samples_per_second': 2598.906, 'eval_steps_per_second': 41.57, 'epoch': 1.0}\n",
            "{'loss': 0.5015, 'grad_norm': 8.958495140075684, 'learning_rate': 1.1090996524952576e-05, 'epoch': 2.0}\n",
            "{'eval_loss': 0.46484678983688354, 'eval_accuracy': 0.794431279620853, 'eval_f1': 0.794352682607421, 'eval_precision': 0.8014600219244685, 'eval_recall': 0.7993976761666763, 'eval_runtime': 0.6293, 'eval_samples_per_second': 2682.186, 'eval_steps_per_second': 42.902, 'epoch': 2.0}\n",
            "{'loss': 0.3952, 'grad_norm': 35.9017333984375, 'learning_rate': 4.594672671010956e-09, 'epoch': 3.0}\n",
            "{'eval_loss': 0.4796874523162842, 'eval_accuracy': 0.792654028436019, 'eval_f1': 0.7921392905996859, 'eval_precision': 0.7918724801751529, 'eval_recall': 0.7929097557845511, 'eval_runtime': 0.6396, 'eval_samples_per_second': 2639.286, 'eval_steps_per_second': 42.216, 'epoch': 3.0}\n",
            "{'train_runtime': 94.9543, 'train_samples_per_second': 159.898, 'train_steps_per_second': 10.015, 'train_loss': 0.49968445915278326, 'epoch': 3.0}\n",
            "{'eval_loss': 0.46484678983688354, 'eval_accuracy': 0.794431279620853, 'eval_f1': 0.794352682607421, 'eval_precision': 0.8014600219244685, 'eval_recall': 0.7993976761666763, 'eval_runtime': 0.6542, 'eval_samples_per_second': 2580.202, 'eval_steps_per_second': 41.271, 'epoch': 3.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-12 15:02:17,526] Trial 32 finished with value: 0.794352682607421 and parameters: {'learning_rate': 2.5419940311121703e-05, 'weight_decay': 0.1, 'per_device_train_batch_size': 16, 'warmup_steps': 250, 'num_train_epochs': 3, 'adam_beta1': 0.85809698205235, 'adam_beta2': 0.9943969847743418, 'adam_epsilon': 3.684761508161111e-08, 'gradient_accumulation_steps': 1, 'lr_scheduler_type': 'cosine', 'label_smoothing_factor': 0.12526032422321864}. Best is trial 27 with value: 0.8031307084843886.\n",
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.6249, 'grad_norm': 14.007655143737793, 'learning_rate': 1.267428260876875e-05, 'epoch': 1.0}\n",
            "{'eval_loss': 0.4970439076423645, 'eval_accuracy': 0.7553317535545023, 'eval_f1': 0.7548733041140782, 'eval_precision': 0.7668642405627548, 'eval_recall': 0.7617266992625729, 'eval_runtime': 0.6369, 'eval_samples_per_second': 2650.344, 'eval_steps_per_second': 42.393, 'epoch': 1.0}\n",
            "{'loss': 0.5246, 'grad_norm': 10.032252311706543, 'learning_rate': 1.4546709884843889e-05, 'epoch': 2.0}\n",
            "{'eval_loss': 0.4624912440776825, 'eval_accuracy': 0.7837677725118484, 'eval_f1': 0.7837640539150912, 'eval_precision': 0.7879842991076471, 'eval_recall': 0.7876403036580842, 'eval_runtime': 0.6327, 'eval_samples_per_second': 2667.915, 'eval_steps_per_second': 42.674, 'epoch': 2.0}\n",
            "{'loss': 0.4459, 'grad_norm': 22.875661849975586, 'learning_rate': 2.7021752108069143e-07, 'epoch': 3.0}\n",
            "{'eval_loss': 0.4661669135093689, 'eval_accuracy': 0.7914691943127962, 'eval_f1': 0.7910456159304086, 'eval_precision': 0.7908564205203834, 'eval_recall': 0.7920410330467207, 'eval_runtime': 0.6418, 'eval_samples_per_second': 2629.963, 'eval_steps_per_second': 42.067, 'epoch': 3.0}\n",
            "{'train_runtime': 89.6312, 'train_samples_per_second': 169.394, 'train_steps_per_second': 10.61, 'train_loss': 0.5318334441079702, 'epoch': 3.0}\n",
            "{'eval_loss': 0.4661669135093689, 'eval_accuracy': 0.7914691943127962, 'eval_f1': 0.7910456159304086, 'eval_precision': 0.7908564205203834, 'eval_recall': 0.7920410330467207, 'eval_runtime': 0.6922, 'eval_samples_per_second': 2438.763, 'eval_steps_per_second': 39.009, 'epoch': 3.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-12 15:03:49,647] Trial 33 finished with value: 0.7910456159304086 and parameters: {'learning_rate': 2.0311350334565305e-05, 'weight_decay': 0.01, 'per_device_train_batch_size': 16, 'warmup_steps': 500, 'num_train_epochs': 3, 'adam_beta1': 0.8943235609486166, 'adam_beta2': 0.9841462263488587, 'adam_epsilon': 6.557636497028676e-08, 'gradient_accumulation_steps': 1, 'lr_scheduler_type': 'linear', 'label_smoothing_factor': 0.1784487153432881}. Best is trial 27 with value: 0.8031307084843886.\n",
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.6006, 'grad_norm': 12.07036018371582, 'learning_rate': 3.0136939070107192e-05, 'epoch': 1.0}\n",
            "{'eval_loss': 0.5069728493690491, 'eval_accuracy': 0.7671800947867299, 'eval_f1': 0.7671368621690071, 'eval_precision': 0.7728320980938782, 'eval_recall': 0.7716197025188729, 'eval_runtime': 0.6444, 'eval_samples_per_second': 2619.568, 'eval_steps_per_second': 41.901, 'epoch': 1.0}\n",
            "{'loss': 0.5029, 'grad_norm': 12.626317024230957, 'learning_rate': 1.5352402082328578e-05, 'epoch': 2.0}\n",
            "{'eval_loss': 0.4485183358192444, 'eval_accuracy': 0.8062796208530806, 'eval_f1': 0.8060094012212802, 'eval_precision': 0.806055032991717, 'eval_recall': 0.807441823076717, 'eval_runtime': 0.649, 'eval_samples_per_second': 2601.11, 'eval_steps_per_second': 41.605, 'epoch': 2.0}\n",
            "{'loss': 0.4198, 'grad_norm': 17.580190658569336, 'learning_rate': 4.7429207563996394e-07, 'epoch': 3.0}\n",
            "{'eval_loss': 0.4714677333831787, 'eval_accuracy': 0.8003554502369669, 'eval_f1': 0.800111249206304, 'eval_precision': 0.80027711797308, 'eval_recall': 0.8016512783339703, 'eval_runtime': 0.6507, 'eval_samples_per_second': 2593.991, 'eval_steps_per_second': 41.492, 'epoch': 3.0}\n",
            "{'train_runtime': 64.1844, 'train_samples_per_second': 236.553, 'train_steps_per_second': 7.432, 'train_loss': 0.5077869247340556, 'epoch': 3.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-12 15:04:56,292] Trial 34 finished with value: 0.8060094012212802 and parameters: {'learning_rate': 3.5377028129066707e-05, 'weight_decay': 0.01, 'per_device_train_batch_size': 32, 'warmup_steps': 100, 'num_train_epochs': 3, 'adam_beta1': 0.8813656661203125, 'adam_beta2': 0.9762973725296302, 'adam_epsilon': 1.2790415907088108e-08, 'gradient_accumulation_steps': 1, 'lr_scheduler_type': 'polynomial', 'label_smoothing_factor': 0.15377759903684282}. Best is trial 34 with value: 0.8060094012212802.\n",
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.4485183358192444, 'eval_accuracy': 0.8062796208530806, 'eval_f1': 0.8060094012212802, 'eval_precision': 0.806055032991717, 'eval_recall': 0.807441823076717, 'eval_runtime': 0.6772, 'eval_samples_per_second': 2492.551, 'eval_steps_per_second': 39.869, 'epoch': 3.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.6023, 'grad_norm': 16.194595336914062, 'learning_rate': 3.176125812158263e-05, 'epoch': 1.0}\n",
            "{'eval_loss': 0.4938512444496155, 'eval_accuracy': 0.7671800947867299, 'eval_f1': 0.7666427581704267, 'eval_precision': 0.7664353152748118, 'eval_recall': 0.7674396794751561, 'eval_runtime': 0.6357, 'eval_samples_per_second': 2655.306, 'eval_steps_per_second': 42.472, 'epoch': 1.0}\n",
            "{'loss': 0.5021, 'grad_norm': 8.69227409362793, 'learning_rate': 1.6275845270839724e-05, 'epoch': 2.0}\n",
            "{'eval_loss': 0.4738679826259613, 'eval_accuracy': 0.7968009478672986, 'eval_f1': 0.7967803359359507, 'eval_precision': 0.7990730735462956, 'eval_recall': 0.799768575647276, 'eval_runtime': 0.6335, 'eval_samples_per_second': 2664.525, 'eval_steps_per_second': 42.62, 'epoch': 2.0}\n",
            "{'loss': 0.4221, 'grad_norm': 22.867389678955078, 'learning_rate': 6.917992172258418e-07, 'epoch': 3.0}\n",
            "{'eval_loss': 0.4707280099391937, 'eval_accuracy': 0.7896919431279621, 'eval_f1': 0.7892065627239222, 'eval_precision': 0.7889653815892999, 'eval_recall': 0.7900546759424443, 'eval_runtime': 0.6451, 'eval_samples_per_second': 2616.699, 'eval_steps_per_second': 41.855, 'epoch': 3.0}\n",
            "{'train_runtime': 60.6739, 'train_samples_per_second': 250.239, 'train_steps_per_second': 7.862, 'train_loss': 0.5088512812520473, 'epoch': 3.0}\n",
            "{'eval_loss': 0.4738679826259613, 'eval_accuracy': 0.7968009478672986, 'eval_f1': 0.7967803359359507, 'eval_precision': 0.7990730735462956, 'eval_recall': 0.799768575647276, 'eval_runtime': 0.6586, 'eval_samples_per_second': 2563.04, 'eval_steps_per_second': 40.996, 'epoch': 3.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-12 15:05:59,793] Trial 35 finished with value: 0.7967803359359507 and parameters: {'learning_rate': 3.7284717482357175e-05, 'weight_decay': 0.01, 'per_device_train_batch_size': 32, 'warmup_steps': 100, 'num_train_epochs': 3, 'adam_beta1': 0.8817460474505493, 'adam_beta2': 0.9761519711440808, 'adam_epsilon': 1.2634865929660097e-08, 'gradient_accumulation_steps': 1, 'lr_scheduler_type': 'polynomial', 'label_smoothing_factor': 0.15701748892626183}. Best is trial 34 with value: 0.8060094012212802.\n",
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.6854, 'grad_norm': 2.370652437210083, 'learning_rate': 1.4822064679666386e-05, 'epoch': 0.9811320754716981}\n",
            "{'eval_loss': 0.6036315560340881, 'eval_accuracy': 0.7031990521327014, 'eval_f1': 0.6943637004309761, 'eval_precision': 0.7095593810468339, 'eval_recall': 0.6948858179602785, 'eval_runtime': 0.6533, 'eval_samples_per_second': 2583.72, 'eval_steps_per_second': 41.327, 'epoch': 0.9811320754716981}\n",
            "{'loss': 0.571, 'grad_norm': 3.0673155784606934, 'learning_rate': 2.964412935933277e-05, 'epoch': 1.9811320754716981}\n",
            "{'eval_loss': 0.5403666496276855, 'eval_accuracy': 0.7304502369668247, 'eval_f1': 0.7260052106243082, 'eval_precision': 0.7693028297867008, 'eval_recall': 0.7424385160072036, 'eval_runtime': 0.645, 'eval_samples_per_second': 2616.876, 'eval_steps_per_second': 41.858, 'epoch': 1.9811320754716981}\n",
            "{'train_runtime': 33.738, 'train_samples_per_second': 300.018, 'train_steps_per_second': 2.312, 'train_loss': 0.6281755153949444, 'epoch': 1.9811320754716981}\n",
            "{'eval_loss': 0.5403666496276855, 'eval_accuracy': 0.7304502369668247, 'eval_f1': 0.7260052106243082, 'eval_precision': 0.7693028297867008, 'eval_recall': 0.7424385160072036, 'eval_runtime': 0.681, 'eval_samples_per_second': 2478.888, 'eval_steps_per_second': 39.65, 'epoch': 1.9811320754716981}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-12 15:06:36,003] Trial 36 finished with value: 0.7260052106243082 and parameters: {'learning_rate': 3.900543336754312e-05, 'weight_decay': 0.01, 'per_device_train_batch_size': 32, 'warmup_steps': 100, 'num_train_epochs': 2, 'adam_beta1': 0.8915245735746392, 'adam_beta2': 0.969088566671458, 'adam_epsilon': 1.93566314645635e-08, 'gradient_accumulation_steps': 4, 'lr_scheduler_type': 'polynomial', 'label_smoothing_factor': 0.10209889966650999}. Best is trial 34 with value: 0.8060094012212802.\n",
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.5991, 'grad_norm': 10.202798843383789, 'learning_rate': 3.668091737224961e-05, 'epoch': 1.0}\n",
            "{'eval_loss': 0.47473734617233276, 'eval_accuracy': 0.7719194312796208, 'eval_f1': 0.7715595346687265, 'eval_precision': 0.7715574612909204, 'eval_recall': 0.7727634267727373, 'eval_runtime': 0.6428, 'eval_samples_per_second': 2625.827, 'eval_steps_per_second': 42.001, 'epoch': 1.0}\n",
            "{'loss': 0.4954, 'grad_norm': 5.503217697143555, 'learning_rate': 1.878931604065089e-05, 'epoch': 2.0}\n",
            "{'eval_loss': 0.45379430055618286, 'eval_accuracy': 0.79739336492891, 'eval_f1': 0.7963294595796024, 'eval_precision': 0.7965516705714217, 'eval_recall': 0.796143491560274, 'eval_runtime': 0.6456, 'eval_samples_per_second': 2614.693, 'eval_steps_per_second': 41.823, 'epoch': 2.0}\n",
            "{'loss': 0.3913, 'grad_norm': 20.393159866333008, 'learning_rate': 6.697962207515523e-07, 'epoch': 3.0}\n",
            "{'eval_loss': 0.48251256346702576, 'eval_accuracy': 0.7867298578199052, 'eval_f1': 0.7860664360037007, 'eval_precision': 0.7857987782836615, 'eval_recall': 0.7865565156320733, 'eval_runtime': 0.6418, 'eval_samples_per_second': 2630.141, 'eval_steps_per_second': 42.07, 'epoch': 3.0}\n",
            "{'train_runtime': 59.4704, 'train_samples_per_second': 255.303, 'train_steps_per_second': 8.021, 'train_loss': 0.4952970460775763, 'epoch': 3.0}\n",
            "{'eval_loss': 0.45379430055618286, 'eval_accuracy': 0.79739336492891, 'eval_f1': 0.7963294595796024, 'eval_precision': 0.7965516705714217, 'eval_recall': 0.796143491560274, 'eval_runtime': 0.6521, 'eval_samples_per_second': 2588.459, 'eval_steps_per_second': 41.403, 'epoch': 3.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-12 15:07:38,668] Trial 37 finished with value: 0.7963294595796024 and parameters: {'learning_rate': 4.306263504466699e-05, 'weight_decay': 0.01, 'per_device_train_batch_size': 32, 'warmup_steps': 100, 'num_train_epochs': 3, 'adam_beta1': 0.9053638648666937, 'adam_beta2': 0.9864811733773884, 'adam_epsilon': 1.0707627499844943e-08, 'gradient_accumulation_steps': 1, 'lr_scheduler_type': 'polynomial', 'label_smoothing_factor': 0.18735974872252176}. Best is trial 34 with value: 0.8060094012212802.\n",
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.6048, 'grad_norm': 17.020153045654297, 'learning_rate': 2.9721770616082203e-05, 'epoch': 1.0}\n",
            "{'eval_loss': 0.5291404724121094, 'eval_accuracy': 0.754739336492891, 'eval_f1': 0.753875543083191, 'eval_precision': 0.7703766393722725, 'eval_recall': 0.7622167660667873, 'eval_runtime': 0.6409, 'eval_samples_per_second': 2633.98, 'eval_steps_per_second': 42.131, 'epoch': 1.0}\n",
            "{'loss': 0.5027, 'grad_norm': 6.445628643035889, 'learning_rate': 1.9909559099504973e-05, 'epoch': 2.0}\n",
            "{'eval_loss': 0.4569593071937561, 'eval_accuracy': 0.7962085308056872, 'eval_f1': 0.7955472429411632, 'eval_precision': 0.795278659004101, 'eval_recall': 0.7959982343492407, 'eval_runtime': 0.6588, 'eval_samples_per_second': 2562.049, 'eval_steps_per_second': 40.981, 'epoch': 2.0}\n",
            "{'loss': 0.4204, 'grad_norm': 15.379705429077148, 'learning_rate': 1.0220771627161418e-05, 'epoch': 3.0}\n",
            "{'eval_loss': 0.4832000136375427, 'eval_accuracy': 0.7908767772511849, 'eval_f1': 0.7907145253035068, 'eval_precision': 0.7913177654597633, 'eval_recall': 0.7926114849094681, 'eval_runtime': 0.6459, 'eval_samples_per_second': 2613.47, 'eval_steps_per_second': 41.803, 'epoch': 3.0}\n",
            "{'loss': 0.3448, 'grad_norm': 35.000919342041016, 'learning_rate': 4.0856011058419145e-07, 'epoch': 4.0}\n",
            "{'eval_loss': 0.5192577838897705, 'eval_accuracy': 0.783175355450237, 'eval_f1': 0.7828433722743295, 'eval_precision': 0.7828474352584398, 'eval_recall': 0.7841111175356479, 'eval_runtime': 0.637, 'eval_samples_per_second': 2649.804, 'eval_steps_per_second': 42.384, 'epoch': 4.0}\n",
            "{'train_runtime': 78.8553, 'train_samples_per_second': 256.723, 'train_steps_per_second': 8.065, 'train_loss': 0.4681827977018536, 'epoch': 4.0}\n",
            "{'eval_loss': 0.4569593071937561, 'eval_accuracy': 0.7962085308056872, 'eval_f1': 0.7955472429411632, 'eval_precision': 0.795278659004101, 'eval_recall': 0.7959982343492407, 'eval_runtime': 0.6655, 'eval_samples_per_second': 2536.391, 'eval_steps_per_second': 40.57, 'epoch': 4.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-12 15:09:00,376] Trial 38 finished with value: 0.7955472429411632 and parameters: {'learning_rate': 3.317764385462513e-05, 'weight_decay': 0.01, 'per_device_train_batch_size': 32, 'warmup_steps': 100, 'num_train_epochs': 4, 'adam_beta1': 0.886754020057389, 'adam_beta2': 0.9831636382223177, 'adam_epsilon': 2.1291926325404428e-08, 'gradient_accumulation_steps': 1, 'lr_scheduler_type': 'polynomial', 'label_smoothing_factor': 0.11584194469469276}. Best is trial 34 with value: 0.8060094012212802.\n",
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.6538, 'grad_norm': 16.016006469726562, 'learning_rate': 8.97402575136791e-06, 'epoch': 1.0}\n",
            "{'eval_loss': 0.5376251339912415, 'eval_accuracy': 0.7381516587677726, 'eval_f1': 0.737438063063063, 'eval_precision': 0.737231418028466, 'eval_recall': 0.7379968889572084, 'eval_runtime': 0.6357, 'eval_samples_per_second': 2655.468, 'eval_steps_per_second': 42.475, 'epoch': 1.0}\n",
            "{'loss': 0.5435, 'grad_norm': 13.333296775817871, 'learning_rate': 1.800557730883433e-05, 'epoch': 2.0}\n",
            "{'eval_loss': 0.4858889877796173, 'eval_accuracy': 0.7654028436018957, 'eval_f1': 0.7651060671655637, 'eval_precision': 0.775375833575468, 'eval_recall': 0.7713214316437899, 'eval_runtime': 0.6313, 'eval_samples_per_second': 2673.678, 'eval_steps_per_second': 42.766, 'epoch': 2.0}\n",
            "{'train_runtime': 40.4385, 'train_samples_per_second': 250.306, 'train_steps_per_second': 7.864, 'train_loss': 0.598659707315313, 'epoch': 2.0}\n",
            "{'eval_loss': 0.4858889877796173, 'eval_accuracy': 0.7654028436018957, 'eval_f1': 0.7651060671655637, 'eval_precision': 0.775375833575468, 'eval_recall': 0.7713214316437899, 'eval_runtime': 0.7126, 'eval_samples_per_second': 2368.838, 'eval_steps_per_second': 37.89, 'epoch': 2.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-12 15:09:43,325] Trial 39 finished with value: 0.7651060671655637 and parameters: {'learning_rate': 2.876290304925612e-05, 'weight_decay': 0.01, 'per_device_train_batch_size': 32, 'warmup_steps': 500, 'num_train_epochs': 2, 'adam_beta1': 0.8970838072408148, 'adam_beta2': 0.9776452443367433, 'adam_epsilon': 2.8660961445169397e-08, 'gradient_accumulation_steps': 1, 'lr_scheduler_type': 'polynomial', 'label_smoothing_factor': 0.15692453198842343}. Best is trial 34 with value: 0.8060094012212802.\n",
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Best Hyperparameters:\n",
            "learning_rate: 3.5377028129066707e-05\n",
            "weight_decay: 0.01\n",
            "per_device_train_batch_size: 32\n",
            "warmup_steps: 100\n",
            "num_train_epochs: 3\n",
            "adam_beta1: 0.8813656661203125\n",
            "adam_beta2: 0.9762973725296302\n",
            "adam_epsilon: 1.2790415907088108e-08\n",
            "gradient_accumulation_steps: 1\n",
            "lr_scheduler_type: polynomial\n",
            "label_smoothing_factor: 0.15377759903684282\n",
            "Best F1: 0.8060\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='477' max='477' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [477/477 01:01, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.600600</td>\n",
              "      <td>0.506973</td>\n",
              "      <td>0.767180</td>\n",
              "      <td>0.767137</td>\n",
              "      <td>0.772832</td>\n",
              "      <td>0.771620</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.502900</td>\n",
              "      <td>0.448518</td>\n",
              "      <td>0.806280</td>\n",
              "      <td>0.806009</td>\n",
              "      <td>0.806055</td>\n",
              "      <td>0.807442</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.419900</td>\n",
              "      <td>0.471794</td>\n",
              "      <td>0.800355</td>\n",
              "      <td>0.800111</td>\n",
              "      <td>0.800277</td>\n",
              "      <td>0.801651</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='27' max='27' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [27/27 00:00]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation Set Results:\n",
            "eval_loss: 0.4485\n",
            "eval_accuracy: 0.8063\n",
            "eval_f1: 0.8060\n",
            "eval_precision: 0.8061\n",
            "eval_recall: 0.8074\n",
            "eval_runtime: 0.6673\n",
            "eval_samples_per_second: 2529.6730\n",
            "eval_steps_per_second: 40.4630\n",
            "epoch: 3.0000\n",
            "\n",
            "distilbert-base-multilingual-cased\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a9dabccf2e10420ca8d840fbd088699c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/466 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c2ba02a52ce246bab82701c20db0827c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c3a69afb5e104bedb07a4c2e76ad4ce1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c4401417ce7c4af7b8079879ffb1e73b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/5061 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "76e840103f294c56b78f507572f3940f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/1688 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "13a51d39d8714d448c8ee24a46f78133"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-12 15:10:51,513] A new study created in memory with name: no-name-43cc3c0c-a52b-45ef-81d2-b43484134709\n",
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/542M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "87b05c0ea7b74000899eefb3985ef789"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.5959, 'grad_norm': 1.2932153940200806, 'learning_rate': 7.638575003656736e-06, 'epoch': 1.0}\n",
            "{'eval_loss': 0.5470951795578003, 'eval_accuracy': 0.7197867298578199, 'eval_f1': 0.7139560120275301, 'eval_precision': 0.7230249970522344, 'eval_recall': 0.7133983559140133, 'eval_runtime': 0.6106, 'eval_samples_per_second': 2764.292, 'eval_steps_per_second': 44.216, 'epoch': 1.0}\n",
            "{'loss': 0.5278, 'grad_norm': 2.380039930343628, 'learning_rate': 1.4771250002314415e-07, 'epoch': 1.9921011058451816}\n",
            "{'eval_loss': 0.5205703973770142, 'eval_accuracy': 0.7505924170616114, 'eval_f1': 0.7496603240527911, 'eval_precision': 0.7494931148094957, 'eval_recall': 0.7498917622238174, 'eval_runtime': 0.6143, 'eval_samples_per_second': 2748.05, 'eval_steps_per_second': 43.956, 'epoch': 1.9921011058451816}\n",
            "{'train_runtime': 41.1099, 'train_samples_per_second': 246.218, 'train_steps_per_second': 7.687, 'train_loss': 0.5620848981640006, 'epoch': 1.9921011058451816}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-12 15:11:39,177] Trial 0 finished with value: 0.7496603240527911 and parameters: {'learning_rate': 1.5177150007313473e-05, 'weight_decay': 0.0, 'per_device_train_batch_size': 8, 'warmup_steps': 0, 'num_train_epochs': 2, 'adam_beta1': 0.9479039417137278, 'adam_beta2': 0.9754200583426529, 'adam_epsilon': 2.8837342453770335e-08, 'gradient_accumulation_steps': 4, 'lr_scheduler_type': 'polynomial', 'label_smoothing_factor': 0.14320413513283003}. Best is trial 0 with value: 0.7496603240527911.\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.5205703973770142, 'eval_accuracy': 0.7505924170616114, 'eval_f1': 0.7496603240527911, 'eval_precision': 0.7494931148094957, 'eval_recall': 0.7498917622238174, 'eval_runtime': 0.6477, 'eval_samples_per_second': 2606.134, 'eval_steps_per_second': 41.686, 'epoch': 1.9921011058451816}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.5979, 'grad_norm': 2.893825054168701, 'learning_rate': 6.789113694396239e-06, 'epoch': 1.0}\n",
            "{'eval_loss': 0.5272175073623657, 'eval_accuracy': 0.7505924170616114, 'eval_f1': 0.7492171947888975, 'eval_precision': 0.7494913185024417, 'eval_recall': 0.7490075265799543, 'eval_runtime': 0.6132, 'eval_samples_per_second': 2752.789, 'eval_steps_per_second': 44.032, 'epoch': 1.0}\n",
            "{'loss': 0.5152, 'grad_norm': 3.1052052974700928, 'learning_rate': 1.2116808131138002e-07, 'epoch': 1.9952606635071088}\n",
            "{'eval_loss': 0.5098060369491577, 'eval_accuracy': 0.7624407582938388, 'eval_f1': 0.7617174670283153, 'eval_precision': 0.7614761476147615, 'eval_recall': 0.7621963172361078, 'eval_runtime': 0.6044, 'eval_samples_per_second': 2792.887, 'eval_steps_per_second': 44.673, 'epoch': 1.9952606635071088}\n",
            "{'train_runtime': 46.1013, 'train_samples_per_second': 219.56, 'train_steps_per_second': 13.709, 'train_loss': 0.5566952379443978, 'epoch': 1.9952606635071088}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-12 15:12:26,881] Trial 1 finished with value: 0.7617174670283153 and parameters: {'learning_rate': 1.1361419257654429e-05, 'weight_decay': 0.1, 'per_device_train_batch_size': 8, 'warmup_steps': 100, 'num_train_epochs': 2, 'adam_beta1': 0.8965464136759623, 'adam_beta2': 0.9875765887237691, 'adam_epsilon': 1.7615621170556236e-07, 'gradient_accumulation_steps': 2, 'lr_scheduler_type': 'polynomial', 'label_smoothing_factor': 0.16941946419677412}. Best is trial 1 with value: 0.7617174670283153.\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.5098060369491577, 'eval_accuracy': 0.7624407582938388, 'eval_f1': 0.7617174670283153, 'eval_precision': 0.7614761476147615, 'eval_recall': 0.7621963172361078, 'eval_runtime': 0.6435, 'eval_samples_per_second': 2623.01, 'eval_steps_per_second': 41.956, 'epoch': 1.9952606635071088}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.6014, 'grad_norm': 2.9660146236419678, 'learning_rate': 1.0504967995517418e-05, 'epoch': 1.0}\n",
            "{'eval_loss': 0.5383229851722717, 'eval_accuracy': 0.7375592417061612, 'eval_f1': 0.7325505913304909, 'eval_precision': 0.7408336509984459, 'eval_recall': 0.7316542257861165, 'eval_runtime': 0.6085, 'eval_samples_per_second': 2773.904, 'eval_steps_per_second': 44.369, 'epoch': 1.0}\n",
            "{'loss': 0.5098, 'grad_norm': 4.24292516708374, 'learning_rate': 3.7589645042394125e-06, 'epoch': 2.0}\n",
            "{'eval_loss': 0.5029618740081787, 'eval_accuracy': 0.7606635071090048, 'eval_f1': 0.7606604831769679, 'eval_precision': 0.7636667206774359, 'eval_recall': 0.7639076728243502, 'eval_runtime': 0.6137, 'eval_samples_per_second': 2750.42, 'eval_steps_per_second': 43.994, 'epoch': 2.0}\n",
            "{'loss': 0.4617, 'grad_norm': 4.02325439453125, 'learning_rate': 1.699667054488003e-10, 'epoch': 2.9921011058451814}\n",
            "{'eval_loss': 0.49782314896583557, 'eval_accuracy': 0.7707345971563981, 'eval_f1': 0.7703282667142003, 'eval_precision': 0.7702488274777431, 'eval_recall': 0.7714123936837087, 'eval_runtime': 0.6094, 'eval_samples_per_second': 2770.038, 'eval_steps_per_second': 44.307, 'epoch': 2.9921011058451814}\n",
            "{'train_runtime': 66.9051, 'train_samples_per_second': 226.933, 'train_steps_per_second': 14.169, 'train_loss': 0.5244876845476496, 'epoch': 2.9921011058451814}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-12 15:13:35,357] Trial 2 finished with value: 0.7703282667142003 and parameters: {'learning_rate': 1.2383910434240576e-05, 'weight_decay': 0.0, 'per_device_train_batch_size': 8, 'warmup_steps': 100, 'num_train_epochs': 3, 'adam_beta1': 0.9495770454753855, 'adam_beta2': 0.9719422378511842, 'adam_epsilon': 2.4538829824151117e-08, 'gradient_accumulation_steps': 2, 'lr_scheduler_type': 'cosine', 'label_smoothing_factor': 0.08417340674848006}. Best is trial 2 with value: 0.7703282667142003.\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.49782314896583557, 'eval_accuracy': 0.7707345971563981, 'eval_f1': 0.7703282667142003, 'eval_precision': 0.7702488274777431, 'eval_recall': 0.7714123936837087, 'eval_runtime': 0.6622, 'eval_samples_per_second': 2549.022, 'eval_steps_per_second': 40.772, 'epoch': 2.9921011058451814}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.5698, 'grad_norm': 5.956332683563232, 'learning_rate': 2.0379473374579314e-05, 'epoch': 1.0}\n",
            "{'eval_loss': 0.4971204400062561, 'eval_accuracy': 0.7784360189573459, 'eval_f1': 0.7780321782178218, 'eval_precision': 0.7779236663947053, 'eval_recall': 0.7791089104721988, 'eval_runtime': 0.6147, 'eval_samples_per_second': 2746.215, 'eval_steps_per_second': 43.926, 'epoch': 1.0}\n",
            "{'loss': 0.4684, 'grad_norm': 4.349844455718994, 'learning_rate': 6.418731771521043e-08, 'epoch': 2.0}\n",
            "{'eval_loss': 0.48797333240509033, 'eval_accuracy': 0.7796208530805687, 'eval_f1': 0.7794115167355198, 'eval_precision': 0.7798307200085356, 'eval_recall': 0.7811030240294914, 'eval_runtime': 0.6155, 'eval_samples_per_second': 2742.565, 'eval_steps_per_second': 43.868, 'epoch': 2.0}\n",
            "{'train_runtime': 50.5425, 'train_samples_per_second': 200.267, 'train_steps_per_second': 25.048, 'train_loss': 0.5190713891485856, 'epoch': 2.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-12 15:14:27,454] Trial 3 finished with value: 0.7794115167355198 and parameters: {'learning_rate': 4.06305721137282e-05, 'weight_decay': 0.0, 'per_device_train_batch_size': 8, 'warmup_steps': 0, 'num_train_epochs': 2, 'adam_beta1': 0.910101571883863, 'adam_beta2': 0.9783053720862299, 'adam_epsilon': 6.63154921976226e-08, 'gradient_accumulation_steps': 1, 'lr_scheduler_type': 'linear', 'label_smoothing_factor': 0.08553616883970022}. Best is trial 3 with value: 0.7794115167355198.\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.48797333240509033, 'eval_accuracy': 0.7796208530805687, 'eval_f1': 0.7794115167355198, 'eval_precision': 0.7798307200085356, 'eval_recall': 0.7811030240294914, 'eval_runtime': 0.647, 'eval_samples_per_second': 2609.01, 'eval_steps_per_second': 41.732, 'epoch': 2.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.574, 'grad_norm': 3.2747037410736084, 'learning_rate': 3.378087124552733e-05, 'epoch': 1.0}\n",
            "{'eval_loss': 0.4988367557525635, 'eval_accuracy': 0.773696682464455, 'eval_f1': 0.7734291878368786, 'eval_precision': 0.7736570574238244, 'eval_recall': 0.7749105539940797, 'eval_runtime': 0.6067, 'eval_samples_per_second': 2782.326, 'eval_steps_per_second': 44.504, 'epoch': 1.0}\n",
            "{'loss': 0.4786, 'grad_norm': 4.966122627258301, 'learning_rate': 2.2565707185829676e-05, 'epoch': 2.0}\n",
            "{'eval_loss': 0.48891741037368774, 'eval_accuracy': 0.7707345971563981, 'eval_f1': 0.770635988173247, 'eval_precision': 0.7717994751718054, 'eval_recall': 0.772859324737303, 'eval_runtime': 0.6066, 'eval_samples_per_second': 2782.934, 'eval_steps_per_second': 44.514, 'epoch': 2.0}\n",
            "{'loss': 0.3715, 'grad_norm': 13.64673137664795, 'learning_rate': 1.1385922192566406e-05, 'epoch': 3.0}\n",
            "{'eval_loss': 0.533074140548706, 'eval_accuracy': 0.7790284360189573, 'eval_f1': 0.7789222166627165, 'eval_precision': 0.7799858414239482, 'eval_recall': 0.7811107804825078, 'eval_runtime': 0.6138, 'eval_samples_per_second': 2749.885, 'eval_steps_per_second': 43.985, 'epoch': 3.0}\n",
            "{'loss': 0.2556, 'grad_norm': 9.723124504089355, 'learning_rate': 1.7075813286875445e-07, 'epoch': 4.0}\n",
            "{'eval_loss': 0.651488184928894, 'eval_accuracy': 0.759478672985782, 'eval_f1': 0.7586315712361729, 'eval_precision': 0.7584290775200673, 'eval_recall': 0.7589393121013359, 'eval_runtime': 0.6117, 'eval_samples_per_second': 2759.685, 'eval_steps_per_second': 44.142, 'epoch': 4.0}\n",
            "{'train_runtime': 61.0741, 'train_samples_per_second': 331.466, 'train_steps_per_second': 20.762, 'train_loss': 0.419906730531518, 'epoch': 4.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-12 15:15:33,989] Trial 4 finished with value: 0.7789222166627165 and parameters: {'learning_rate': 4.49606562387906e-05, 'weight_decay': 0.01, 'per_device_train_batch_size': 16, 'warmup_steps': 0, 'num_train_epochs': 4, 'adam_beta1': 0.925820367803265, 'adam_beta2': 0.9572213477831167, 'adam_epsilon': 6.810742992112595e-08, 'gradient_accumulation_steps': 1, 'lr_scheduler_type': 'polynomial', 'label_smoothing_factor': 0.15958887618934053}. Best is trial 3 with value: 0.7794115167355198.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.533074140548706, 'eval_accuracy': 0.7790284360189573, 'eval_f1': 0.7789222166627165, 'eval_precision': 0.7799858414239482, 'eval_recall': 0.7811107804825078, 'eval_runtime': 0.6442, 'eval_samples_per_second': 2620.185, 'eval_steps_per_second': 41.911, 'epoch': 4.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.6174, 'grad_norm': 6.108518123626709, 'learning_rate': 1.583518906373818e-05, 'epoch': 1.0}\n",
            "{'eval_loss': 0.5282049775123596, 'eval_accuracy': 0.7488151658767772, 'eval_f1': 0.7469218833759779, 'eval_precision': 0.7480121803198398, 'eval_recall': 0.7463780890074138, 'eval_runtime': 0.6067, 'eval_samples_per_second': 2782.469, 'eval_steps_per_second': 44.506, 'epoch': 1.0}\n",
            "{'loss': 0.5181, 'grad_norm': 5.580143451690674, 'learning_rate': 1.8698267834567091e-07, 'epoch': 2.0}\n",
            "{'eval_loss': 0.4946326017379761, 'eval_accuracy': 0.7701421800947867, 'eval_f1': 0.7701392759224345, 'eval_precision': 0.7731801306121299, 'eval_recall': 0.7734297766000505, 'eval_runtime': 0.6084, 'eval_samples_per_second': 2774.71, 'eval_steps_per_second': 44.382, 'epoch': 2.0}\n",
            "{'train_runtime': 29.9546, 'train_samples_per_second': 337.911, 'train_steps_per_second': 21.165, 'train_loss': 0.5677420920001972, 'epoch': 2.0}\n",
            "{'eval_loss': 0.4946326017379761, 'eval_accuracy': 0.7701421800947867, 'eval_f1': 0.7701392759224345, 'eval_precision': 0.7731801306121299, 'eval_recall': 0.7734297766000505, 'eval_runtime': 0.6846, 'eval_samples_per_second': 2465.816, 'eval_steps_per_second': 39.441, 'epoch': 2.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-12 15:16:05,747] Trial 5 finished with value: 0.7701392759224345 and parameters: {'learning_rate': 2.5055678898319902e-05, 'weight_decay': 0.0, 'per_device_train_batch_size': 16, 'warmup_steps': 500, 'num_train_epochs': 2, 'adam_beta1': 0.8692289569644388, 'adam_beta2': 0.9742708321559255, 'adam_epsilon': 1.2847412264749222e-07, 'gradient_accumulation_steps': 1, 'lr_scheduler_type': 'linear', 'label_smoothing_factor': 0.02759278400043579}. Best is trial 3 with value: 0.7794115167355198.\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.5826, 'grad_norm': 6.302615642547607, 'learning_rate': 2.187161978736571e-05, 'epoch': 1.0}\n",
            "{'eval_loss': 0.4973509609699249, 'eval_accuracy': 0.7754739336492891, 'eval_f1': 0.7744632170839834, 'eval_precision': 0.7744416243654823, 'eval_recall': 0.7744853593423656, 'eval_runtime': 0.6108, 'eval_samples_per_second': 2763.759, 'eval_steps_per_second': 44.207, 'epoch': 1.0}\n",
            "{'loss': 0.4912, 'grad_norm': 4.802183151245117, 'learning_rate': 1.3768302872511216e-05, 'epoch': 2.0}\n",
            "{'eval_loss': 0.48285165429115295, 'eval_accuracy': 0.7796208530805687, 'eval_f1': 0.7794273237311212, 'eval_precision': 0.779920172627045, 'eval_recall': 0.7811834090880245, 'eval_runtime': 0.618, 'eval_samples_per_second': 2731.375, 'eval_steps_per_second': 43.689, 'epoch': 2.0}\n",
            "{'loss': 0.3915, 'grad_norm': 10.936203956604004, 'learning_rate': 4.215598194136753e-06, 'epoch': 3.0}\n",
            "{'eval_loss': 0.5409538149833679, 'eval_accuracy': 0.7742890995260664, 'eval_f1': 0.7735100331285703, 'eval_precision': 0.7732854657937549, 'eval_recall': 0.7738577917801341, 'eval_runtime': 0.6262, 'eval_samples_per_second': 2695.795, 'eval_steps_per_second': 43.12, 'epoch': 3.0}\n",
            "{'loss': 0.2978, 'grad_norm': 22.59221839904785, 'learning_rate': 1.0002564841464018e-10, 'epoch': 4.0}\n",
            "{'eval_loss': 0.654070258140564, 'eval_accuracy': 0.7677725118483413, 'eval_f1': 0.7665125364859449, 'eval_precision': 0.7667730455875352, 'eval_recall': 0.7663065322026775, 'eval_runtime': 0.6101, 'eval_samples_per_second': 2766.778, 'eval_steps_per_second': 44.255, 'epoch': 4.0}\n",
            "{'train_runtime': 100.3173, 'train_samples_per_second': 201.8, 'train_steps_per_second': 25.24, 'train_loss': 0.4407707081775153, 'epoch': 4.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-12 15:17:48,000] Trial 6 finished with value: 0.7794273237311212 and parameters: {'learning_rate': 2.3456381115148355e-05, 'weight_decay': 0.0, 'per_device_train_batch_size': 8, 'warmup_steps': 250, 'num_train_epochs': 4, 'adam_beta1': 0.9026821353237153, 'adam_beta2': 0.9652263230400947, 'adam_epsilon': 1.2392862682389777e-07, 'gradient_accumulation_steps': 1, 'lr_scheduler_type': 'cosine', 'label_smoothing_factor': 0.00766636550903701}. Best is trial 6 with value: 0.7794273237311212.\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.48285165429115295, 'eval_accuracy': 0.7796208530805687, 'eval_f1': 0.7794273237311212, 'eval_precision': 0.779920172627045, 'eval_recall': 0.7811834090880245, 'eval_runtime': 0.643, 'eval_samples_per_second': 2625.261, 'eval_steps_per_second': 41.992, 'epoch': 4.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.6539, 'grad_norm': 1.7214051485061646, 'learning_rate': 4.868949192812398e-06, 'epoch': 1.0}\n",
            "{'eval_loss': 0.6055148839950562, 'eval_accuracy': 0.6741706161137441, 'eval_f1': 0.6728313269995208, 'eval_precision': 0.6727609600766483, 'eval_recall': 0.6729188378859012, 'eval_runtime': 0.6037, 'eval_samples_per_second': 2796.321, 'eval_steps_per_second': 44.728, 'epoch': 1.0}\n",
            "{'loss': 0.5779, 'grad_norm': 3.8896095752716064, 'learning_rate': 9.76871451975652e-06, 'epoch': 2.0}\n",
            "{'eval_loss': 0.5443307161331177, 'eval_accuracy': 0.7316350710900474, 'eval_f1': 0.7316236742438202, 'eval_precision': 0.7358869144510761, 'eval_recall': 0.7354295030087986, 'eval_runtime': 0.6081, 'eval_samples_per_second': 2775.914, 'eval_steps_per_second': 44.401, 'epoch': 2.0}\n",
            "{'loss': 0.5273, 'grad_norm': 2.516629934310913, 'learning_rate': 1.4576031444305468e-05, 'epoch': 2.9842271293375395}\n",
            "{'eval_loss': 0.5061455368995667, 'eval_accuracy': 0.7624407582938388, 'eval_f1': 0.7619708557702081, 'eval_precision': 0.7618396880951445, 'eval_recall': 0.762919782762905, 'eval_runtime': 0.6083, 'eval_samples_per_second': 2775.14, 'eval_steps_per_second': 44.389, 'epoch': 2.9842271293375395}\n",
            "{'train_runtime': 41.2242, 'train_samples_per_second': 368.303, 'train_steps_per_second': 11.498, 'train_loss': 0.5867543119921463, 'epoch': 2.9842271293375395}\n",
            "{'eval_loss': 0.5061455368995667, 'eval_accuracy': 0.7624407582938388, 'eval_f1': 0.7619708557702081, 'eval_precision': 0.7618396880951445, 'eval_recall': 0.762919782762905, 'eval_runtime': 0.6584, 'eval_samples_per_second': 2563.777, 'eval_steps_per_second': 41.008, 'epoch': 2.9842271293375395}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-12 15:18:31,697] Trial 7 finished with value: 0.7619708557702081 and parameters: {'learning_rate': 1.5408067065862018e-05, 'weight_decay': 0.1, 'per_device_train_batch_size': 16, 'warmup_steps': 500, 'num_train_epochs': 3, 'adam_beta1': 0.8921060191331066, 'adam_beta2': 0.9644322367300893, 'adam_epsilon': 3.3320357527865475e-08, 'gradient_accumulation_steps': 2, 'lr_scheduler_type': 'cosine', 'label_smoothing_factor': 0.10525296536683347}. Best is trial 6 with value: 0.7794273237311212.\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.6368, 'grad_norm': 2.3596272468566895, 'learning_rate': 6.904374628264572e-06, 'epoch': 1.0}\n",
            "{'eval_loss': 0.5864535570144653, 'eval_accuracy': 0.707345971563981, 'eval_f1': 0.7029215065117915, 'eval_precision': 0.7078629543495114, 'eval_recall': 0.7023073332327345, 'eval_runtime': 0.6132, 'eval_samples_per_second': 2752.956, 'eval_steps_per_second': 44.034, 'epoch': 1.0}\n",
            "{'loss': 0.5474, 'grad_norm': 2.7820332050323486, 'learning_rate': 1.546958493156557e-09, 'epoch': 1.9952606635071088}\n",
            "{'eval_loss': 0.5115440487861633, 'eval_accuracy': 0.754739336492891, 'eval_f1': 0.7545239570556026, 'eval_precision': 0.7550369443777578, 'eval_recall': 0.7561878866768112, 'eval_runtime': 0.609, 'eval_samples_per_second': 2771.676, 'eval_steps_per_second': 44.334, 'epoch': 1.9952606635071088}\n",
            "{'train_runtime': 44.815, 'train_samples_per_second': 225.862, 'train_steps_per_second': 14.102, 'train_loss': 0.5922707183451592, 'epoch': 1.9952606635071088}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-12 15:19:18,276] Trial 8 finished with value: 0.7545239570556026 and parameters: {'learning_rate': 1.0924643399152804e-05, 'weight_decay': 0.1, 'per_device_train_batch_size': 8, 'warmup_steps': 500, 'num_train_epochs': 2, 'adam_beta1': 0.9379114689531804, 'adam_beta2': 0.9913725361858621, 'adam_epsilon': 1.3219193417148182e-07, 'gradient_accumulation_steps': 2, 'lr_scheduler_type': 'cosine_with_restarts', 'label_smoothing_factor': 0.01377431504331268}. Best is trial 6 with value: 0.7794273237311212.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.5115440487861633, 'eval_accuracy': 0.754739336492891, 'eval_f1': 0.7545239570556026, 'eval_precision': 0.7550369443777578, 'eval_recall': 0.7561878866768112, 'eval_runtime': 0.6489, 'eval_samples_per_second': 2601.158, 'eval_steps_per_second': 41.606, 'epoch': 1.9952606635071088}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.5649, 'grad_norm': 4.5064287185668945, 'learning_rate': 3.392914746731913e-05, 'epoch': 1.0}\n",
            "{'eval_loss': 0.5003267526626587, 'eval_accuracy': 0.7636255924170616, 'eval_f1': 0.7636155541352854, 'eval_precision': 0.7680864532465184, 'eval_recall': 0.7675666032517872, 'eval_runtime': 0.6171, 'eval_samples_per_second': 2735.336, 'eval_steps_per_second': 43.752, 'epoch': 1.0}\n",
            "{'loss': 0.4679, 'grad_norm': 5.483977794647217, 'learning_rate': 1.1395726537949538e-05, 'epoch': 2.0}\n",
            "{'eval_loss': 0.495725154876709, 'eval_accuracy': 0.7671800947867299, 'eval_f1': 0.7668553354127312, 'eval_precision': 0.7669535308156676, 'eval_recall': 0.7681631450019533, 'eval_runtime': 0.6064, 'eval_samples_per_second': 2783.658, 'eval_steps_per_second': 44.525, 'epoch': 2.0}\n",
            "{'loss': 0.3602, 'grad_norm': inf, 'learning_rate': 4.887288433747618e-10, 'epoch': 3.0}\n",
            "{'eval_loss': 0.5155379176139832, 'eval_accuracy': 0.7612559241706162, 'eval_f1': 0.7608327945370096, 'eval_precision': 0.7607675457073048, 'eval_recall': 0.7618902899080084, 'eval_runtime': 0.6035, 'eval_samples_per_second': 2797.039, 'eval_steps_per_second': 44.739, 'epoch': 3.0}\n",
            "{'train_runtime': 29.8779, 'train_samples_per_second': 508.167, 'train_steps_per_second': 15.965, 'train_loss': 0.4643603870703739, 'epoch': 3.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-12 15:19:50,117] Trial 9 finished with value: 0.7668553354127312 and parameters: {'learning_rate': 4.506781931642593e-05, 'weight_decay': 0.0, 'per_device_train_batch_size': 32, 'warmup_steps': 0, 'num_train_epochs': 3, 'adam_beta1': 0.8534011808404797, 'adam_beta2': 0.9949452496024472, 'adam_epsilon': 2.2666795335704801e-07, 'gradient_accumulation_steps': 1, 'lr_scheduler_type': 'cosine', 'label_smoothing_factor': 0.054536017090397064}. Best is trial 6 with value: 0.7794273237311212.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.495725154876709, 'eval_accuracy': 0.7671800947867299, 'eval_f1': 0.7668553354127312, 'eval_precision': 0.7669535308156676, 'eval_recall': 0.7681631450019533, 'eval_runtime': 0.6456, 'eval_samples_per_second': 2614.572, 'eval_steps_per_second': 41.821, 'epoch': 3.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.6806, 'grad_norm': 0.918028712272644, 'learning_rate': 4.024689669063174e-06, 'epoch': 0.9811320754716981}\n",
            "{'eval_loss': 0.6502759456634521, 'eval_accuracy': 0.6735781990521327, 'eval_f1': 0.6717783238210987, 'eval_precision': 0.6719739102911919, 'eval_recall': 0.6716404334023893, 'eval_runtime': 0.6082, 'eval_samples_per_second': 2775.185, 'eval_steps_per_second': 44.39, 'epoch': 0.9811320754716981}\n",
            "{'loss': 0.6283, 'grad_norm': 1.0658454895019531, 'learning_rate': 8.155292224154325e-06, 'epoch': 1.9811320754716981}\n",
            "{'eval_loss': 0.5976706147193909, 'eval_accuracy': 0.6883886255924171, 'eval_f1': 0.6883028617968376, 'eval_precision': 0.689772942887863, 'eval_recall': 0.6904173958907722, 'eval_runtime': 0.6065, 'eval_samples_per_second': 2783.319, 'eval_steps_per_second': 44.52, 'epoch': 1.9811320754716981}\n",
            "{'loss': 0.5825, 'grad_norm': 1.447824239730835, 'learning_rate': 1.2285894779245477e-05, 'epoch': 2.981132075471698}\n",
            "{'eval_loss': 0.5511346459388733, 'eval_accuracy': 0.7345971563981043, 'eval_f1': 0.733062238789111, 'eval_precision': 0.7334082667062541, 'eval_recall': 0.7328183988706605, 'eval_runtime': 0.6092, 'eval_samples_per_second': 2770.927, 'eval_steps_per_second': 44.322, 'epoch': 2.981132075471698}\n",
            "{'loss': 0.5325, 'grad_norm': 1.5250990390777588, 'learning_rate': 1.641649733433663e-05, 'epoch': 3.981132075471698}\n",
            "{'eval_loss': 0.5288166403770447, 'eval_accuracy': 0.7440758293838863, 'eval_f1': 0.7439320388349515, 'eval_precision': 0.7512757370253241, 'eval_recall': 0.749092847563134, 'eval_runtime': 0.6043, 'eval_samples_per_second': 2793.35, 'eval_steps_per_second': 44.68, 'epoch': 3.981132075471698}\n",
            "{'loss': 0.4995, 'grad_norm': 4.115214824676514, 'learning_rate': 2.0547099889427782e-05, 'epoch': 4.981132075471698}\n",
            "{'eval_loss': 0.5123569369316101, 'eval_accuracy': 0.7665876777251185, 'eval_f1': 0.7635282739788372, 'eval_precision': 0.7680443041530822, 'eval_recall': 0.7623831772405925, 'eval_runtime': 0.6061, 'eval_samples_per_second': 2784.928, 'eval_steps_per_second': 44.546, 'epoch': 4.981132075471698}\n",
            "{'train_runtime': 44.7415, 'train_samples_per_second': 565.583, 'train_steps_per_second': 4.358, 'train_loss': 0.5846964322603666, 'epoch': 4.981132075471698}\n",
            "{'eval_loss': 0.5123569369316101, 'eval_accuracy': 0.7665876777251185, 'eval_f1': 0.7635282739788372, 'eval_precision': 0.7680443041530822, 'eval_recall': 0.7623831772405925, 'eval_runtime': 0.6479, 'eval_samples_per_second': 2605.439, 'eval_steps_per_second': 41.675, 'epoch': 4.981132075471698}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-12 15:20:36,669] Trial 10 finished with value: 0.7635282739788372 and parameters: {'learning_rate': 2.6478221506994563e-05, 'weight_decay': 0.01, 'per_device_train_batch_size': 32, 'warmup_steps': 250, 'num_train_epochs': 5, 'adam_beta1': 0.8792341910091057, 'adam_beta2': 0.9516080626252564, 'adam_epsilon': 8.463781010043537e-07, 'gradient_accumulation_steps': 4, 'lr_scheduler_type': 'cosine_with_restarts', 'label_smoothing_factor': 0.000530969978053613}. Best is trial 6 with value: 0.7794273237311212.\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.5826, 'grad_norm': 6.521717548370361, 'learning_rate': 2.791368795527934e-05, 'epoch': 1.0}\n",
            "{'eval_loss': 0.5056868195533752, 'eval_accuracy': 0.7772511848341233, 'eval_f1': 0.7754782365068607, 'eval_precision': 0.7768922830747194, 'eval_recall': 0.7747836302174487, 'eval_runtime': 0.6052, 'eval_samples_per_second': 2789.142, 'eval_steps_per_second': 44.613, 'epoch': 1.0}\n",
            "{'loss': 0.4964, 'grad_norm': 3.4822468757629395, 'learning_rate': 1.862871385647063e-05, 'epoch': 2.0}\n",
            "{'eval_loss': 0.4990115463733673, 'eval_accuracy': 0.7731042654028436, 'eval_f1': 0.7728452493679936, 'eval_precision': 0.7731042654028436, 'eval_recall': 0.7743556150373649, 'eval_runtime': 0.6076, 'eval_samples_per_second': 2778.292, 'eval_steps_per_second': 44.44, 'epoch': 2.0}\n",
            "{'loss': 0.4, 'grad_norm': 8.65524673461914, 'learning_rate': 9.343739757661927e-06, 'epoch': 3.0}\n",
            "{'eval_loss': 0.5537356734275818, 'eval_accuracy': 0.7707345971563981, 'eval_f1': 0.7699432619967368, 'eval_precision': 0.7697221393468924, 'eval_recall': 0.7702870028642466, 'eval_runtime': 0.6053, 'eval_samples_per_second': 2788.586, 'eval_steps_per_second': 44.604, 'epoch': 3.0}\n",
            "{'train_runtime': 75.0003, 'train_samples_per_second': 269.919, 'train_steps_per_second': 33.76, 'train_loss': 0.49296892838078843, 'epoch': 3.0}\n",
            "{'eval_loss': 0.5056868195533752, 'eval_accuracy': 0.7772511848341233, 'eval_f1': 0.7754782365068607, 'eval_precision': 0.7768922830747194, 'eval_recall': 0.7747836302174487, 'eval_runtime': 0.6391, 'eval_samples_per_second': 2641.186, 'eval_steps_per_second': 42.246, 'epoch': 3.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-12 15:21:53,742] Trial 11 finished with value: 0.7754782365068607 and parameters: {'learning_rate': 3.352580837576182e-05, 'weight_decay': 0.0, 'per_device_train_batch_size': 8, 'warmup_steps': 250, 'num_train_epochs': 4, 'adam_beta1': 0.9144798289875328, 'adam_beta2': 0.9819735136700836, 'adam_epsilon': 4.260288043838968e-07, 'gradient_accumulation_steps': 1, 'lr_scheduler_type': 'linear', 'label_smoothing_factor': 0.09804896510154705}. Best is trial 6 with value: 0.7794273237311212.\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.5825, 'grad_norm': 7.4194865226745605, 'learning_rate': 2.8404345874206535e-05, 'epoch': 1.0}\n",
            "{'eval_loss': 0.5034193992614746, 'eval_accuracy': 0.7772511848341233, 'eval_f1': 0.7755722362013389, 'eval_precision': 0.7767679563893444, 'eval_recall': 0.7749444003345147, 'eval_runtime': 0.6173, 'eval_samples_per_second': 2734.289, 'eval_steps_per_second': 43.736, 'epoch': 1.0}\n",
            "{'loss': 0.4956, 'grad_norm': 5.691338062286377, 'learning_rate': 2.1320073343623058e-05, 'epoch': 2.0}\n",
            "{'eval_loss': 0.4991161525249481, 'eval_accuracy': 0.7725118483412322, 'eval_f1': 0.7724399707348995, 'eval_precision': 0.7738723761851696, 'eval_recall': 0.7748456818415794, 'eval_runtime': 0.6053, 'eval_samples_per_second': 2788.502, 'eval_steps_per_second': 44.603, 'epoch': 2.0}\n",
            "{'loss': 0.4049, 'grad_norm': 35.26005172729492, 'learning_rate': 1.422459152106081e-05, 'epoch': 3.0}\n",
            "{'eval_loss': 0.5861511826515198, 'eval_accuracy': 0.7630331753554502, 'eval_f1': 0.7630168738163128, 'eval_precision': 0.765442576711467, 'eval_recall': 0.7659666585341431, 'eval_runtime': 0.6081, 'eval_samples_per_second': 2775.966, 'eval_steps_per_second': 44.402, 'epoch': 3.0}\n",
            "{'train_runtime': 75.0993, 'train_samples_per_second': 336.954, 'train_steps_per_second': 42.144, 'train_loss': 0.4943047192549944, 'epoch': 3.0}\n",
            "{'eval_loss': 0.5034193992614746, 'eval_accuracy': 0.7772511848341233, 'eval_f1': 0.7755722362013389, 'eval_precision': 0.7767679563893444, 'eval_recall': 0.7749444003345147, 'eval_runtime': 0.661, 'eval_samples_per_second': 2553.824, 'eval_steps_per_second': 40.849, 'epoch': 3.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-12 15:23:10,838] Trial 12 finished with value: 0.7755722362013389 and parameters: {'learning_rate': 3.267508611811841e-05, 'weight_decay': 0.0, 'per_device_train_batch_size': 8, 'warmup_steps': 250, 'num_train_epochs': 5, 'adam_beta1': 0.9114227733277834, 'adam_beta2': 0.9658253982542773, 'adam_epsilon': 1.1949973203832363e-08, 'gradient_accumulation_steps': 1, 'lr_scheduler_type': 'linear', 'label_smoothing_factor': 0.04956019877593317}. Best is trial 6 with value: 0.7794273237311212.\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.5824, 'grad_norm': 6.478797912597656, 'learning_rate': 1.711983648619174e-05, 'epoch': 1.0}\n",
            "{'eval_loss': 0.5026352405548096, 'eval_accuracy': 0.7725118483412322, 'eval_f1': 0.7710255018990776, 'eval_precision': 0.7717318491137878, 'eval_recall': 0.7705852737393296, 'eval_runtime': 0.6115, 'eval_samples_per_second': 2760.536, 'eval_steps_per_second': 44.155, 'epoch': 1.0}\n",
            "{'loss': 0.4933, 'grad_norm': 8.207365036010742, 'learning_rate': 1.0777029600977069e-05, 'epoch': 2.0}\n",
            "{'eval_loss': 0.4896235764026642, 'eval_accuracy': 0.7796208530805687, 'eval_f1': 0.7795601988373073, 'eval_precision': 0.7811130085243045, 'eval_recall': 0.7820676447318876, 'eval_runtime': 0.6068, 'eval_samples_per_second': 2782.007, 'eval_steps_per_second': 44.499, 'epoch': 2.0}\n",
            "{'loss': 0.399, 'grad_norm': 19.136920928955078, 'learning_rate': 3.2997259680234612e-06, 'epoch': 3.0}\n",
            "{'eval_loss': 0.5355581641197205, 'eval_accuracy': 0.7683649289099526, 'eval_f1': 0.7681187526767028, 'eval_precision': 0.7684464547430131, 'eval_recall': 0.7696749482080478, 'eval_runtime': 0.6121, 'eval_samples_per_second': 2757.869, 'eval_steps_per_second': 44.113, 'epoch': 3.0}\n",
            "{'loss': 0.3099, 'grad_norm': 22.69468116760254, 'learning_rate': 7.829428098750768e-11, 'epoch': 4.0}\n",
            "{'eval_loss': 0.5981805920600891, 'eval_accuracy': 0.7677725118483413, 'eval_f1': 0.7667084574953427, 'eval_precision': 0.7667084574953427, 'eval_recall': 0.7667084574953427, 'eval_runtime': 0.6124, 'eval_samples_per_second': 2756.395, 'eval_steps_per_second': 44.089, 'epoch': 4.0}\n",
            "{'train_runtime': 100.5588, 'train_samples_per_second': 201.315, 'train_steps_per_second': 25.179, 'train_loss': 0.44617471197770103, 'epoch': 4.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-12 15:24:55,233] Trial 13 finished with value: 0.7795601988373073 and parameters: {'learning_rate': 1.8360295814995146e-05, 'weight_decay': 0.0, 'per_device_train_batch_size': 8, 'warmup_steps': 250, 'num_train_epochs': 4, 'adam_beta1': 0.9155300518327477, 'adam_beta2': 0.9817834337593364, 'adam_epsilon': 6.600611872866662e-08, 'gradient_accumulation_steps': 1, 'lr_scheduler_type': 'cosine', 'label_smoothing_factor': 0.1961695982291689}. Best is trial 13 with value: 0.7795601988373073.\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.4896235764026642, 'eval_accuracy': 0.7796208530805687, 'eval_f1': 0.7795601988373073, 'eval_precision': 0.7811130085243045, 'eval_recall': 0.7820676447318876, 'eval_runtime': 0.6661, 'eval_samples_per_second': 2534.081, 'eval_steps_per_second': 40.533, 'epoch': 4.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.6351, 'grad_norm': 4.54283332824707, 'learning_rate': 1.2486273923006206e-05, 'epoch': 1.0}\n",
            "{'eval_loss': 0.5832272171974182, 'eval_accuracy': 0.70260663507109, 'eval_f1': 0.6926376024328528, 'eval_precision': 0.7107766145116048, 'eval_recall': 0.6936074134767666, 'eval_runtime': 0.6331, 'eval_samples_per_second': 2666.234, 'eval_steps_per_second': 42.647, 'epoch': 1.0}\n",
            "{'loss': 0.5358, 'grad_norm': 9.485013961791992, 'learning_rate': 1.832410591000149e-05, 'epoch': 2.0}\n",
            "{'eval_loss': 0.5064395666122437, 'eval_accuracy': 0.7665876777251185, 'eval_f1': 0.7665824348607367, 'eval_precision': 0.7694563586807244, 'eval_recall': 0.7697786026256299, 'eval_runtime': 0.6185, 'eval_samples_per_second': 2729.07, 'eval_steps_per_second': 43.652, 'epoch': 2.0}\n",
            "{'loss': 0.4716, 'grad_norm': 12.977092742919922, 'learning_rate': 7.257011478722438e-06, 'epoch': 3.0}\n",
            "{'eval_loss': 0.4914429187774658, 'eval_accuracy': 0.7719194312796208, 'eval_f1': 0.7715807317725549, 'eval_precision': 0.77162376170901, 'eval_recall': 0.7728438118312704, 'eval_runtime': 0.6095, 'eval_samples_per_second': 2769.66, 'eval_steps_per_second': 44.301, 'epoch': 3.0}\n",
            "{'loss': 0.4007, 'grad_norm': 14.460955619812012, 'learning_rate': 3.271738068780782e-10, 'epoch': 4.0}\n",
            "{'eval_loss': 0.5002848505973816, 'eval_accuracy': 0.759478672985782, 'eval_f1': 0.7586652299154147, 'eval_precision': 0.7584481921607531, 'eval_recall': 0.759019697159869, 'eval_runtime': 0.6111, 'eval_samples_per_second': 2762.15, 'eval_steps_per_second': 44.181, 'epoch': 4.0}\n",
            "{'train_runtime': 40.0824, 'train_samples_per_second': 505.06, 'train_steps_per_second': 15.867, 'train_loss': 0.510826218802974, 'epoch': 4.0}\n",
            "{'eval_loss': 0.4914429187774658, 'eval_accuracy': 0.7719194312796208, 'eval_f1': 0.7715807317725549, 'eval_precision': 0.77162376170901, 'eval_recall': 0.7728438118312704, 'eval_runtime': 0.6436, 'eval_samples_per_second': 2622.694, 'eval_steps_per_second': 41.951, 'epoch': 4.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-12 15:25:37,342] Trial 14 finished with value: 0.7715807317725549 and parameters: {'learning_rate': 1.9756762536402225e-05, 'weight_decay': 0.0, 'per_device_train_batch_size': 32, 'warmup_steps': 250, 'num_train_epochs': 4, 'adam_beta1': 0.921587501784025, 'adam_beta2': 0.9854277966810029, 'adam_epsilon': 6.708841486290775e-08, 'gradient_accumulation_steps': 1, 'lr_scheduler_type': 'cosine', 'label_smoothing_factor': 0.13170175984471894}. Best is trial 13 with value: 0.7795601988373073.\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.5829, 'grad_norm': 6.254123210906982, 'learning_rate': 1.8115362468029128e-05, 'epoch': 1.0}\n",
            "{'eval_loss': 0.501297116279602, 'eval_accuracy': 0.7713270142180095, 'eval_f1': 0.7702419841686272, 'eval_precision': 0.7702872942985519, 'eval_recall': 0.7701988613526972, 'eval_runtime': 0.6048, 'eval_samples_per_second': 2790.78, 'eval_steps_per_second': 44.639, 'epoch': 1.0}\n",
            "{'loss': 0.4919, 'grad_norm': 5.248583793640137, 'learning_rate': 1.1412661954227291e-05, 'epoch': 2.0}\n",
            "{'eval_loss': 0.4910103678703308, 'eval_accuracy': 0.7748815165876777, 'eval_f1': 0.7746508957893894, 'eval_precision': 0.7750050187624504, 'eval_recall': 0.7762615870831082, 'eval_runtime': 0.6116, 'eval_samples_per_second': 2760.112, 'eval_steps_per_second': 44.149, 'epoch': 2.0}\n",
            "{'loss': 0.401, 'grad_norm': 25.485544204711914, 'learning_rate': 3.49031458902303e-06, 'epoch': 3.0}\n",
            "{'eval_loss': 0.5476474761962891, 'eval_accuracy': 0.7695497630331753, 'eval_f1': 0.7691861272367133, 'eval_precision': 0.7691874603361807, 'eval_recall': 0.7703829008288123, 'eval_runtime': 0.6115, 'eval_samples_per_second': 2760.532, 'eval_steps_per_second': 44.155, 'epoch': 3.0}\n",
            "{'loss': 0.3181, 'grad_norm': 18.02618980407715, 'learning_rate': 8.281647440301126e-11, 'epoch': 4.0}\n",
            "{'eval_loss': 0.6060979962348938, 'eval_accuracy': 0.7636255924170616, 'eval_f1': 0.7624845053435626, 'eval_precision': 0.7625519234242369, 'eval_recall': 0.7624219595056743, 'eval_runtime': 0.6057, 'eval_samples_per_second': 2786.754, 'eval_steps_per_second': 44.575, 'epoch': 4.0}\n",
            "{'train_runtime': 102.6049, 'train_samples_per_second': 197.301, 'train_steps_per_second': 24.677, 'train_loss': 0.44849137822975293, 'epoch': 4.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-12 15:27:21,936] Trial 15 finished with value: 0.7746508957893894 and parameters: {'learning_rate': 1.9420766743318976e-05, 'weight_decay': 0.01, 'per_device_train_batch_size': 8, 'warmup_steps': 250, 'num_train_epochs': 4, 'adam_beta1': 0.885612795570779, 'adam_beta2': 0.9674271898660894, 'adam_epsilon': 2.942234386522296e-07, 'gradient_accumulation_steps': 1, 'lr_scheduler_type': 'cosine', 'label_smoothing_factor': 0.19921642063594044}. Best is trial 13 with value: 0.7795601988373073.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.4910103678703308, 'eval_accuracy': 0.7748815165876777, 'eval_f1': 0.7746508957893894, 'eval_precision': 0.7750050187624504, 'eval_recall': 0.7762615870831082, 'eval_runtime': 0.6515, 'eval_samples_per_second': 2590.746, 'eval_steps_per_second': 41.44, 'epoch': 4.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.6361, 'grad_norm': 1.143227458000183, 'learning_rate': 1.0322679049677963e-05, 'epoch': 1.0}\n",
            "{'eval_loss': 0.5889102816581726, 'eval_accuracy': 0.7014218009478673, 'eval_f1': 0.697083894122925, 'eval_precision': 0.7015699157108739, 'eval_recall': 0.6965167884899879, 'eval_runtime': 0.6116, 'eval_samples_per_second': 2759.927, 'eval_steps_per_second': 44.146, 'epoch': 1.0}\n",
            "{'loss': 0.5443, 'grad_norm': 2.2770068645477295, 'learning_rate': 1.572076075113325e-05, 'epoch': 2.0}\n",
            "{'eval_loss': 0.512201726436615, 'eval_accuracy': 0.7535545023696683, 'eval_f1': 0.753465902707561, 'eval_precision': 0.760006530241406, 'eval_recall': 0.7582934111047022, 'eval_runtime': 0.6071, 'eval_samples_per_second': 2780.653, 'eval_steps_per_second': 44.477, 'epoch': 2.0}\n",
            "{'loss': 0.4853, 'grad_norm': 2.935288906097412, 'learning_rate': 1.0234439483348967e-05, 'epoch': 3.0}\n",
            "{'eval_loss': 0.49727341532707214, 'eval_accuracy': 0.7624407582938388, 'eval_f1': 0.7624366729313743, 'eval_precision': 0.7665339432960578, 'eval_recall': 0.7662155701627587, 'eval_runtime': 0.631, 'eval_samples_per_second': 2675.191, 'eval_steps_per_second': 42.79, 'epoch': 3.0}\n",
            "{'loss': 0.4228, 'grad_norm': 4.050345420837402, 'learning_rate': 3.101413875076248e-06, 'epoch': 4.0}\n",
            "{'eval_loss': 0.49892884492874146, 'eval_accuracy': 0.7588862559241706, 'eval_f1': 0.7585913306438152, 'eval_precision': 0.7588076907958649, 'eval_recall': 0.7599920743152815, 'eval_runtime': 0.6146, 'eval_samples_per_second': 2746.348, 'eval_steps_per_second': 43.929, 'epoch': 4.0}\n",
            "{'loss': 0.3824, 'grad_norm': 5.559894561767578, 'learning_rate': 1.3820582754612548e-10, 'epoch': 4.973143759873618}\n",
            "{'eval_loss': 0.5048491954803467, 'eval_accuracy': 0.7571090047393365, 'eval_f1': 0.7569124423963134, 'eval_precision': 0.7574983678598255, 'eval_recall': 0.7586487976792693, 'eval_runtime': 0.6113, 'eval_samples_per_second': 2761.351, 'eval_steps_per_second': 44.169, 'epoch': 4.973143759873618}\n",
            "{'train_runtime': 102.3147, 'train_samples_per_second': 247.325, 'train_steps_per_second': 7.721, 'train_loss': 0.4948848096630241, 'epoch': 4.973143759873618}\n",
            "{'eval_loss': 0.49727341532707214, 'eval_accuracy': 0.7624407582938388, 'eval_f1': 0.7624366729313743, 'eval_precision': 0.7665339432960578, 'eval_recall': 0.7662155701627587, 'eval_runtime': 0.6441, 'eval_samples_per_second': 2620.782, 'eval_steps_per_second': 41.92, 'epoch': 4.973143759873618}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-12 15:29:06,919] Trial 16 finished with value: 0.7624366729313743 and parameters: {'learning_rate': 1.633335292670564e-05, 'weight_decay': 0.0, 'per_device_train_batch_size': 8, 'warmup_steps': 250, 'num_train_epochs': 5, 'adam_beta1': 0.9020821515256727, 'adam_beta2': 0.9610666659963336, 'adam_epsilon': 4.1844462122069103e-08, 'gradient_accumulation_steps': 4, 'lr_scheduler_type': 'cosine', 'label_smoothing_factor': 0.19620793954527482}. Best is trial 13 with value: 0.7795601988373073.\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.5828, 'grad_norm': 6.171378135681152, 'learning_rate': 2.005630766532253e-05, 'epoch': 1.0}\n",
            "{'eval_loss': 0.503715991973877, 'eval_accuracy': 0.7731042654028436, 'eval_f1': 0.7722890322889688, 'eval_precision': 0.7720833333333332, 'eval_recall': 0.7725871437496385, 'eval_runtime': 0.6099, 'eval_samples_per_second': 2767.671, 'eval_steps_per_second': 44.27, 'epoch': 1.0}\n",
            "{'loss': 0.4917, 'grad_norm': 5.346166133880615, 'learning_rate': 1.2620882649045271e-05, 'epoch': 2.0}\n",
            "{'eval_loss': 0.4869488775730133, 'eval_accuracy': 0.7754739336492891, 'eval_f1': 0.7754700724214236, 'eval_precision': 0.7784516423511909, 'eval_recall': 0.7787457674446154, 'eval_runtime': 0.6167, 'eval_samples_per_second': 2737.322, 'eval_steps_per_second': 43.784, 'epoch': 2.0}\n",
            "{'loss': 0.3954, 'grad_norm': 24.621166229248047, 'learning_rate': 3.864279468310614e-06, 'epoch': 3.0}\n",
            "{'eval_loss': 0.5339842438697815, 'eval_accuracy': 0.7808056872037915, 'eval_f1': 0.7799047108905867, 'eval_precision': 0.779786821841955, 'eval_recall': 0.7800425053625295, 'eval_runtime': 0.607, 'eval_samples_per_second': 2780.991, 'eval_steps_per_second': 44.483, 'epoch': 3.0}\n",
            "{'loss': 0.3013, 'grad_norm': 31.482799530029297, 'learning_rate': 9.168972982547283e-11, 'epoch': 4.0}\n",
            "{'eval_loss': 0.6330715417861938, 'eval_accuracy': 0.7701421800947867, 'eval_f1': 0.7690134089482334, 'eval_precision': 0.7691073431976461, 'eval_recall': 0.7689282133222016, 'eval_runtime': 0.6085, 'eval_samples_per_second': 2774.15, 'eval_steps_per_second': 44.373, 'epoch': 4.0}\n",
            "{'train_runtime': 100.298, 'train_samples_per_second': 201.838, 'train_steps_per_second': 25.245, 'train_loss': 0.44278739338614176, 'epoch': 4.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-12 15:30:49,047] Trial 17 finished with value: 0.7799047108905867 and parameters: {'learning_rate': 2.150157765752098e-05, 'weight_decay': 0.0, 'per_device_train_batch_size': 8, 'warmup_steps': 250, 'num_train_epochs': 4, 'adam_beta1': 0.9359906802486777, 'adam_beta2': 0.9561654926909313, 'adam_epsilon': 4.38379747104442e-07, 'gradient_accumulation_steps': 1, 'lr_scheduler_type': 'cosine', 'label_smoothing_factor': 0.0616757540974705}. Best is trial 17 with value: 0.7799047108905867.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.5339842438697815, 'eval_accuracy': 0.7808056872037915, 'eval_f1': 0.7799047108905867, 'eval_precision': 0.779786821841955, 'eval_recall': 0.7800425053625295, 'eval_runtime': 0.6549, 'eval_samples_per_second': 2577.389, 'eval_steps_per_second': 41.226, 'epoch': 4.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.637, 'grad_norm': 4.710549831390381, 'learning_rate': 1.2430329419603511e-05, 'epoch': 1.0}\n",
            "{'eval_loss': 0.5862286686897278, 'eval_accuracy': 0.7008293838862559, 'eval_f1': 0.6916658558384874, 'eval_precision': 0.7074105051856808, 'eval_recall': 0.6923445218992874, 'eval_runtime': 0.6096, 'eval_samples_per_second': 2769.12, 'eval_steps_per_second': 44.293, 'epoch': 1.0}\n",
            "{'loss': 0.5403, 'grad_norm': 9.075372695922852, 'learning_rate': 1.5734916745710644e-05, 'epoch': 2.0}\n",
            "{'eval_loss': 0.5062986612319946, 'eval_accuracy': 0.7600710900473934, 'eval_f1': 0.7599557580386064, 'eval_precision': 0.7610178443006113, 'eval_recall': 0.7620665729311071, 'eval_runtime': 0.6061, 'eval_samples_per_second': 2785.156, 'eval_steps_per_second': 44.549, 'epoch': 2.0}\n",
            "{'loss': 0.4771, 'grad_norm': inf, 'learning_rate': 9.4177393744498e-10, 'epoch': 3.0}\n",
            "{'eval_loss': 0.5004329085350037, 'eval_accuracy': 0.7760663507109005, 'eval_f1': 0.7758196168375607, 'eval_precision': 0.7761067475274976, 'eval_recall': 0.7773714649965378, 'eval_runtime': 0.6105, 'eval_samples_per_second': 2765.105, 'eval_steps_per_second': 44.229, 'epoch': 3.0}\n",
            "{'train_runtime': 31.4198, 'train_samples_per_second': 483.231, 'train_steps_per_second': 15.182, 'train_loss': 0.5514585067141231, 'epoch': 3.0}\n",
            "{'eval_loss': 0.5004329085350037, 'eval_accuracy': 0.7760663507109005, 'eval_f1': 0.7758196168375607, 'eval_precision': 0.7761067475274976, 'eval_recall': 0.7773714649965378, 'eval_runtime': 0.6618, 'eval_samples_per_second': 2550.811, 'eval_steps_per_second': 40.801, 'epoch': 3.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-12 15:31:22,520] Trial 18 finished with value: 0.7758196168375607 and parameters: {'learning_rate': 1.96682427525372e-05, 'weight_decay': 0.1, 'per_device_train_batch_size': 32, 'warmup_steps': 250, 'num_train_epochs': 3, 'adam_beta1': 0.9315687992711842, 'adam_beta2': 0.9531799238983656, 'adam_epsilon': 4.3608576714100137e-07, 'gradient_accumulation_steps': 1, 'lr_scheduler_type': 'cosine', 'label_smoothing_factor': 0.057097114309103136}. Best is trial 17 with value: 0.7799047108905867.\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.6295, 'grad_norm': 1.058957576751709, 'learning_rate': 2.380600738758027e-05, 'epoch': 1.0}\n",
            "{'eval_loss': 0.5860141515731812, 'eval_accuracy': 0.7008293838862559, 'eval_f1': 0.6888658700342327, 'eval_precision': 0.7122351944261369, 'eval_recall': 0.6907368207286271, 'eval_runtime': 0.6134, 'eval_samples_per_second': 2751.677, 'eval_steps_per_second': 44.014, 'epoch': 1.0}\n",
            "{'loss': 0.5247, 'grad_norm': 2.194319009780884, 'learning_rate': 2.7256627805283703e-05, 'epoch': 2.0}\n",
            "{'eval_loss': 0.5020690560340881, 'eval_accuracy': 0.7719194312796208, 'eval_f1': 0.7718520921340086, 'eval_precision': 0.7733405288720994, 'eval_recall': 0.7742907428848647, 'eval_runtime': 0.6131, 'eval_samples_per_second': 2753.058, 'eval_steps_per_second': 44.036, 'epoch': 2.0}\n",
            "{'loss': 0.4502, 'grad_norm': 3.698544979095459, 'learning_rate': 1.6429111760260868e-05, 'epoch': 3.0}\n",
            "{'eval_loss': 0.4944072663784027, 'eval_accuracy': 0.7577014218009479, 'eval_f1': 0.7574970766889256, 'eval_precision': 0.7580446795606173, 'eval_recall': 0.7592037366359841, 'eval_runtime': 0.6155, 'eval_samples_per_second': 2742.283, 'eval_steps_per_second': 43.864, 'epoch': 3.0}\n",
            "{'loss': 0.3601, 'grad_norm': 2.4408178329467773, 'learning_rate': 4.671368092452849e-06, 'epoch': 4.0}\n",
            "{'eval_loss': 0.527969241142273, 'eval_accuracy': 0.7505924170616114, 'eval_f1': 0.7505727209007416, 'eval_precision': 0.7553312442020952, 'eval_recall': 0.7546344806772654, 'eval_runtime': 0.607, 'eval_samples_per_second': 2781.047, 'eval_steps_per_second': 44.484, 'epoch': 4.0}\n",
            "{'train_runtime': 51.4992, 'train_samples_per_second': 491.367, 'train_steps_per_second': 7.67, 'train_loss': 0.491131854057312, 'epoch': 4.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-12 15:32:16,016] Trial 19 finished with value: 0.7718520921340086 and parameters: {'learning_rate': 3.01341865665573e-05, 'weight_decay': 0.01, 'per_device_train_batch_size': 16, 'warmup_steps': 100, 'num_train_epochs': 5, 'adam_beta1': 0.9376093948942518, 'adam_beta2': 0.9987895327971468, 'adam_epsilon': 6.748299316339818e-07, 'gradient_accumulation_steps': 4, 'lr_scheduler_type': 'cosine_with_restarts', 'label_smoothing_factor': 0.10766897990106575}. Best is trial 17 with value: 0.7799047108905867.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.5020690560340881, 'eval_accuracy': 0.7719194312796208, 'eval_f1': 0.7718520921340086, 'eval_precision': 0.7733405288720994, 'eval_recall': 0.7742907428848647, 'eval_runtime': 0.6528, 'eval_samples_per_second': 2585.756, 'eval_steps_per_second': 41.36, 'epoch': 4.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.5865, 'grad_norm': 5.971297740936279, 'learning_rate': 1.3202720932143119e-05, 'epoch': 1.0}\n",
            "{'eval_loss': 0.5072198510169983, 'eval_accuracy': 0.773696682464455, 'eval_f1': 0.7725853665418174, 'eval_precision': 0.7726805105734426, 'eval_recall': 0.7724990022380893, 'eval_runtime': 0.6099, 'eval_samples_per_second': 2767.498, 'eval_steps_per_second': 44.267, 'epoch': 1.0}\n",
            "{'loss': 0.4963, 'grad_norm': 6.795348167419434, 'learning_rate': 8.308109065397722e-06, 'epoch': 2.0}\n",
            "{'eval_loss': 0.5003267526626587, 'eval_accuracy': 0.7731042654028436, 'eval_f1': 0.7731035487227722, 'eval_precision': 0.7763962109892588, 'eval_recall': 0.7765260116177564, 'eval_runtime': 0.617, 'eval_samples_per_second': 2735.675, 'eval_steps_per_second': 43.758, 'epoch': 2.0}\n",
            "{'loss': 0.4185, 'grad_norm': 21.426677703857422, 'learning_rate': 2.543788431812281e-06, 'epoch': 3.0}\n",
            "{'eval_loss': 0.5206122398376465, 'eval_accuracy': 0.7725118483412322, 'eval_f1': 0.7722787999555935, 'eval_precision': 0.7726353205760836, 'eval_recall': 0.7738810611391832, 'eval_runtime': 0.6111, 'eval_samples_per_second': 2762.158, 'eval_steps_per_second': 44.181, 'epoch': 3.0}\n",
            "{'loss': 0.3455, 'grad_norm': 22.428377151489258, 'learning_rate': 6.035776551844447e-11, 'epoch': 4.0}\n",
            "{'eval_loss': 0.5499762296676636, 'eval_accuracy': 0.764218009478673, 'eval_f1': 0.7633195797258296, 'eval_precision': 0.7631536330825331, 'eval_recall': 0.7635395938721201, 'eval_runtime': 0.6019, 'eval_samples_per_second': 2804.313, 'eval_steps_per_second': 44.856, 'epoch': 4.0}\n",
            "{'train_runtime': 101.0478, 'train_samples_per_second': 200.341, 'train_steps_per_second': 25.057, 'train_loss': 0.46169786618972464, 'epoch': 4.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-12 15:33:59,047] Trial 20 finished with value: 0.7731035487227722 and parameters: {'learning_rate': 1.4154117205924301e-05, 'weight_decay': 0.0, 'per_device_train_batch_size': 8, 'warmup_steps': 250, 'num_train_epochs': 4, 'adam_beta1': 0.9217978033774207, 'adam_beta2': 0.9791520426928013, 'adam_epsilon': 1.2995287605320349e-08, 'gradient_accumulation_steps': 1, 'lr_scheduler_type': 'cosine', 'label_smoothing_factor': 0.12990415520788004}. Best is trial 17 with value: 0.7799047108905867.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.5003267526626587, 'eval_accuracy': 0.7731042654028436, 'eval_f1': 0.7731035487227722, 'eval_precision': 0.7763962109892588, 'eval_recall': 0.7765260116177564, 'eval_runtime': 0.6495, 'eval_samples_per_second': 2598.764, 'eval_steps_per_second': 41.568, 'epoch': 4.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.5823, 'grad_norm': 6.775486469268799, 'learning_rate': 2.171655307180995e-05, 'epoch': 1.0}\n",
            "{'eval_loss': 0.49805548787117004, 'eval_accuracy': 0.7754739336492891, 'eval_f1': 0.7744632170839834, 'eval_precision': 0.7744416243654823, 'eval_recall': 0.7744853593423656, 'eval_runtime': 0.6178, 'eval_samples_per_second': 2732.107, 'eval_steps_per_second': 43.701, 'epoch': 1.0}\n",
            "{'loss': 0.4928, 'grad_norm': 5.78942346572876, 'learning_rate': 1.3670687536931424e-05, 'epoch': 2.0}\n",
            "{'eval_loss': 0.48755696415901184, 'eval_accuracy': 0.7796208530805687, 'eval_f1': 0.7794115167355198, 'eval_precision': 0.7798307200085356, 'eval_recall': 0.7811030240294914, 'eval_runtime': 0.6143, 'eval_samples_per_second': 2747.694, 'eval_steps_per_second': 43.95, 'epoch': 2.0}\n",
            "{'loss': 0.3927, 'grad_norm': 27.1278018951416, 'learning_rate': 4.185710194417354e-06, 'epoch': 3.0}\n",
            "{'eval_loss': 0.5393950343132019, 'eval_accuracy': 0.773696682464455, 'eval_f1': 0.773107741591331, 'eval_precision': 0.7728585583876204, 'eval_recall': 0.7737851631746175, 'eval_runtime': 0.6247, 'eval_samples_per_second': 2702.267, 'eval_steps_per_second': 43.223, 'epoch': 3.0}\n",
            "{'loss': 0.3016, 'grad_norm': 15.594311714172363, 'learning_rate': 9.931648060165754e-11, 'epoch': 4.0}\n",
            "{'eval_loss': 0.6497929096221924, 'eval_accuracy': 0.773696682464455, 'eval_f1': 0.7726597723551553, 'eval_precision': 0.7726597723551553, 'eval_recall': 0.7726597723551553, 'eval_runtime': 0.6317, 'eval_samples_per_second': 2672.341, 'eval_steps_per_second': 42.745, 'epoch': 4.0}\n",
            "{'train_runtime': 100.5478, 'train_samples_per_second': 201.337, 'train_steps_per_second': 25.182, 'train_loss': 0.4423526565045542, 'epoch': 4.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-12 15:35:43,639] Trial 21 finished with value: 0.7794115167355198 and parameters: {'learning_rate': 2.329007866413138e-05, 'weight_decay': 0.0, 'per_device_train_batch_size': 8, 'warmup_steps': 250, 'num_train_epochs': 4, 'adam_beta1': 0.9050095434860893, 'adam_beta2': 0.9557029143223745, 'adam_epsilon': 8.945092161717351e-08, 'gradient_accumulation_steps': 1, 'lr_scheduler_type': 'cosine', 'label_smoothing_factor': 0.03207067260828997}. Best is trial 17 with value: 0.7799047108905867.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.48755696415901184, 'eval_accuracy': 0.7796208530805687, 'eval_f1': 0.7794115167355198, 'eval_precision': 0.7798307200085356, 'eval_recall': 0.7811030240294914, 'eval_runtime': 0.6491, 'eval_samples_per_second': 2600.668, 'eval_steps_per_second': 41.598, 'epoch': 4.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.5844, 'grad_norm': 6.097361087799072, 'learning_rate': 1.6620658469266695e-05, 'epoch': 1.0}\n",
            "{'eval_loss': 0.508326530456543, 'eval_accuracy': 0.7766587677725119, 'eval_f1': 0.7749520375848515, 'eval_precision': 0.7761986038771094, 'eval_recall': 0.7743090763192668, 'eval_runtime': 0.606, 'eval_samples_per_second': 2785.461, 'eval_steps_per_second': 44.554, 'epoch': 1.0}\n",
            "{'loss': 0.4955, 'grad_norm': 5.867090702056885, 'learning_rate': 1.0462794341259648e-05, 'epoch': 2.0}\n",
            "{'eval_loss': 0.4910452961921692, 'eval_accuracy': 0.7742890995260664, 'eval_f1': 0.774279514099107, 'eval_precision': 0.7769522119906508, 'eval_recall': 0.7773947343555868, 'eval_runtime': 0.6095, 'eval_samples_per_second': 2769.336, 'eval_steps_per_second': 44.296, 'epoch': 2.0}\n",
            "{'loss': 0.4092, 'grad_norm': 23.255186080932617, 'learning_rate': 3.2035129775289224e-06, 'epoch': 3.0}\n",
            "{'eval_loss': 0.519985020160675, 'eval_accuracy': 0.7742890995260664, 'eval_f1': 0.7735719121690204, 'eval_precision': 0.7733228113412445, 'eval_recall': 0.7740185618972002, 'eval_runtime': 0.6086, 'eval_samples_per_second': 2773.723, 'eval_steps_per_second': 44.366, 'epoch': 3.0}\n",
            "{'train_runtime': 75.6602, 'train_samples_per_second': 267.565, 'train_steps_per_second': 33.465, 'train_loss': 0.4963461186397446, 'epoch': 3.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-12 15:37:02,067] Trial 22 finished with value: 0.7749520375848515 and parameters: {'learning_rate': 1.7824948642580347e-05, 'weight_decay': 0.0, 'per_device_train_batch_size': 8, 'warmup_steps': 250, 'num_train_epochs': 4, 'adam_beta1': 0.9334161991532788, 'adam_beta2': 0.9695413346478973, 'adam_epsilon': 3.009257314256516e-07, 'gradient_accumulation_steps': 1, 'lr_scheduler_type': 'cosine', 'label_smoothing_factor': 0.0698794348893453}. Best is trial 17 with value: 0.7799047108905867.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.508326530456543, 'eval_accuracy': 0.7766587677725119, 'eval_f1': 0.7749520375848515, 'eval_precision': 0.7761986038771094, 'eval_recall': 0.7743090763192668, 'eval_runtime': 0.6459, 'eval_samples_per_second': 2613.369, 'eval_steps_per_second': 41.802, 'epoch': 3.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.584, 'grad_norm': 6.520965099334717, 'learning_rate': 2.0066499236570247e-05, 'epoch': 1.0}\n",
            "{'eval_loss': 0.5026801824569702, 'eval_accuracy': 0.7754739336492891, 'eval_f1': 0.774498832050357, 'eval_precision': 0.7744368639170027, 'eval_recall': 0.7745657444008986, 'eval_runtime': 0.6071, 'eval_samples_per_second': 2780.634, 'eval_steps_per_second': 44.477, 'epoch': 1.0}\n",
            "{'loss': 0.493, 'grad_norm': 6.162556171417236, 'learning_rate': 1.262729592445335e-05, 'epoch': 2.0}\n",
            "{'eval_loss': 0.4873028099536896, 'eval_accuracy': 0.7808056872037915, 'eval_f1': 0.7807906082800893, 'eval_precision': 0.7832613404608388, 'eval_recall': 0.7838206031135813, 'eval_runtime': 0.6161, 'eval_samples_per_second': 2739.735, 'eval_steps_per_second': 43.823, 'epoch': 2.0}\n",
            "{'loss': 0.3944, 'grad_norm': 19.013883590698242, 'learning_rate': 3.866243093928029e-06, 'epoch': 3.0}\n",
            "{'eval_loss': 0.5397420525550842, 'eval_accuracy': 0.7725118483412322, 'eval_f1': 0.7719737425332968, 'eval_precision': 0.7717500502310629, 'eval_recall': 0.7727556703197209, 'eval_runtime': 0.6094, 'eval_samples_per_second': 2769.736, 'eval_steps_per_second': 44.303, 'epoch': 3.0}\n",
            "{'loss': 0.3031, 'grad_norm': 20.853260040283203, 'learning_rate': 9.173632177199628e-11, 'epoch': 4.0}\n",
            "{'eval_loss': 0.624129593372345, 'eval_accuracy': 0.7713270142180095, 'eval_f1': 0.7703870767669364, 'eval_precision': 0.7702734119072612, 'eval_recall': 0.7705204015868292, 'eval_runtime': 0.6131, 'eval_samples_per_second': 2753.127, 'eval_steps_per_second': 44.037, 'epoch': 4.0}\n",
            "{'train_runtime': 100.7406, 'train_samples_per_second': 200.952, 'train_steps_per_second': 25.134, 'train_loss': 0.4436325868724082, 'epoch': 4.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-12 15:38:44,647] Trial 23 finished with value: 0.7807906082800893 and parameters: {'learning_rate': 2.1512503639725265e-05, 'weight_decay': 0.0, 'per_device_train_batch_size': 8, 'warmup_steps': 250, 'num_train_epochs': 4, 'adam_beta1': 0.917230942911024, 'adam_beta2': 0.9604140740076812, 'adam_epsilon': 4.448566994506484e-08, 'gradient_accumulation_steps': 1, 'lr_scheduler_type': 'cosine', 'label_smoothing_factor': 0.0297022958818125}. Best is trial 23 with value: 0.7807906082800893.\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.4873028099536896, 'eval_accuracy': 0.7808056872037915, 'eval_f1': 0.7807906082800893, 'eval_precision': 0.7832613404608388, 'eval_recall': 0.7838206031135813, 'eval_runtime': 0.6439, 'eval_samples_per_second': 2621.662, 'eval_steps_per_second': 41.934, 'epoch': 4.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.584, 'grad_norm': 6.758755207061768, 'learning_rate': 2.4644075028251478e-05, 'epoch': 1.0}\n",
            "{'eval_loss': 0.49286937713623047, 'eval_accuracy': 0.7784360189573459, 'eval_f1': 0.7776556838481001, 'eval_precision': 0.7774357588527305, 'eval_recall': 0.7779835196527365, 'eval_runtime': 0.6043, 'eval_samples_per_second': 2793.284, 'eval_steps_per_second': 44.679, 'epoch': 1.0}\n",
            "{'loss': 0.4844, 'grad_norm': 5.674421787261963, 'learning_rate': 9.125376992931733e-06, 'epoch': 2.0}\n",
            "{'eval_loss': 0.47962290048599243, 'eval_accuracy': 0.7748815165876777, 'eval_f1': 0.7746334772441087, 'eval_precision': 0.7749219117409645, 'eval_recall': 0.7761812020245753, 'eval_runtime': 0.6174, 'eval_samples_per_second': 2733.979, 'eval_steps_per_second': 43.731, 'epoch': 2.0}\n",
            "{'loss': 0.375, 'grad_norm': 35.17387771606445, 'learning_rate': 2.3044832775821144e-10, 'epoch': 3.0}\n",
            "{'eval_loss': 0.5525879263877869, 'eval_accuracy': 0.7760663507109005, 'eval_f1': 0.7753697578038858, 'eval_precision': 0.7751147380961423, 'eval_recall': 0.7758441488844106, 'eval_runtime': 0.6112, 'eval_samples_per_second': 2761.629, 'eval_steps_per_second': 44.173, 'epoch': 3.0}\n",
            "{'train_runtime': 75.6828, 'train_samples_per_second': 200.614, 'train_steps_per_second': 25.092, 'train_loss': 0.48113791097648023, 'epoch': 3.0}\n",
            "{'eval_loss': 0.49286937713623047, 'eval_accuracy': 0.7784360189573459, 'eval_f1': 0.7776556838481001, 'eval_precision': 0.7774357588527305, 'eval_recall': 0.7779835196527365, 'eval_runtime': 0.6847, 'eval_samples_per_second': 2465.32, 'eval_steps_per_second': 39.433, 'epoch': 3.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-12 15:40:02,380] Trial 24 finished with value: 0.7776556838481001 and parameters: {'learning_rate': 2.8218491335262325e-05, 'weight_decay': 0.0, 'per_device_train_batch_size': 8, 'warmup_steps': 250, 'num_train_epochs': 3, 'adam_beta1': 0.9179084215239781, 'adam_beta2': 0.9606401521730592, 'adam_epsilon': 1.9030283754497298e-08, 'gradient_accumulation_steps': 1, 'lr_scheduler_type': 'cosine', 'label_smoothing_factor': 0.036206898372993124}. Best is trial 23 with value: 0.7807906082800893.\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.5831, 'grad_norm': 5.905630111694336, 'learning_rate': 2.0274928133159198e-05, 'epoch': 1.0}\n",
            "{'eval_loss': 0.5042816996574402, 'eval_accuracy': 0.7742890995260664, 'eval_f1': 0.7729631380345909, 'eval_precision': 0.7733980813177366, 'eval_recall': 0.7726520159021389, 'eval_runtime': 0.6079, 'eval_samples_per_second': 2776.616, 'eval_steps_per_second': 44.413, 'epoch': 1.0}\n",
            "{'loss': 0.4959, 'grad_norm': 5.540457248687744, 'learning_rate': 1.5442335800997908e-05, 'epoch': 2.0}\n",
            "{'eval_loss': 0.48731717467308044, 'eval_accuracy': 0.7790284360189573, 'eval_f1': 0.7789333942858427, 'eval_precision': 0.7800989531531836, 'eval_recall': 0.7811911655410408, 'eval_runtime': 0.6165, 'eval_samples_per_second': 2738.232, 'eval_steps_per_second': 43.799, 'epoch': 2.0}\n",
            "{'loss': 0.4028, 'grad_norm': 12.045178413391113, 'learning_rate': 8.443062302602019e-06, 'epoch': 3.0}\n",
            "{'eval_loss': 0.5442660450935364, 'eval_accuracy': 0.7754739336492891, 'eval_f1': 0.7751803383652718, 'eval_precision': 0.7753200808625337, 'eval_recall': 0.776575370864224, 'eval_runtime': 0.6129, 'eval_samples_per_second': 2754.082, 'eval_steps_per_second': 44.052, 'epoch': 3.0}\n",
            "{'loss': 0.3042, 'grad_norm': 17.025440216064453, 'learning_rate': 2.3961415256039493e-06, 'epoch': 4.0}\n",
            "{'eval_loss': 0.7342889308929443, 'eval_accuracy': 0.7648104265402843, 'eval_f1': 0.7646577079152026, 'eval_precision': 0.7654435925520263, 'eval_recall': 0.7665864696433583, 'eval_runtime': 0.6132, 'eval_samples_per_second': 2752.931, 'eval_steps_per_second': 44.034, 'epoch': 4.0}\n",
            "{'train_runtime': 100.2826, 'train_samples_per_second': 252.337, 'train_steps_per_second': 31.561, 'train_loss': 0.4464995337511879, 'epoch': 4.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-12 15:41:44,730] Trial 25 finished with value: 0.7789333942858427 and parameters: {'learning_rate': 2.1154147974264716e-05, 'weight_decay': 0.0, 'per_device_train_batch_size': 8, 'warmup_steps': 250, 'num_train_epochs': 5, 'adam_beta1': 0.9270969776764544, 'adam_beta2': 0.9502766279622683, 'adam_epsilon': 4.418512984309781e-08, 'gradient_accumulation_steps': 1, 'lr_scheduler_type': 'cosine', 'label_smoothing_factor': 0.06913813492382653}. Best is trial 23 with value: 0.7807906082800893.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.48731717467308044, 'eval_accuracy': 0.7790284360189573, 'eval_f1': 0.7789333942858427, 'eval_precision': 0.7800989531531836, 'eval_recall': 0.7811911655410408, 'eval_runtime': 0.6437, 'eval_samples_per_second': 2622.337, 'eval_steps_per_second': 41.945, 'epoch': 4.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.5849, 'grad_norm': 5.856456756591797, 'learning_rate': 1.6778527946815643e-05, 'epoch': 1.0}\n",
            "{'eval_loss': 0.5033971667289734, 'eval_accuracy': 0.7748815165876777, 'eval_f1': 0.7739214533540295, 'eval_precision': 0.7738408827289166, 'eval_recall': 0.7740108054441839, 'eval_runtime': 0.6111, 'eval_samples_per_second': 2762.301, 'eval_steps_per_second': 44.184, 'epoch': 1.0}\n",
            "{'loss': 0.4907, 'grad_norm': 6.923046588897705, 'learning_rate': 1.0562174030663109e-05, 'epoch': 2.0}\n",
            "{'eval_loss': 0.49012312293052673, 'eval_accuracy': 0.7772511848341233, 'eval_f1': 0.7772311701147876, 'eval_precision': 0.7795512590497499, 'eval_recall': 0.7801694291391605, 'eval_runtime': 0.6102, 'eval_samples_per_second': 2766.358, 'eval_steps_per_second': 44.249, 'epoch': 2.0}\n",
            "{'loss': 0.3999, 'grad_norm': 16.52829360961914, 'learning_rate': 3.2434581282841475e-06, 'epoch': 3.0}\n",
            "{'eval_loss': 0.5191471576690674, 'eval_accuracy': 0.7754739336492891, 'eval_f1': 0.7750532258092873, 'eval_precision': 0.7749325918436131, 'eval_recall': 0.7760930605130258, 'eval_runtime': 0.6118, 'eval_samples_per_second': 2759.055, 'eval_steps_per_second': 44.132, 'epoch': 3.0}\n",
            "{'loss': 0.3128, 'grad_norm': 23.842741012573242, 'learning_rate': 1.3641473273512827e-10, 'epoch': 4.0}\n",
            "{'eval_loss': 0.5873845815658569, 'eval_accuracy': 0.7742890995260664, 'eval_f1': 0.7731233219141476, 'eval_precision': 0.773300011295606, 'eval_recall': 0.7729735561362711, 'eval_runtime': 0.6092, 'eval_samples_per_second': 2770.695, 'eval_steps_per_second': 44.318, 'epoch': 4.0}\n",
            "{'train_runtime': 100.0756, 'train_samples_per_second': 202.287, 'train_steps_per_second': 25.301, 'train_loss': 0.4470808441981697, 'epoch': 4.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-12 15:43:26,783] Trial 26 finished with value: 0.7772311701147876 and parameters: {'learning_rate': 1.7994256936517338e-05, 'weight_decay': 0.0, 'per_device_train_batch_size': 8, 'warmup_steps': 250, 'num_train_epochs': 4, 'adam_beta1': 0.9415989083990041, 'adam_beta2': 0.960843654674804, 'adam_epsilon': 5.002492849665034e-08, 'gradient_accumulation_steps': 1, 'lr_scheduler_type': 'cosine', 'label_smoothing_factor': 0.17991534724721348}. Best is trial 23 with value: 0.7807906082800893.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.49012312293052673, 'eval_accuracy': 0.7772511848341233, 'eval_f1': 0.7772311701147876, 'eval_precision': 0.7795512590497499, 'eval_recall': 0.7801694291391605, 'eval_runtime': 0.6486, 'eval_samples_per_second': 2602.663, 'eval_steps_per_second': 41.63, 'epoch': 4.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.5772, 'grad_norm': 6.19439697265625, 'learning_rate': 1.0538665837700766e-05, 'epoch': 1.0}\n",
            "{'eval_loss': 0.5072512030601501, 'eval_accuracy': 0.7707345971563981, 'eval_f1': 0.7700061155102649, 'eval_precision': 0.7697610888855129, 'eval_recall': 0.7704477729813126, 'eval_runtime': 0.6083, 'eval_samples_per_second': 2774.739, 'eval_steps_per_second': 44.383, 'epoch': 1.0}\n",
            "{'loss': 0.4899, 'grad_norm': 9.127567291259766, 'learning_rate': 3.6513234265083563e-06, 'epoch': 2.0}\n",
            "{'eval_loss': 0.49589163064956665, 'eval_accuracy': 0.7683649289099526, 'eval_f1': 0.7683648476157486, 'eval_precision': 0.7719698380189909, 'eval_recall': 0.7719257298469722, 'eval_runtime': 0.6129, 'eval_samples_per_second': 2754.278, 'eval_steps_per_second': 44.055, 'epoch': 2.0}\n",
            "{'loss': 0.4247, 'grad_norm': 25.185848236083984, 'learning_rate': 4.017264283223365e-11, 'epoch': 3.0}\n",
            "{'eval_loss': 0.5057586431503296, 'eval_accuracy': 0.7725118483412322, 'eval_f1': 0.7720972097209722, 'eval_precision': 0.7719976239659266, 'eval_recall': 0.7731575956123862, 'eval_runtime': 0.6104, 'eval_samples_per_second': 2765.412, 'eval_steps_per_second': 44.233, 'epoch': 3.0}\n",
            "{'train_runtime': 75.8457, 'train_samples_per_second': 200.183, 'train_steps_per_second': 25.038, 'train_loss': 0.49723500135762244, 'epoch': 3.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-12 15:44:44,413] Trial 27 finished with value: 0.7720972097209722 and parameters: {'learning_rate': 1.3173264936788414e-05, 'weight_decay': 0.0, 'per_device_train_batch_size': 8, 'warmup_steps': 100, 'num_train_epochs': 3, 'adam_beta1': 0.9288951773130764, 'adam_beta2': 0.9565612365176811, 'adam_epsilon': 8.993066222923483e-08, 'gradient_accumulation_steps': 1, 'lr_scheduler_type': 'cosine_with_restarts', 'label_smoothing_factor': 0.04579729531845956}. Best is trial 23 with value: 0.7807906082800893.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.5057586431503296, 'eval_accuracy': 0.7725118483412322, 'eval_f1': 0.7720972097209722, 'eval_precision': 0.7719976239659266, 'eval_recall': 0.7731575956123862, 'eval_runtime': 0.6403, 'eval_samples_per_second': 2636.313, 'eval_steps_per_second': 42.169, 'epoch': 3.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.6663, 'grad_norm': 1.4321751594543457, 'learning_rate': 3.5109098985625004e-06, 'epoch': 1.0}\n",
            "{'eval_loss': 0.632268488407135, 'eval_accuracy': 0.6718009478672986, 'eval_f1': 0.6690463146599623, 'eval_precision': 0.6701677414582169, 'eval_recall': 0.6686894555957168, 'eval_runtime': 0.6088, 'eval_samples_per_second': 2772.622, 'eval_steps_per_second': 44.349, 'epoch': 1.0}\n",
            "{'loss': 0.6046, 'grad_norm': 2.7002735137939453, 'learning_rate': 7.066261694575159e-06, 'epoch': 2.0}\n",
            "{'eval_loss': 0.5800911784172058, 'eval_accuracy': 0.7031990521327014, 'eval_f1': 0.7031814472610493, 'eval_precision': 0.7074265375242424, 'eval_recall': 0.7069435767402308, 'eval_runtime': 0.6164, 'eval_samples_per_second': 2738.483, 'eval_steps_per_second': 43.803, 'epoch': 2.0}\n",
            "{'loss': 0.5548, 'grad_norm': 6.00657844543457, 'learning_rate': 1.0621613490587818e-05, 'epoch': 3.0}\n",
            "{'eval_loss': 0.5271294713020325, 'eval_accuracy': 0.7446682464454977, 'eval_f1': 0.7446638554449434, 'eval_precision': 0.7475374593568092, 'eval_recall': 0.7477989301735894, 'eval_runtime': 0.6077, 'eval_samples_per_second': 2777.469, 'eval_steps_per_second': 44.426, 'epoch': 3.0}\n",
            "{'loss': 0.5067, 'grad_norm': 3.046297550201416, 'learning_rate': 1.3999197696799843e-05, 'epoch': 3.9559748427672954}\n",
            "{'eval_loss': 0.5039567947387695, 'eval_accuracy': 0.7665876777251185, 'eval_f1': 0.7663485603711038, 'eval_precision': 0.7667110751101671, 'eval_recall': 0.7679297462793705, 'eval_runtime': 0.6097, 'eval_samples_per_second': 2768.661, 'eval_steps_per_second': 44.285, 'epoch': 3.9559748427672954}\n",
            "{'train_runtime': 37.3095, 'train_samples_per_second': 542.596, 'train_steps_per_second': 8.47, 'train_loss': 0.5840665720686128, 'epoch': 3.9559748427672954}\n",
            "{'eval_loss': 0.5039567947387695, 'eval_accuracy': 0.7665876777251185, 'eval_f1': 0.7663485603711038, 'eval_precision': 0.7667110751101671, 'eval_recall': 0.7679297462793705, 'eval_runtime': 0.6684, 'eval_samples_per_second': 2525.316, 'eval_steps_per_second': 40.393, 'epoch': 3.9559748427672954}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-12 15:45:23,554] Trial 28 finished with value: 0.7663485603711038 and parameters: {'learning_rate': 2.2220948725079116e-05, 'weight_decay': 0.01, 'per_device_train_batch_size': 32, 'warmup_steps': 500, 'num_train_epochs': 4, 'adam_beta1': 0.9084805846398057, 'adam_beta2': 0.9835620854791276, 'adam_epsilon': 1.93589536733009e-07, 'gradient_accumulation_steps': 2, 'lr_scheduler_type': 'polynomial', 'label_smoothing_factor': 0.018197162772512106}. Best is trial 23 with value: 0.7807906082800893.\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.6075, 'grad_norm': 1.7567919492721558, 'learning_rate': 1.3717242520859978e-05, 'epoch': 1.0}\n",
            "{'eval_loss': 0.5572124123573303, 'eval_accuracy': 0.7209715639810427, 'eval_f1': 0.7190109925452286, 'eval_precision': 0.719787885662432, 'eval_recall': 0.7186078718126268, 'eval_runtime': 0.6059, 'eval_samples_per_second': 2785.973, 'eval_steps_per_second': 44.562, 'epoch': 1.0}\n",
            "{'loss': 0.5238, 'grad_norm': 2.3875575065612793, 'learning_rate': 1.026983935102201e-05, 'epoch': 2.0}\n",
            "{'eval_loss': 0.5191766023635864, 'eval_accuracy': 0.7588862559241706, 'eval_f1': 0.758885494334643, 'eval_precision': 0.7626140057793029, 'eval_recall': 0.7624840111298049, 'eval_runtime': 0.6135, 'eval_samples_per_second': 2751.584, 'eval_steps_per_second': 44.012, 'epoch': 2.0}\n",
            "{'loss': 0.4898, 'grad_norm': 2.8417885303497314, 'learning_rate': 6.822436181184039e-06, 'epoch': 3.0}\n",
            "{'eval_loss': 0.5035222172737122, 'eval_accuracy': 0.7659952606635071, 'eval_f1': 0.7649040819514272, 'eval_precision': 0.7649258362812636, 'eval_recall': 0.7648828705081323, 'eval_runtime': 0.6074, 'eval_samples_per_second': 2778.904, 'eval_steps_per_second': 44.449, 'epoch': 3.0}\n",
            "{'loss': 0.4555, 'grad_norm': 1.5454916954040527, 'learning_rate': 3.375033011346071e-06, 'epoch': 4.0}\n",
            "{'eval_loss': 0.5035380721092224, 'eval_accuracy': 0.7701421800947867, 'eval_f1': 0.7694573512432767, 'eval_precision': 0.7692048076882512, 'eval_recall': 0.7699732190831309, 'eval_runtime': 0.6138, 'eval_samples_per_second': 2750.143, 'eval_steps_per_second': 43.989, 'epoch': 4.0}\n",
            "{'loss': 0.4388, 'grad_norm': 3.6218314170837402, 'learning_rate': 1.4309253962297483e-07, 'epoch': 4.946372239747634}\n",
            "{'eval_loss': 0.4999586343765259, 'eval_accuracy': 0.7618483412322274, 'eval_f1': 0.7614380877247012, 'eval_precision': 0.7613902357171843, 'eval_recall': 0.7625256139232562, 'eval_runtime': 0.6193, 'eval_samples_per_second': 2725.728, 'eval_steps_per_second': 43.599, 'epoch': 4.946372239747634}\n",
            "{'train_runtime': 63.7802, 'train_samples_per_second': 396.753, 'train_steps_per_second': 6.193, 'train_loss': 0.5039094079898883, 'epoch': 4.946372239747634}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-12 15:46:29,332] Trial 29 finished with value: 0.7694573512432767 and parameters: {'learning_rate': 1.712155315107497e-05, 'weight_decay': 0.1, 'per_device_train_batch_size': 16, 'warmup_steps': 0, 'num_train_epochs': 5, 'adam_beta1': 0.9452819674210865, 'adam_beta2': 0.9752829489361733, 'adam_epsilon': 2.143654588710646e-08, 'gradient_accumulation_steps': 4, 'lr_scheduler_type': 'polynomial', 'label_smoothing_factor': 0.1179169839663381}. Best is trial 23 with value: 0.7807906082800893.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.5035380721092224, 'eval_accuracy': 0.7701421800947867, 'eval_f1': 0.7694573512432767, 'eval_precision': 0.7692048076882512, 'eval_recall': 0.7699732190831309, 'eval_runtime': 0.6582, 'eval_samples_per_second': 2564.661, 'eval_steps_per_second': 41.022, 'epoch': 4.946372239747634}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.638, 'grad_norm': 1.1852644681930542, 'learning_rate': 9.408645693909295e-06, 'epoch': 1.0}\n",
            "{'eval_loss': 0.5924587845802307, 'eval_accuracy': 0.6972748815165877, 'eval_f1': 0.6935134799022874, 'eval_precision': 0.6967413248667023, 'eval_recall': 0.6929537560271166, 'eval_runtime': 0.6157, 'eval_samples_per_second': 2741.404, 'eval_steps_per_second': 43.849, 'epoch': 1.0}\n",
            "{'loss': 0.5475, 'grad_norm': 1.8445844650268555, 'learning_rate': 1.3785414851449072e-05, 'epoch': 2.0}\n",
            "{'eval_loss': 0.508601725101471, 'eval_accuracy': 0.7612559241706162, 'eval_f1': 0.7612538294238027, 'eval_precision': 0.7643398268398269, 'eval_recall': 0.764542996839598, 'eval_runtime': 0.6286, 'eval_samples_per_second': 2685.502, 'eval_steps_per_second': 42.955, 'epoch': 2.0}\n",
            "{'loss': 0.4855, 'grad_norm': 2.8098459243774414, 'learning_rate': 5.330445003092999e-06, 'epoch': 3.0}\n",
            "{'eval_loss': 0.4952853322029114, 'eval_accuracy': 0.764218009478673, 'eval_f1': 0.764085535778497, 'eval_precision': 0.7650056808047784, 'eval_recall': 0.7661119157451766, 'eval_runtime': 0.6177, 'eval_samples_per_second': 2732.851, 'eval_steps_per_second': 43.713, 'epoch': 3.0}\n",
            "{'loss': 0.4343, 'grad_norm': 4.563652515411377, 'learning_rate': 2.517216770818344e-10, 'epoch': 3.9794628751974725}\n",
            "{'eval_loss': 0.4982088506221771, 'eval_accuracy': 0.7683649289099526, 'eval_f1': 0.7680209509690105, 'eval_precision': 0.7680689399555226, 'eval_recall': 0.7692730229153828, 'eval_runtime': 0.6078, 'eval_samples_per_second': 2777.016, 'eval_steps_per_second': 44.419, 'epoch': 3.9794628751974725}\n",
            "{'train_runtime': 83.4849, 'train_samples_per_second': 242.487, 'train_steps_per_second': 7.57, 'train_loss': 0.5268958610824391, 'epoch': 3.9794628751974725}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-12 15:47:54,634] Trial 30 finished with value: 0.7680209509690105 and parameters: {'learning_rate': 1.4887097616945089e-05, 'weight_decay': 0.0, 'per_device_train_batch_size': 8, 'warmup_steps': 250, 'num_train_epochs': 4, 'adam_beta1': 0.9178415260207867, 'adam_beta2': 0.97844530783338, 'adam_epsilon': 3.2878335369686354e-08, 'gradient_accumulation_steps': 4, 'lr_scheduler_type': 'cosine', 'label_smoothing_factor': 0.1492912751690832}. Best is trial 23 with value: 0.7807906082800893.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.4982088506221771, 'eval_accuracy': 0.7683649289099526, 'eval_f1': 0.7680209509690105, 'eval_precision': 0.7680689399555226, 'eval_recall': 0.7692730229153828, 'eval_runtime': 0.6503, 'eval_samples_per_second': 2595.725, 'eval_steps_per_second': 41.519, 'epoch': 3.9794628751974725}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.5826, 'grad_norm': 6.138160705566406, 'learning_rate': 2.209541262702716e-05, 'epoch': 1.0}\n",
            "{'eval_loss': 0.49510011076927185, 'eval_accuracy': 0.7766587677725119, 'eval_f1': 0.7755432365388759, 'eval_precision': 0.7756657802769826, 'eval_recall': 0.775434467138729, 'eval_runtime': 0.6065, 'eval_samples_per_second': 2783.258, 'eval_steps_per_second': 44.519, 'epoch': 1.0}\n",
            "{'loss': 0.4913, 'grad_norm': 4.396045207977295, 'learning_rate': 1.3904035304070432e-05, 'epoch': 2.0}\n",
            "{'eval_loss': 0.4878448247909546, 'eval_accuracy': 0.7754739336492891, 'eval_f1': 0.7749297702162392, 'eval_precision': 0.7746943765281173, 'eval_recall': 0.7756911352203608, 'eval_runtime': 0.6049, 'eval_samples_per_second': 2790.367, 'eval_steps_per_second': 44.633, 'epoch': 2.0}\n",
            "{'loss': 0.3873, 'grad_norm': 25.504989624023438, 'learning_rate': 4.257156939514823e-06, 'epoch': 3.0}\n",
            "{'eval_loss': 0.5520023107528687, 'eval_accuracy': 0.7707345971563981, 'eval_f1': 0.7701517775997416, 'eval_precision': 0.769911404928944, 'eval_recall': 0.7708496982739776, 'eval_runtime': 0.6061, 'eval_samples_per_second': 2784.979, 'eval_steps_per_second': 44.546, 'epoch': 3.0}\n",
            "{'train_runtime': 74.949, 'train_samples_per_second': 270.104, 'train_steps_per_second': 33.783, 'train_loss': 0.4870560070035833, 'epoch': 3.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-12 15:49:13,333] Trial 31 finished with value: 0.7755432365388759 and parameters: {'learning_rate': 2.3687621789748513e-05, 'weight_decay': 0.0, 'per_device_train_batch_size': 8, 'warmup_steps': 250, 'num_train_epochs': 4, 'adam_beta1': 0.8953557366736487, 'adam_beta2': 0.9631449260835093, 'adam_epsilon': 1.0175920235469332e-07, 'gradient_accumulation_steps': 1, 'lr_scheduler_type': 'cosine', 'label_smoothing_factor': 0.0034784028619428108}. Best is trial 23 with value: 0.7807906082800893.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.49510011076927185, 'eval_accuracy': 0.7766587677725119, 'eval_f1': 0.7755432365388759, 'eval_precision': 0.7756657802769826, 'eval_recall': 0.775434467138729, 'eval_runtime': 0.6485, 'eval_samples_per_second': 2602.828, 'eval_steps_per_second': 41.633, 'epoch': 3.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.5828, 'grad_norm': 6.264747142791748, 'learning_rate': 1.982874344222806e-05, 'epoch': 1.0}\n",
            "{'eval_loss': 0.502041757106781, 'eval_accuracy': 0.7784360189573459, 'eval_f1': 0.7771546368462667, 'eval_precision': 0.7775595755660651, 'eval_recall': 0.7768581288332743, 'eval_runtime': 0.6107, 'eval_samples_per_second': 2764.194, 'eval_steps_per_second': 44.214, 'epoch': 1.0}\n",
            "{'loss': 0.4922, 'grad_norm': 5.106565475463867, 'learning_rate': 1.249209041677378e-05, 'epoch': 2.0}\n",
            "{'eval_loss': 0.4853045642375946, 'eval_accuracy': 0.7790284360189573, 'eval_f1': 0.7788712820727147, 'eval_precision': 0.7795650611071172, 'eval_recall': 0.7807892402483758, 'eval_runtime': 0.6099, 'eval_samples_per_second': 2767.65, 'eval_steps_per_second': 44.269, 'epoch': 2.0}\n",
            "{'loss': 0.3934, 'grad_norm': 14.711858749389648, 'learning_rate': 3.82043432145208e-06, 'epoch': 3.0}\n",
            "{'eval_loss': 0.5293079614639282, 'eval_accuracy': 0.768957345971564, 'eval_f1': 0.7686247385113705, 'eval_precision': 0.7686957644881909, 'eval_recall': 0.7699083469306305, 'eval_runtime': 0.613, 'eval_samples_per_second': 2753.833, 'eval_steps_per_second': 44.048, 'epoch': 3.0}\n",
            "{'loss': 0.3008, 'grad_norm': 12.51347541809082, 'learning_rate': 9.064939366381974e-11, 'epoch': 4.0}\n",
            "{'eval_loss': 0.6236964464187622, 'eval_accuracy': 0.7695497630331753, 'eval_f1': 0.7687862574319483, 'eval_precision': 0.7685525989959703, 'eval_recall': 0.769177124950817, 'eval_runtime': 0.6123, 'eval_samples_per_second': 2756.938, 'eval_steps_per_second': 44.098, 'epoch': 4.0}\n",
            "{'train_runtime': 100.2143, 'train_samples_per_second': 202.007, 'train_steps_per_second': 25.266, 'train_loss': 0.44231441310985015, 'epoch': 4.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-12 15:50:55,540] Trial 32 finished with value: 0.7788712820727147 and parameters: {'learning_rate': 2.1257615015115014e-05, 'weight_decay': 0.0, 'per_device_train_batch_size': 8, 'warmup_steps': 250, 'num_train_epochs': 4, 'adam_beta1': 0.9003071827879563, 'adam_beta2': 0.9695901908970255, 'adam_epsilon': 1.4089010963776664e-07, 'gradient_accumulation_steps': 1, 'lr_scheduler_type': 'cosine', 'label_smoothing_factor': 0.01767652418953796}. Best is trial 23 with value: 0.7807906082800893.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.4853045642375946, 'eval_accuracy': 0.7790284360189573, 'eval_f1': 0.7788712820727147, 'eval_precision': 0.7795650611071172, 'eval_recall': 0.7807892402483758, 'eval_runtime': 0.6495, 'eval_samples_per_second': 2598.753, 'eval_steps_per_second': 41.568, 'epoch': 4.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.5826, 'grad_norm': 7.400217056274414, 'learning_rate': 2.4277827893228254e-05, 'epoch': 1.0}\n",
            "{'eval_loss': 0.49759337306022644, 'eval_accuracy': 0.7778436018957346, 'eval_f1': 0.7767711516386866, 'eval_precision': 0.7768421527903197, 'eval_recall': 0.7767051151692246, 'eval_runtime': 0.6114, 'eval_samples_per_second': 2760.66, 'eval_steps_per_second': 44.157, 'epoch': 1.0}\n",
            "{'loss': 0.491, 'grad_norm': 3.318420886993408, 'learning_rate': 1.5295009592952885e-05, 'epoch': 2.0}\n",
            "{'eval_loss': 0.4926312267780304, 'eval_accuracy': 0.7754739336492891, 'eval_f1': 0.7745681217944991, 'eval_precision': 0.7744366304329451, 'eval_recall': 0.7747265145179647, 'eval_runtime': 0.6137, 'eval_samples_per_second': 2750.347, 'eval_steps_per_second': 43.993, 'epoch': 2.0}\n",
            "{'loss': 0.39, 'grad_norm': 33.242958068847656, 'learning_rate': 4.691411743258158e-06, 'epoch': 3.0}\n",
            "{'eval_loss': 0.5602683424949646, 'eval_accuracy': 0.7731042654028436, 'eval_f1': 0.7723833132827687, 'eval_precision': 0.7721355705226673, 'eval_recall': 0.7728282989252376, 'eval_runtime': 0.6113, 'eval_samples_per_second': 2761.224, 'eval_steps_per_second': 44.166, 'epoch': 3.0}\n",
            "{'train_runtime': 74.8542, 'train_samples_per_second': 270.446, 'train_steps_per_second': 33.826, 'train_loss': 0.4878891256371569, 'epoch': 3.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-12 15:52:12,382] Trial 33 finished with value: 0.7767711516386866 and parameters: {'learning_rate': 2.6027303256059306e-05, 'weight_decay': 0.0, 'per_device_train_batch_size': 8, 'warmup_steps': 250, 'num_train_epochs': 4, 'adam_beta1': 0.8906235907999336, 'adam_beta2': 0.9578676256736212, 'adam_epsilon': 5.192558890031579e-07, 'gradient_accumulation_steps': 1, 'lr_scheduler_type': 'cosine', 'label_smoothing_factor': 0.06758127267685857}. Best is trial 23 with value: 0.7807906082800893.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.49759337306022644, 'eval_accuracy': 0.7778436018957346, 'eval_f1': 0.7767711516386866, 'eval_precision': 0.7768421527903197, 'eval_recall': 0.7767051151692246, 'eval_runtime': 0.6496, 'eval_samples_per_second': 2598.669, 'eval_steps_per_second': 41.566, 'epoch': 3.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.5779, 'grad_norm': 6.185602188110352, 'learning_rate': 1.7162225790635283e-05, 'epoch': 1.0}\n",
            "{'eval_loss': 0.4932396113872528, 'eval_accuracy': 0.7778436018957346, 'eval_f1': 0.7773051816123737, 'eval_precision': 0.7770662957030042, 'eval_recall': 0.7780716611642859, 'eval_runtime': 0.6181, 'eval_samples_per_second': 2730.985, 'eval_steps_per_second': 43.683, 'epoch': 1.0}\n",
            "{'loss': 0.4854, 'grad_norm': 6.657686710357666, 'learning_rate': 1.0337129077228535e-05, 'epoch': 2.0}\n",
            "{'eval_loss': 0.503876805305481, 'eval_accuracy': 0.7648104265402843, 'eval_f1': 0.7648103439985989, 'eval_precision': 0.7683984698972597, 'eval_recall': 0.7683549409310846, 'eval_runtime': 0.6145, 'eval_samples_per_second': 2746.962, 'eval_steps_per_second': 43.938, 'epoch': 2.0}\n",
            "{'loss': 0.3961, 'grad_norm': 25.132831573486328, 'learning_rate': 3.0856852257917984e-06, 'epoch': 3.0}\n",
            "{'eval_loss': 0.5441128015518188, 'eval_accuracy': 0.7784360189573459, 'eval_f1': 0.7774911146168606, 'eval_precision': 0.777409457286333, 'eval_recall': 0.7775815943600715, 'eval_runtime': 0.6094, 'eval_samples_per_second': 2770.116, 'eval_steps_per_second': 44.309, 'epoch': 3.0}\n",
            "{'loss': 0.3164, 'grad_norm': 19.75795555114746, 'learning_rate': 1.291586246854746e-10, 'epoch': 4.0}\n",
            "{'eval_loss': 0.6017817854881287, 'eval_accuracy': 0.7624407582938388, 'eval_f1': 0.7614090544912748, 'eval_precision': 0.7613501169969834, 'eval_recall': 0.7614728517093107, 'eval_runtime': 0.6145, 'eval_samples_per_second': 2746.757, 'eval_steps_per_second': 43.935, 'epoch': 4.0}\n",
            "{'train_runtime': 100.4611, 'train_samples_per_second': 201.511, 'train_steps_per_second': 25.204, 'train_loss': 0.44394907416513935, 'epoch': 4.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-12 15:53:54,842] Trial 34 finished with value: 0.7774911146168606 and parameters: {'learning_rate': 1.935048179206626e-05, 'weight_decay': 0.0, 'per_device_train_batch_size': 8, 'warmup_steps': 100, 'num_train_epochs': 4, 'adam_beta1': 0.9155437845342477, 'adam_beta2': 0.9893992929650748, 'adam_epsilon': 5.559251702925067e-08, 'gradient_accumulation_steps': 1, 'lr_scheduler_type': 'cosine', 'label_smoothing_factor': 0.08523163049776698}. Best is trial 23 with value: 0.7807906082800893.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.5441128015518188, 'eval_accuracy': 0.7784360189573459, 'eval_f1': 0.7774911146168606, 'eval_precision': 0.777409457286333, 'eval_recall': 0.7775815943600715, 'eval_runtime': 0.6431, 'eval_samples_per_second': 2624.819, 'eval_steps_per_second': 41.985, 'epoch': 4.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.5898, 'grad_norm': 2.8803064823150635, 'learning_rate': 3.43747527872357e-05, 'epoch': 1.0}\n",
            "{'eval_loss': 0.49888452887535095, 'eval_accuracy': 0.7713270142180095, 'eval_f1': 0.7704889853255088, 'eval_precision': 0.7702949834649893, 'eval_recall': 0.7707615567624283, 'eval_runtime': 0.6027, 'eval_samples_per_second': 2800.885, 'eval_steps_per_second': 44.801, 'epoch': 1.0}\n",
            "{'loss': 0.4903, 'grad_norm': 2.427635431289673, 'learning_rate': 1.7183144189840573e-05, 'epoch': 2.0}\n",
            "{'eval_loss': 0.4762943387031555, 'eval_accuracy': 0.7772511848341233, 'eval_f1': 0.7770856425969399, 'eval_precision': 0.7777384638065752, 'eval_recall': 0.7789636532611655, 'eval_runtime': 0.6176, 'eval_samples_per_second': 2732.98, 'eval_steps_per_second': 43.715, 'epoch': 2.0}\n",
            "{'loss': 0.3757, 'grad_norm': 5.5807342529296875, 'learning_rate': 2.084644075545431e-07, 'epoch': 2.9921011058451814}\n",
            "{'eval_loss': 0.5091217160224915, 'eval_accuracy': 0.7725118483412322, 'eval_f1': 0.7712775867617419, 'eval_precision': 0.7715424378432063, 'eval_recall': 0.7710675840905277, 'eval_runtime': 0.6098, 'eval_samples_per_second': 2768.022, 'eval_steps_per_second': 44.275, 'epoch': 2.9921011058451814}\n",
            "{'train_runtime': 66.1316, 'train_samples_per_second': 229.588, 'train_steps_per_second': 14.335, 'train_loss': 0.485617923334178, 'epoch': 2.9921011058451814}\n",
            "{'eval_loss': 0.4762943387031555, 'eval_accuracy': 0.7772511848341233, 'eval_f1': 0.7770856425969399, 'eval_precision': 0.7777384638065752, 'eval_recall': 0.7789636532611655, 'eval_runtime': 0.68, 'eval_samples_per_second': 2482.218, 'eval_steps_per_second': 39.704, 'epoch': 2.9921011058451814}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-12 15:55:03,040] Trial 35 finished with value: 0.7770856425969399 and parameters: {'learning_rate': 3.795407823653563e-05, 'weight_decay': 0.0, 'per_device_train_batch_size': 8, 'warmup_steps': 250, 'num_train_epochs': 3, 'adam_beta1': 0.9069002179336795, 'adam_beta2': 0.9542992404926097, 'adam_epsilon': 9.705486028954303e-07, 'gradient_accumulation_steps': 2, 'lr_scheduler_type': 'polynomial', 'label_smoothing_factor': 0.03864954512220499}. Best is trial 23 with value: 0.7807906082800893.\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.5616, 'grad_norm': 6.9828643798828125, 'learning_rate': 2.081871403124517e-05, 'epoch': 1.0}\n",
            "{'eval_loss': 0.49353310465812683, 'eval_accuracy': 0.7796208530805687, 'eval_f1': 0.7790473295077883, 'eval_precision': 0.7787897314808958, 'eval_recall': 0.7797364780344302, 'eval_runtime': 0.6125, 'eval_samples_per_second': 2755.856, 'eval_steps_per_second': 44.081, 'epoch': 1.0}\n",
            "{'loss': 0.4741, 'grad_norm': 3.2478461265563965, 'learning_rate': 1.2219307051058585e-05, 'epoch': 2.0}\n",
            "{'eval_loss': 0.49573883414268494, 'eval_accuracy': 0.7754739336492891, 'eval_f1': 0.7751405125760996, 'eval_precision': 0.7751785834624976, 'eval_recall': 0.776414600747158, 'eval_runtime': 0.6123, 'eval_samples_per_second': 2756.78, 'eval_steps_per_second': 44.095, 'epoch': 2.0}\n",
            "{'loss': 0.3701, 'grad_norm': 24.3775634765625, 'learning_rate': 3.602234682401774e-06, 'epoch': 3.0}\n",
            "{'eval_loss': 0.5696070790290833, 'eval_accuracy': 0.7808056872037915, 'eval_f1': 0.7801526287629186, 'eval_precision': 0.7798866843572945, 'eval_recall': 0.7806855858307937, 'eval_runtime': 0.6116, 'eval_samples_per_second': 2759.983, 'eval_steps_per_second': 44.147, 'epoch': 3.0}\n",
            "{'loss': 0.2851, 'grad_norm': 23.419597625732422, 'learning_rate': 8.444123232169485e-11, 'epoch': 4.0}\n",
            "{'eval_loss': 0.6692463755607605, 'eval_accuracy': 0.7719194312796208, 'eval_f1': 0.7706210082891697, 'eval_precision': 0.7709828168201788, 'eval_recall': 0.7703518750167468, 'eval_runtime': 0.6214, 'eval_samples_per_second': 2716.428, 'eval_steps_per_second': 43.45, 'epoch': 4.0}\n",
            "{'train_runtime': 100.4371, 'train_samples_per_second': 201.559, 'train_steps_per_second': 25.21, 'train_loss': 0.4227119554275585, 'epoch': 4.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-12 15:56:45,503] Trial 36 finished with value: 0.7801526287629186 and parameters: {'learning_rate': 2.4378119597508688e-05, 'weight_decay': 0.0, 'per_device_train_batch_size': 8, 'warmup_steps': 0, 'num_train_epochs': 4, 'adam_beta1': 0.9235744554589894, 'adam_beta2': 0.9594771425047831, 'adam_epsilon': 2.7496080759930113e-08, 'gradient_accumulation_steps': 1, 'lr_scheduler_type': 'cosine', 'label_smoothing_factor': 0.0256311827712841}. Best is trial 23 with value: 0.7807906082800893.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.5696070790290833, 'eval_accuracy': 0.7808056872037915, 'eval_f1': 0.7801526287629186, 'eval_precision': 0.7798866843572945, 'eval_recall': 0.7806855858307937, 'eval_runtime': 0.66, 'eval_samples_per_second': 2557.48, 'eval_steps_per_second': 40.908, 'epoch': 4.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.569, 'grad_norm': 7.686618328094482, 'learning_rate': 2.3137548846217856e-05, 'epoch': 1.0}\n",
            "{'eval_loss': 0.49593523144721985, 'eval_accuracy': 0.773696682464455, 'eval_f1': 0.7733708977213938, 'eval_precision': 0.7734354270118646, 'eval_recall': 0.7746693988184807, 'eval_runtime': 0.6177, 'eval_samples_per_second': 2732.766, 'eval_steps_per_second': 43.711, 'epoch': 1.0}\n",
            "{'loss': 0.4858, 'grad_norm': 4.681979656219482, 'learning_rate': 1.735772705471987e-05, 'epoch': 2.0}\n",
            "{'eval_loss': 0.4945911467075348, 'eval_accuracy': 0.7659952606635071, 'eval_f1': 0.7659261724491777, 'eval_precision': 0.7674098309048389, 'eval_recall': 0.768339428025052, 'eval_runtime': 0.6168, 'eval_samples_per_second': 2736.587, 'eval_steps_per_second': 43.772, 'epoch': 2.0}\n",
            "{'loss': 0.3929, 'grad_norm': 30.900468826293945, 'learning_rate': 1.1587036103334829e-05, 'epoch': 3.0}\n",
            "{'eval_loss': 0.5829804539680481, 'eval_accuracy': 0.773696682464455, 'eval_f1': 0.7734291878368786, 'eval_precision': 0.7736570574238244, 'eval_recall': 0.7749105539940797, 'eval_runtime': 0.6093, 'eval_samples_per_second': 2770.264, 'eval_steps_per_second': 44.311, 'epoch': 3.0}\n",
            "{'loss': 0.3163, 'grad_norm': 18.648822784423828, 'learning_rate': 5.816345151949792e-06, 'epoch': 4.0}\n",
            "{'eval_loss': 0.7371718287467957, 'eval_accuracy': 0.7624407582938388, 'eval_f1': 0.7622564437583746, 'eval_precision': 0.7628777540028867, 'eval_recall': 0.7640451735823672, 'eval_runtime': 0.6158, 'eval_samples_per_second': 2741.344, 'eval_steps_per_second': 43.849, 'epoch': 4.0}\n",
            "{'loss': 0.2259, 'grad_norm': 0.06625420600175858, 'learning_rate': 4.565420056475504e-08, 'epoch': 5.0}\n",
            "{'eval_loss': 1.0297249555587769, 'eval_accuracy': 0.7577014218009479, 'eval_f1': 0.7565317360539277, 'eval_precision': 0.7565976611883691, 'eval_recall': 0.7564706446458616, 'eval_runtime': 0.6117, 'eval_samples_per_second': 2759.726, 'eval_steps_per_second': 44.143, 'epoch': 5.0}\n",
            "{'train_runtime': 128.2841, 'train_samples_per_second': 197.258, 'train_steps_per_second': 24.672, 'train_loss': 0.39797642905188585, 'epoch': 5.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-12 15:58:56,082] Trial 37 finished with value: 0.7734291878368786 and parameters: {'learning_rate': 2.889910895748994e-05, 'weight_decay': 0.1, 'per_device_train_batch_size': 8, 'warmup_steps': 0, 'num_train_epochs': 5, 'adam_beta1': 0.9251690064083634, 'adam_beta2': 0.9585531533356786, 'adam_epsilon': 3.0358841769253994e-08, 'gradient_accumulation_steps': 1, 'lr_scheduler_type': 'linear', 'label_smoothing_factor': 0.026294809357667398}. Best is trial 23 with value: 0.7807906082800893.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.5829804539680481, 'eval_accuracy': 0.773696682464455, 'eval_f1': 0.7734291878368786, 'eval_precision': 0.7736570574238244, 'eval_recall': 0.7749105539940797, 'eval_runtime': 0.6421, 'eval_samples_per_second': 2628.837, 'eval_steps_per_second': 42.049, 'epoch': 5.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.5722, 'grad_norm': 4.4924492835998535, 'learning_rate': 1.3774035950017284e-05, 'epoch': 1.0}\n",
            "{'eval_loss': 0.510859489440918, 'eval_accuracy': 0.7582938388625592, 'eval_f1': 0.7578284224162303, 'eval_precision': 0.7577182405094045, 'eval_recall': 0.7587940548903026, 'eval_runtime': 0.6081, 'eval_samples_per_second': 2775.88, 'eval_steps_per_second': 44.401, 'epoch': 1.0}\n",
            "{'loss': 0.4931, 'grad_norm': 9.209553718566895, 'learning_rate': 8.080352061311414e-06, 'epoch': 2.0}\n",
            "{'eval_loss': 0.4932234287261963, 'eval_accuracy': 0.7648104265402843, 'eval_f1': 0.7647502382736875, 'eval_precision': 0.7663472188643814, 'eval_recall': 0.7672295501116224, 'eval_runtime': 0.6094, 'eval_samples_per_second': 2769.762, 'eval_steps_per_second': 44.303, 'epoch': 2.0}\n",
            "{'loss': 0.4275, 'grad_norm': 13.50163459777832, 'learning_rate': 2.389143376022667e-06, 'epoch': 3.0}\n",
            "{'eval_loss': 0.4943138360977173, 'eval_accuracy': 0.7719194312796208, 'eval_f1': 0.7706210082891697, 'eval_precision': 0.7709828168201788, 'eval_recall': 0.7703518750167468, 'eval_runtime': 0.6087, 'eval_samples_per_second': 2773.317, 'eval_steps_per_second': 44.36, 'epoch': 3.0}\n",
            "{'loss': 0.3755, 'grad_norm': 11.284701347351074, 'learning_rate': 9.895687088361956e-11, 'epoch': 4.0}\n",
            "{'eval_loss': 0.5040237903594971, 'eval_accuracy': 0.7683649289099526, 'eval_f1': 0.7673589533817667, 'eval_precision': 0.7672986383242648, 'eval_recall': 0.7674241665691235, 'eval_runtime': 0.6072, 'eval_samples_per_second': 2780.009, 'eval_steps_per_second': 44.467, 'epoch': 4.0}\n",
            "{'train_runtime': 60.1117, 'train_samples_per_second': 336.773, 'train_steps_per_second': 21.094, 'train_loss': 0.46706884366104656, 'epoch': 4.0}\n",
            "{'eval_loss': 0.4943138360977173, 'eval_accuracy': 0.7719194312796208, 'eval_f1': 0.7706210082891697, 'eval_precision': 0.7709828168201788, 'eval_recall': 0.7703518750167468, 'eval_runtime': 0.6446, 'eval_samples_per_second': 2618.838, 'eval_steps_per_second': 41.889, 'epoch': 4.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-12 15:59:58,187] Trial 38 finished with value: 0.7706210082891697 and parameters: {'learning_rate': 1.612076341276949e-05, 'weight_decay': 0.0, 'per_device_train_batch_size': 16, 'warmup_steps': 0, 'num_train_epochs': 4, 'adam_beta1': 0.9336816908274063, 'adam_beta2': 0.9704218834717661, 'adam_epsilon': 1.578436482377285e-08, 'gradient_accumulation_steps': 1, 'lr_scheduler_type': 'cosine', 'label_smoothing_factor': 0.06309546815304225}. Best is trial 23 with value: 0.7807906082800893.\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.5846, 'grad_norm': 2.9734444618225098, 'learning_rate': 9.298679249628523e-06, 'epoch': 1.0}\n",
            "{'eval_loss': 0.5299274921417236, 'eval_accuracy': 0.7464454976303317, 'eval_f1': 0.7430515544288383, 'eval_precision': 0.7474851023765549, 'eval_recall': 0.7420683216586963, 'eval_runtime': 0.611, 'eval_samples_per_second': 2762.65, 'eval_steps_per_second': 44.189, 'epoch': 1.0}\n",
            "{'loss': 0.5068, 'grad_norm': 3.811605930328369, 'learning_rate': 3.0817857277919206e-06, 'epoch': 2.0}\n",
            "{'eval_loss': 0.5011488795280457, 'eval_accuracy': 0.7648104265402843, 'eval_f1': 0.7647865695818439, 'eval_precision': 0.7670105355575065, 'eval_recall': 0.7676314754042874, 'eval_runtime': 0.6082, 'eval_samples_per_second': 2775.406, 'eval_steps_per_second': 44.393, 'epoch': 2.0}\n",
            "{'loss': 0.4641, 'grad_norm': 3.7642040252685547, 'learning_rate': 3.4039461883471047e-11, 'epoch': 2.9921011058451814}\n",
            "{'eval_loss': 0.4975951015949249, 'eval_accuracy': 0.7677725118483413, 'eval_f1': 0.7673253470273584, 'eval_precision': 0.7672007606239476, 'eval_recall': 0.7683161586660029, 'eval_runtime': 0.6085, 'eval_samples_per_second': 2773.95, 'eval_steps_per_second': 44.37, 'epoch': 2.9921011058451814}\n",
            "{'train_runtime': 66.3544, 'train_samples_per_second': 228.817, 'train_steps_per_second': 14.287, 'train_loss': 0.518682809821664, 'epoch': 2.9921011058451814}\n",
            "{'eval_loss': 0.4975951015949249, 'eval_accuracy': 0.7677725118483413, 'eval_f1': 0.7673253470273584, 'eval_precision': 0.7672007606239476, 'eval_recall': 0.7683161586660029, 'eval_runtime': 0.6462, 'eval_samples_per_second': 2612.098, 'eval_steps_per_second': 41.781, 'epoch': 2.9921011058451814}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-12 16:01:06,338] Trial 39 finished with value: 0.7673253470273584 and parameters: {'learning_rate': 1.2398238999504697e-05, 'weight_decay': 0.0, 'per_device_train_batch_size': 8, 'warmup_steps': 0, 'num_train_epochs': 3, 'adam_beta1': 0.948897103623991, 'adam_beta2': 0.9741502808379606, 'adam_epsilon': 2.380785455227762e-08, 'gradient_accumulation_steps': 2, 'lr_scheduler_type': 'cosine', 'label_smoothing_factor': 0.04475175160892823}. Best is trial 23 with value: 0.7807906082800893.\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Best Hyperparameters:\n",
            "learning_rate: 2.1512503639725265e-05\n",
            "weight_decay: 0.0\n",
            "per_device_train_batch_size: 8\n",
            "warmup_steps: 250\n",
            "num_train_epochs: 4\n",
            "adam_beta1: 0.917230942911024\n",
            "adam_beta2: 0.9604140740076812\n",
            "adam_epsilon: 4.448566994506484e-08\n",
            "gradient_accumulation_steps: 1\n",
            "lr_scheduler_type: cosine\n",
            "label_smoothing_factor: 0.0297022958818125\n",
            "Best F1: 0.7808\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2532' max='2532' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2532/2532 01:41, Epoch 4/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.584000</td>\n",
              "      <td>0.502680</td>\n",
              "      <td>0.775474</td>\n",
              "      <td>0.774499</td>\n",
              "      <td>0.774437</td>\n",
              "      <td>0.774566</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.493000</td>\n",
              "      <td>0.487303</td>\n",
              "      <td>0.780806</td>\n",
              "      <td>0.780791</td>\n",
              "      <td>0.783261</td>\n",
              "      <td>0.783821</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.394400</td>\n",
              "      <td>0.539742</td>\n",
              "      <td>0.772512</td>\n",
              "      <td>0.771974</td>\n",
              "      <td>0.771750</td>\n",
              "      <td>0.772756</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.303100</td>\n",
              "      <td>0.624130</td>\n",
              "      <td>0.771327</td>\n",
              "      <td>0.770387</td>\n",
              "      <td>0.770273</td>\n",
              "      <td>0.770520</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='27' max='27' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [27/27 00:00]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation Set Results:\n",
            "eval_loss: 0.4873\n",
            "eval_accuracy: 0.7808\n",
            "eval_f1: 0.7808\n",
            "eval_precision: 0.7833\n",
            "eval_recall: 0.7838\n",
            "eval_runtime: 0.6707\n",
            "eval_samples_per_second: 2516.6040\n",
            "eval_steps_per_second: 40.2540\n",
            "epoch: 4.0000\n",
            "\n",
            "ai4bharat/indic-bert\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/507 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1b2bfb75c30c4edfb9bbe7055d14d051"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "spiece.model:   0%|          | 0.00/5.65M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9bb0dcad18f449a194bb55a3112e2dd9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/5061 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7175583ad87247a9bbbdd801c95312ea"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/1688 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e2adcdbd991e41bc8d04a5b685e24656"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-12 16:02:53,923] A new study created in memory with name: no-name-e7a949cb-1746-4888-8b3c-8be3d94f97f7\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/135M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9853fedeaf934e1499efcf9aa32a6cef"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ai4bharat/indic-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ai4bharat/indic-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/135M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f730744cb6e84684a883250a1fbb89cd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.6536, 'grad_norm': 1.853535771369934, 'learning_rate': 2.8924212746483848e-05, 'epoch': 1.0}\n",
            "{'eval_loss': 0.5952516794204712, 'eval_accuracy': 0.6795023696682464, 'eval_f1': 0.6793942387663456, 'eval_precision': 0.6807141050501644, 'eval_recall': 0.6813698460132537, 'eval_runtime': 0.8325, 'eval_samples_per_second': 2027.53, 'eval_steps_per_second': 32.431, 'epoch': 1.0}\n",
            "{'loss': 0.6056, 'grad_norm': 2.381476640701294, 'learning_rate': 4.266065490044498e-05, 'epoch': 2.0}\n",
            "{'eval_loss': 0.6010676026344299, 'eval_accuracy': 0.6481042654028436, 'eval_f1': 0.6359850135052714, 'eval_precision': 0.7001386608094131, 'eval_recall': 0.6634531446775924, 'eval_runtime': 0.8181, 'eval_samples_per_second': 2063.284, 'eval_steps_per_second': 33.003, 'epoch': 2.0}\n",
            "{'loss': 0.5806, 'grad_norm': 13.141846656799316, 'learning_rate': 1.6892864628973534e-05, 'epoch': 3.0}\n",
            "{'eval_loss': 0.5816959738731384, 'eval_accuracy': 0.6380331753554502, 'eval_f1': 0.6202787874905333, 'eval_precision': 0.7065560821484993, 'eval_recall': 0.6556268835841018, 'eval_runtime': 0.8275, 'eval_samples_per_second': 2039.912, 'eval_steps_per_second': 32.629, 'epoch': 3.0}\n",
            "{'train_runtime': 76.2361, 'train_samples_per_second': 265.544, 'train_steps_per_second': 16.58, 'train_loss': 0.6132946796597742, 'epoch': 3.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-12 16:04:12,861] Trial 0 finished with value: 0.6793942387663456 and parameters: {'learning_rate': 4.591144880394261e-05, 'weight_decay': 0.1, 'per_device_train_batch_size': 8, 'warmup_steps': 500, 'num_train_epochs': 4, 'adam_beta1': 0.892147446649597, 'adam_beta2': 0.9531295674726382, 'adam_epsilon': 1.0707736842049479e-07, 'gradient_accumulation_steps': 2, 'lr_scheduler_type': 'cosine', 'label_smoothing_factor': 0.09299242623507475}. Best is trial 0 with value: 0.6793942387663456.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.5952516794204712, 'eval_accuracy': 0.6795023696682464, 'eval_f1': 0.6793942387663456, 'eval_precision': 0.6807141050501644, 'eval_recall': 0.6813698460132537, 'eval_runtime': 0.7975, 'eval_samples_per_second': 2116.593, 'eval_steps_per_second': 33.855, 'epoch': 3.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ai4bharat/indic-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ai4bharat/indic-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.698, 'grad_norm': 0.3575160503387451, 'learning_rate': 1.4535421572841318e-06, 'epoch': 0.9811320754716981}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.6929519176483154, 'eval_accuracy': 0.4662322274881517, 'eval_f1': 0.317979797979798, 'eval_precision': 0.23311611374407584, 'eval_recall': 0.5, 'eval_runtime': 0.7929, 'eval_samples_per_second': 2128.921, 'eval_steps_per_second': 34.053, 'epoch': 0.9811320754716981}\n",
            "{'loss': 0.6965, 'grad_norm': 0.25845980644226074, 'learning_rate': 2.9453354239704777e-06, 'epoch': 1.9811320754716981}\n",
            "{'eval_loss': 0.6898471713066101, 'eval_accuracy': 0.67239336492891, 'eval_f1': 0.6722966414288487, 'eval_precision': 0.6737048030221262, 'eval_recall': 0.6743086532400114, 'eval_runtime': 0.8004, 'eval_samples_per_second': 2108.953, 'eval_steps_per_second': 33.733, 'epoch': 1.9811320754716981}\n",
            "{'loss': 0.6892, 'grad_norm': 0.26744112372398376, 'learning_rate': 4.437128690656824e-06, 'epoch': 2.981132075471698}\n",
            "{'eval_loss': 0.6757391095161438, 'eval_accuracy': 0.6759478672985783, 'eval_f1': 0.6749612330785248, 'eval_precision': 0.6748256075607562, 'eval_recall': 0.6753071202828427, 'eval_runtime': 0.7949, 'eval_samples_per_second': 2123.652, 'eval_steps_per_second': 33.968, 'epoch': 2.981132075471698}\n",
            "{'loss': 0.6644, 'grad_norm': 1.5030840635299683, 'learning_rate': 5.9289219573431695e-06, 'epoch': 3.981132075471698}\n",
            "{'eval_loss': 0.6377637386322021, 'eval_accuracy': 0.6753554502369669, 'eval_f1': 0.6746607767140536, 'eval_precision': 0.674623364400649, 'eval_recall': 0.675314876735859, 'eval_runtime': 0.7927, 'eval_samples_per_second': 2129.383, 'eval_steps_per_second': 34.06, 'epoch': 3.981132075471698}\n",
            "{'loss': 0.627, 'grad_norm': 4.544449329376221, 'learning_rate': 7.420715224029516e-06, 'epoch': 4.981132075471698}\n",
            "{'eval_loss': 0.6077105402946472, 'eval_accuracy': 0.6848341232227488, 'eval_f1': 0.6848341232227488, 'eval_precision': 0.6879719977943468, 'eval_recall': 0.6879719977943468, 'eval_runtime': 0.7959, 'eval_samples_per_second': 2120.848, 'eval_steps_per_second': 33.924, 'epoch': 4.981132075471698}\n",
            "{'train_runtime': 42.9912, 'train_samples_per_second': 588.608, 'train_steps_per_second': 4.536, 'train_loss': 0.6750063969538762, 'epoch': 4.981132075471698}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-12 16:04:57,937] Trial 1 finished with value: 0.6848341232227488 and parameters: {'learning_rate': 1.9125554701106998e-05, 'weight_decay': 0.01, 'per_device_train_batch_size': 32, 'warmup_steps': 500, 'num_train_epochs': 5, 'adam_beta1': 0.9249760043230154, 'adam_beta2': 0.9508032812857025, 'adam_epsilon': 3.401626513612043e-08, 'gradient_accumulation_steps': 4, 'lr_scheduler_type': 'linear', 'label_smoothing_factor': 0.1161410000780913}. Best is trial 1 with value: 0.6848341232227488.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.6077105402946472, 'eval_accuracy': 0.6848341232227488, 'eval_f1': 0.6848341232227488, 'eval_precision': 0.6879719977943468, 'eval_recall': 0.6879719977943468, 'eval_runtime': 0.7915, 'eval_samples_per_second': 2132.661, 'eval_steps_per_second': 34.112, 'epoch': 4.981132075471698}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ai4bharat/indic-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ai4bharat/indic-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.67, 'grad_norm': 1.2130725383758545, 'learning_rate': 2.0260188832174976e-05, 'epoch': 1.0}\n",
            "{'eval_loss': 0.619908332824707, 'eval_accuracy': 0.6806872037914692, 'eval_f1': 0.6800888884200681, 'eval_precision': 0.680111785192675, 'eval_recall': 0.6808720227560229, 'eval_runtime': 0.798, 'eval_samples_per_second': 2115.293, 'eval_steps_per_second': 33.835, 'epoch': 1.0}\n",
            "{'loss': 0.6007, 'grad_norm': 3.754939556121826, 'learning_rate': 1.8668018323086448e-05, 'epoch': 2.0}\n",
            "{'eval_loss': 0.5588499903678894, 'eval_accuracy': 0.6919431279620853, 'eval_f1': 0.6919323160810377, 'eval_precision': 0.6944264899095514, 'eval_recall': 0.6947116503334569, 'eval_runtime': 0.8178, 'eval_samples_per_second': 2064.048, 'eval_steps_per_second': 33.015, 'epoch': 2.0}\n",
            "{'loss': 0.5442, 'grad_norm': 3.3276772499084473, 'learning_rate': 9.20660771259654e-06, 'epoch': 3.0}\n",
            "{'eval_loss': 0.5296058058738708, 'eval_accuracy': 0.7132701421800948, 'eval_f1': 0.7131537665350742, 'eval_precision': 0.7143495357589909, 'eval_recall': 0.7151717631263865, 'eval_runtime': 0.8184, 'eval_samples_per_second': 2062.613, 'eval_steps_per_second': 32.992, 'epoch': 3.0}\n",
            "{'loss': 0.5007, 'grad_norm': 9.139071464538574, 'learning_rate': 3.365352652622488e-07, 'epoch': 3.958990536277603}\n",
            "{'eval_loss': 0.5310315489768982, 'eval_accuracy': 0.7328199052132701, 'eval_f1': 0.7327928032277369, 'eval_precision': 0.7376071640777524, 'eval_recall': 0.7368609211563603, 'eval_runtime': 0.7968, 'eval_samples_per_second': 2118.357, 'eval_steps_per_second': 33.884, 'epoch': 3.958990536277603}\n",
            "{'train_runtime': 54.6717, 'train_samples_per_second': 370.283, 'train_steps_per_second': 5.78, 'train_loss': 0.5798859173738504, 'epoch': 3.958990536277603}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-12 16:05:54,555] Trial 2 finished with value: 0.7327928032277369 and parameters: {'learning_rate': 2.5645808648322754e-05, 'weight_decay': 0.1, 'per_device_train_batch_size': 16, 'warmup_steps': 100, 'num_train_epochs': 4, 'adam_beta1': 0.9018170904176112, 'adam_beta2': 0.9715260575153506, 'adam_epsilon': 5.162813762900972e-07, 'gradient_accumulation_steps': 4, 'lr_scheduler_type': 'polynomial', 'label_smoothing_factor': 0.010297818058220255}. Best is trial 2 with value: 0.7327928032277369.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.5310315489768982, 'eval_accuracy': 0.7328199052132701, 'eval_f1': 0.7327928032277369, 'eval_precision': 0.7376071640777524, 'eval_recall': 0.7368609211563603, 'eval_runtime': 0.8059, 'eval_samples_per_second': 2094.472, 'eval_steps_per_second': 33.502, 'epoch': 3.958990536277603}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ai4bharat/indic-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ai4bharat/indic-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.6245, 'grad_norm': 16.329978942871094, 'learning_rate': 9.52097793130051e-06, 'epoch': 1.0}\n",
            "{'eval_loss': 0.576400876045227, 'eval_accuracy': 0.7037914691943128, 'eval_f1': 0.7019129840205581, 'eval_precision': 0.7023953613143697, 'eval_recall': 0.7016304064240355, 'eval_runtime': 0.815, 'eval_samples_per_second': 2071.229, 'eval_steps_per_second': 33.13, 'epoch': 1.0}\n",
            "{'loss': 0.564, 'grad_norm': 7.9235968589782715, 'learning_rate': 7.173163241796596e-06, 'epoch': 2.0}\n",
            "{'eval_loss': 0.5262362957000732, 'eval_accuracy': 0.740521327014218, 'eval_f1': 0.7372622601279317, 'eval_precision': 0.7410882151976121, 'eval_recall': 0.7363581619744827, 'eval_runtime': 0.813, 'eval_samples_per_second': 2076.179, 'eval_steps_per_second': 33.209, 'epoch': 2.0}\n",
            "{'loss': 0.508, 'grad_norm': 63.279178619384766, 'learning_rate': 4.821633655632076e-06, 'epoch': 3.0}\n",
            "{'eval_loss': 0.5364928841590881, 'eval_accuracy': 0.7618483412322274, 'eval_f1': 0.7585259097411516, 'eval_precision': 0.7635629752113855, 'eval_recall': 0.7573809701771432, 'eval_runtime': 0.8232, 'eval_samples_per_second': 2050.644, 'eval_steps_per_second': 32.801, 'epoch': 3.0}\n",
            "{'loss': 0.4375, 'grad_norm': 62.107261657714844, 'learning_rate': 2.473818966128164e-06, 'epoch': 4.0}\n",
            "{'eval_loss': 0.5691460371017456, 'eval_accuracy': 0.7541469194312796, 'eval_f1': 0.7537111904041165, 'eval_precision': 0.753656584379476, 'eval_recall': 0.7547487120762333, 'eval_runtime': 0.814, 'eval_samples_per_second': 2073.607, 'eval_steps_per_second': 33.168, 'epoch': 4.0}\n",
            "{'loss': 0.3655, 'grad_norm': inf, 'learning_rate': 1.2228937996364497e-07, 'epoch': 5.0}\n",
            "{'eval_loss': 0.6196686625480652, 'eval_accuracy': 0.7541469194312796, 'eval_f1': 0.7520015435294785, 'eval_precision': 0.753707627118644, 'eval_recall': 0.7512921545593136, 'eval_runtime': 0.8219, 'eval_samples_per_second': 2053.854, 'eval_steps_per_second': 32.852, 'epoch': 5.0}\n",
            "{'train_runtime': 131.6166, 'train_samples_per_second': 192.263, 'train_steps_per_second': 24.047, 'train_loss': 0.49989010297103326, 'epoch': 5.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-12 16:08:07,950] Trial 3 finished with value: 0.7585259097411516 and parameters: {'learning_rate': 1.1857647930822598e-05, 'weight_decay': 0.0, 'per_device_train_batch_size': 8, 'warmup_steps': 0, 'num_train_epochs': 5, 'adam_beta1': 0.8781555242633273, 'adam_beta2': 0.9611620581283679, 'adam_epsilon': 2.0387135125420313e-07, 'gradient_accumulation_steps': 1, 'lr_scheduler_type': 'polynomial', 'label_smoothing_factor': 0.12867059313167908}. Best is trial 3 with value: 0.7585259097411516.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.5364928841590881, 'eval_accuracy': 0.7618483412322274, 'eval_f1': 0.7585259097411516, 'eval_precision': 0.7635629752113855, 'eval_recall': 0.7573809701771432, 'eval_runtime': 0.8003, 'eval_samples_per_second': 2109.247, 'eval_steps_per_second': 33.738, 'epoch': 5.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ai4bharat/indic-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ai4bharat/indic-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.6474, 'grad_norm': 4.997060775756836, 'learning_rate': 7.874464551355882e-06, 'epoch': 1.0}\n",
            "{'eval_loss': 0.6151363253593445, 'eval_accuracy': 0.6812796208530806, 'eval_f1': 0.680820793125942, 'eval_precision': 0.6810120078002266, 'eval_recall': 0.6818288870054028, 'eval_runtime': 0.8096, 'eval_samples_per_second': 2084.885, 'eval_steps_per_second': 33.348, 'epoch': 1.0}\n",
            "{'loss': 0.5886, 'grad_norm': 13.811491012573242, 'learning_rate': 7.382310516896139e-08, 'epoch': 2.0}\n",
            "{'eval_loss': 0.564797580242157, 'eval_accuracy': 0.6872037914691943, 'eval_f1': 0.6870913380519044, 'eval_precision': 0.6883692959642327, 'eval_recall': 0.6890663628017437, 'eval_runtime': 0.8102, 'eval_samples_per_second': 2083.453, 'eval_steps_per_second': 33.325, 'epoch': 2.0}\n",
            "{'train_runtime': 29.5064, 'train_samples_per_second': 343.044, 'train_steps_per_second': 21.487, 'train_loss': 0.6180254818889245, 'epoch': 2.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-12 16:08:39,469] Trial 4 finished with value: 0.6870913380519044 and parameters: {'learning_rate': 1.3140512720075128e-05, 'weight_decay': 0.01, 'per_device_train_batch_size': 16, 'warmup_steps': 100, 'num_train_epochs': 2, 'adam_beta1': 0.9428630197894282, 'adam_beta2': 0.9785563399578024, 'adam_epsilon': 2.3828332485457713e-08, 'gradient_accumulation_steps': 1, 'lr_scheduler_type': 'linear', 'label_smoothing_factor': 0.12270119120637493}. Best is trial 3 with value: 0.7585259097411516.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.564797580242157, 'eval_accuracy': 0.6872037914691943, 'eval_f1': 0.6870913380519044, 'eval_precision': 0.6883692959642327, 'eval_recall': 0.6890663628017437, 'eval_runtime': 0.8106, 'eval_samples_per_second': 2082.392, 'eval_steps_per_second': 33.308, 'epoch': 2.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ai4bharat/indic-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ai4bharat/indic-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.6979, 'grad_norm': 0.3560083210468292, 'learning_rate': 1.8432329666686433e-06, 'epoch': 0.9811320754716981}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.6927589178085327, 'eval_accuracy': 0.4662322274881517, 'eval_f1': 0.317979797979798, 'eval_precision': 0.23311611374407584, 'eval_recall': 0.5, 'eval_runtime': 0.8184, 'eval_samples_per_second': 2062.495, 'eval_steps_per_second': 32.99, 'epoch': 0.9811320754716981}\n",
            "{'loss': 0.6954, 'grad_norm': 0.27083542943000793, 'learning_rate': 3.7349720640390932e-06, 'epoch': 1.9811320754716981}\n",
            "{'eval_loss': 0.6863992810249329, 'eval_accuracy': 0.6795023696682464, 'eval_f1': 0.6789659503695269, 'eval_precision': 0.679058456146657, 'eval_recall': 0.6798425299011264, 'eval_runtime': 0.7996, 'eval_samples_per_second': 2110.994, 'eval_steps_per_second': 33.766, 'epoch': 1.9811320754716981}\n",
            "{'loss': 0.6766, 'grad_norm': 0.6069542169570923, 'learning_rate': 5.626711161409543e-06, 'epoch': 2.981132075471698}\n",
            "{'eval_loss': 0.6488041281700134, 'eval_accuracy': 0.6759478672985783, 'eval_f1': 0.674918204610116, 'eval_precision': 0.6747818233993349, 'eval_recall': 0.6752267352243095, 'eval_runtime': 0.7954, 'eval_samples_per_second': 2122.087, 'eval_steps_per_second': 33.943, 'epoch': 2.981132075471698}\n",
            "{'loss': 0.6283, 'grad_norm': 2.2500314712524414, 'learning_rate': 7.518450258779993e-06, 'epoch': 3.981132075471698}\n",
            "{'eval_loss': 0.6227213740348816, 'eval_accuracy': 0.6563981042654028, 'eval_f1': 0.6420459946619868, 'eval_precision': 0.7204980667288621, 'eval_recall': 0.6729907613593255, 'eval_runtime': 0.8281, 'eval_samples_per_second': 2038.445, 'eval_steps_per_second': 32.605, 'epoch': 3.981132075471698}\n",
            "{'train_runtime': 34.5366, 'train_samples_per_second': 732.701, 'train_steps_per_second': 5.646, 'train_loss': 0.6745507411467724, 'epoch': 3.981132075471698}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-12 16:09:15,830] Trial 5 finished with value: 0.6789659503695269 and parameters: {'learning_rate': 2.4253065350903203e-05, 'weight_decay': 0.1, 'per_device_train_batch_size': 32, 'warmup_steps': 500, 'num_train_epochs': 5, 'adam_beta1': 0.8766126189749783, 'adam_beta2': 0.9693435240109636, 'adam_epsilon': 6.440782770344238e-08, 'gradient_accumulation_steps': 4, 'lr_scheduler_type': 'cosine', 'label_smoothing_factor': 0.11750047006881564}. Best is trial 3 with value: 0.7585259097411516.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.6863992810249329, 'eval_accuracy': 0.6795023696682464, 'eval_f1': 0.6789659503695269, 'eval_precision': 0.679058456146657, 'eval_recall': 0.6798425299011264, 'eval_runtime': 0.7949, 'eval_samples_per_second': 2123.618, 'eval_steps_per_second': 33.968, 'epoch': 3.981132075471698}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ai4bharat/indic-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ai4bharat/indic-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.6978, 'grad_norm': 0.35493728518486023, 'learning_rate': 2.6539299241556273e-06, 'epoch': 0.9811320754716981}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.6925614476203918, 'eval_accuracy': 0.4662322274881517, 'eval_f1': 0.317979797979798, 'eval_precision': 0.23311611374407584, 'eval_recall': 0.5, 'eval_runtime': 0.7978, 'eval_samples_per_second': 2115.95, 'eval_steps_per_second': 33.845, 'epoch': 0.9811320754716981}\n",
            "{'loss': 0.6946, 'grad_norm': 0.28617510199546814, 'learning_rate': 5.3777001094732446e-06, 'epoch': 1.9811320754716981}\n",
            "{'eval_loss': 0.6845303177833557, 'eval_accuracy': 0.6777251184834123, 'eval_f1': 0.6768468468468469, 'eval_precision': 0.6767232472324722, 'eval_recall': 0.6772934773871189, 'eval_runtime': 0.7936, 'eval_samples_per_second': 2126.995, 'eval_steps_per_second': 34.022, 'epoch': 1.9811320754716981}\n",
            "{'loss': 0.6736, 'grad_norm': 0.6607808470726013, 'learning_rate': 8.101470294790862e-06, 'epoch': 2.981132075471698}\n",
            "{'eval_loss': 0.6467846035957336, 'eval_accuracy': 0.6741706161137441, 'eval_f1': 0.6730230644825002, 'eval_precision': 0.6728990115230821, 'eval_recall': 0.6732403781200332, 'eval_runtime': 0.794, 'eval_samples_per_second': 2126.032, 'eval_steps_per_second': 34.006, 'epoch': 2.981132075471698}\n",
            "{'loss': 0.628, 'grad_norm': 2.502021074295044, 'learning_rate': 1.082524048010848e-05, 'epoch': 3.981132075471698}\n",
            "{'eval_loss': 0.5955395102500916, 'eval_accuracy': 0.6753554502369669, 'eval_f1': 0.6704473392074126, 'eval_precision': 0.705867993849187, 'eval_recall': 0.6867295550475471, 'eval_runtime': 0.794, 'eval_samples_per_second': 2125.949, 'eval_steps_per_second': 34.005, 'epoch': 3.981132075471698}\n",
            "{'train_runtime': 34.5943, 'train_samples_per_second': 731.479, 'train_steps_per_second': 5.637, 'train_loss': 0.6735094755123823, 'epoch': 3.981132075471698}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-12 16:09:52,376] Trial 6 finished with value: 0.6768468468468469 and parameters: {'learning_rate': 1.746006529049755e-05, 'weight_decay': 0.01, 'per_device_train_batch_size': 32, 'warmup_steps': 250, 'num_train_epochs': 5, 'adam_beta1': 0.9402681317507545, 'adam_beta2': 0.9581031100040748, 'adam_epsilon': 2.472227240207275e-07, 'gradient_accumulation_steps': 4, 'lr_scheduler_type': 'cosine', 'label_smoothing_factor': 0.16478384339343358}. Best is trial 3 with value: 0.7585259097411516.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.6845303177833557, 'eval_accuracy': 0.6777251184834123, 'eval_f1': 0.6768468468468469, 'eval_precision': 0.6767232472324722, 'eval_recall': 0.6772934773871189, 'eval_runtime': 0.7976, 'eval_samples_per_second': 2116.333, 'eval_steps_per_second': 33.851, 'epoch': 3.981132075471698}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ai4bharat/indic-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ai4bharat/indic-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.6617, 'grad_norm': 1.162973165512085, 'learning_rate': 1.2400243353862703e-05, 'epoch': 1.0}\n",
            "{'eval_loss': 0.6318211555480957, 'eval_accuracy': 0.6789099526066351, 'eval_f1': 0.6782916787980314, 'eval_precision': 0.6783021345501069, 'eval_recall': 0.6790464357688126, 'eval_runtime': 0.817, 'eval_samples_per_second': 2066.177, 'eval_steps_per_second': 33.049, 'epoch': 1.0}\n",
            "{'loss': 0.5916, 'grad_norm': 7.565008640289307, 'learning_rate': 7.878142119561367e-10, 'epoch': 1.9921011058451816}\n",
            "{'eval_loss': 0.5628877878189087, 'eval_accuracy': 0.6919431279620853, 'eval_f1': 0.6919392357723348, 'eval_precision': 0.6946944986509049, 'eval_recall': 0.694872420450523, 'eval_runtime': 0.8328, 'eval_samples_per_second': 2026.904, 'eval_steps_per_second': 32.421, 'epoch': 1.9921011058451816}\n",
            "{'train_runtime': 49.0315, 'train_samples_per_second': 206.439, 'train_steps_per_second': 6.445, 'train_loss': 0.6268673425988306, 'epoch': 1.9921011058451816}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-12 16:10:43,227] Trial 7 finished with value: 0.6919392357723348 and parameters: {'learning_rate': 1.4897013633908543e-05, 'weight_decay': 0.0, 'per_device_train_batch_size': 8, 'warmup_steps': 100, 'num_train_epochs': 2, 'adam_beta1': 0.8974500969952888, 'adam_beta2': 0.9955569459757634, 'adam_epsilon': 9.644097276767667e-07, 'gradient_accumulation_steps': 4, 'lr_scheduler_type': 'cosine_with_restarts', 'label_smoothing_factor': 0.1519106759458975}. Best is trial 3 with value: 0.7585259097411516.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.5628877878189087, 'eval_accuracy': 0.6919431279620853, 'eval_f1': 0.6919392357723348, 'eval_precision': 0.6946944986509049, 'eval_recall': 0.694872420450523, 'eval_runtime': 0.8037, 'eval_samples_per_second': 2100.263, 'eval_steps_per_second': 33.594, 'epoch': 1.9921011058451816}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ai4bharat/indic-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ai4bharat/indic-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.6423, 'grad_norm': 15.754006385803223, 'learning_rate': 8.688073705096663e-06, 'epoch': 1.0}\n",
            "{'eval_loss': 0.578274667263031, 'eval_accuracy': 0.6866113744075829, 'eval_f1': 0.6846536742646968, 'eval_precision': 0.6850713315217392, 'eval_recall': 0.6844117858598451, 'eval_runtime': 0.8017, 'eval_samples_per_second': 2105.395, 'eval_steps_per_second': 33.676, 'epoch': 1.0}\n",
            "{'loss': 0.5704, 'grad_norm': 13.865829467773438, 'learning_rate': 4.414259946045419e-06, 'epoch': 2.0}\n",
            "{'eval_loss': 0.5428743958473206, 'eval_accuracy': 0.7132701421800948, 'eval_f1': 0.7113967263519624, 'eval_precision': 0.7119665200858241, 'eval_recall': 0.7110721251412027, 'eval_runtime': 0.8251, 'eval_samples_per_second': 2045.845, 'eval_steps_per_second': 32.724, 'epoch': 2.0}\n",
            "{'loss': 0.5171, 'grad_norm': 17.356517791748047, 'learning_rate': 1.5392824932556818e-07, 'epoch': 3.0}\n",
            "{'eval_loss': 0.5312721133232117, 'eval_accuracy': 0.7221563981042654, 'eval_f1': 0.7205746828249886, 'eval_precision': 0.7208751038963241, 'eval_recall': 0.7203608301943203, 'eval_runtime': 0.8298, 'eval_samples_per_second': 2034.132, 'eval_steps_per_second': 32.536, 'epoch': 3.0}\n",
            "{'train_runtime': 44.7832, 'train_samples_per_second': 339.033, 'train_steps_per_second': 21.236, 'train_loss': 0.5766060685760466, 'epoch': 3.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-12 16:11:29,872] Trial 8 finished with value: 0.7205746828249886 and parameters: {'learning_rate': 1.1573235044014538e-05, 'weight_decay': 0.01, 'per_device_train_batch_size': 16, 'warmup_steps': 100, 'num_train_epochs': 3, 'adam_beta1': 0.9154850893769964, 'adam_beta2': 0.9956933716265864, 'adam_epsilon': 6.368528287132393e-08, 'gradient_accumulation_steps': 1, 'lr_scheduler_type': 'polynomial', 'label_smoothing_factor': 0.013269735884846125}. Best is trial 3 with value: 0.7585259097411516.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.5312721133232117, 'eval_accuracy': 0.7221563981042654, 'eval_f1': 0.7205746828249886, 'eval_precision': 0.7208751038963241, 'eval_recall': 0.7203608301943203, 'eval_runtime': 0.8526, 'eval_samples_per_second': 1979.789, 'eval_steps_per_second': 31.667, 'epoch': 3.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ai4bharat/indic-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ai4bharat/indic-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.6376, 'grad_norm': nan, 'learning_rate': 1.7092167350686092e-05, 'epoch': 1.0}\n",
            "{'eval_loss': 0.5773633122444153, 'eval_accuracy': 0.6812796208530806, 'eval_f1': 0.680411036036036, 'eval_precision': 0.6802825513969426, 'eval_recall': 0.6808642663030066, 'eval_runtime': 0.8142, 'eval_samples_per_second': 2073.254, 'eval_steps_per_second': 33.162, 'epoch': 1.0}\n",
            "{'loss': 0.5629, 'grad_norm': 10.819687843322754, 'learning_rate': 1.287746964046882e-05, 'epoch': 2.0}\n",
            "{'eval_loss': 0.5370059013366699, 'eval_accuracy': 0.731042654028436, 'eval_f1': 0.730824997190064, 'eval_precision': 0.7314290527971021, 'eval_recall': 0.7324630122960935, 'eval_runtime': 0.8553, 'eval_samples_per_second': 1973.559, 'eval_steps_per_second': 31.568, 'epoch': 2.0}\n",
            "{'loss': 0.499, 'grad_norm': 44.95118713378906, 'learning_rate': 8.689447232214949e-06, 'epoch': 3.0}\n",
            "{'eval_loss': 0.5311976671218872, 'eval_accuracy': 0.7393364928909952, 'eval_f1': 0.7392306968500675, 'eval_precision': 0.740435258433817, 'eval_recall': 0.7413575485095623, 'eval_runtime': 0.8166, 'eval_samples_per_second': 2067.075, 'eval_steps_per_second': 33.063, 'epoch': 3.0}\n",
            "{'loss': 0.4277, 'grad_norm': 37.16984558105469, 'learning_rate': 4.448074220034274e-06, 'epoch': 4.0}\n",
            "{'eval_loss': 0.5450423359870911, 'eval_accuracy': 0.7422985781990521, 'eval_f1': 0.7408776240692885, 'eval_precision': 0.7411432899258455, 'eval_recall': 0.7406756857762165, 'eval_runtime': 0.7972, 'eval_samples_per_second': 2117.371, 'eval_steps_per_second': 33.868, 'epoch': 4.0}\n",
            "{'loss': 0.3581, 'grad_norm': 27.076602935791016, 'learning_rate': 2.067012078536018e-07, 'epoch': 5.0}\n",
            "{'eval_loss': 0.5648874044418335, 'eval_accuracy': 0.7417061611374408, 'eval_f1': 0.7409698005521526, 'eval_precision': 0.7407539153064968, 'eval_recall': 0.741487292814563, 'eval_runtime': 0.8015, 'eval_samples_per_second': 2106.032, 'eval_steps_per_second': 33.687, 'epoch': 5.0}\n",
            "{'train_runtime': 46.9794, 'train_samples_per_second': 538.64, 'train_steps_per_second': 16.922, 'train_loss': 0.4970471172212805, 'epoch': 5.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-12 16:12:18,665] Trial 9 finished with value: 0.7409698005521526 and parameters: {'learning_rate': 2.1306865060903365e-05, 'weight_decay': 0.0, 'per_device_train_batch_size': 32, 'warmup_steps': 0, 'num_train_epochs': 5, 'adam_beta1': 0.9310587441131611, 'adam_beta2': 0.9934064925787912, 'adam_epsilon': 5.106538694855391e-08, 'gradient_accumulation_steps': 1, 'lr_scheduler_type': 'polynomial', 'label_smoothing_factor': 0.12794942460035327}. Best is trial 3 with value: 0.7585259097411516.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.5648874044418335, 'eval_accuracy': 0.7417061611374408, 'eval_f1': 0.7409698005521526, 'eval_precision': 0.7407539153064968, 'eval_recall': 0.741487292814563, 'eval_runtime': 0.8301, 'eval_samples_per_second': 2033.539, 'eval_steps_per_second': 32.527, 'epoch': 5.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ai4bharat/indic-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ai4bharat/indic-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.6185, 'grad_norm': 4.830303192138672, 'learning_rate': 2.5707460034549777e-05, 'epoch': 1.0}\n",
            "{'eval_loss': 0.5417864322662354, 'eval_accuracy': 0.7393364928909952, 'eval_f1': 0.7335858585858586, 'eval_precision': 0.7442010490415845, 'eval_recall': 0.7327563472465297, 'eval_runtime': 0.8191, 'eval_samples_per_second': 2060.814, 'eval_steps_per_second': 32.963, 'epoch': 1.0}\n",
            "{'loss': 0.5276, 'grad_norm': 6.545473098754883, 'learning_rate': 8.56915334484993e-06, 'epoch': 2.0}\n",
            "{'eval_loss': 0.4947498142719269, 'eval_accuracy': 0.7636255924170616, 'eval_f1': 0.7635737326145542, 'eval_precision': 0.7652872438536114, 'eval_recall': 0.7661196721981929, 'eval_runtime': 0.8284, 'eval_samples_per_second': 2037.743, 'eval_steps_per_second': 32.594, 'epoch': 2.0}\n",
            "{'loss': 0.4408, 'grad_norm': 7.426018238067627, 'learning_rate': 8.469541837304178e-10, 'epoch': 2.9921011058451814}\n",
            "{'eval_loss': 0.49644458293914795, 'eval_accuracy': 0.7659952606635071, 'eval_f1': 0.7657465660033187, 'eval_precision': 0.7660767731900293, 'eval_recall': 0.7672944222641227, 'eval_runtime': 0.8466, 'eval_samples_per_second': 1993.876, 'eval_steps_per_second': 31.893, 'epoch': 2.9921011058451814}\n",
            "{'train_runtime': 75.6514, 'train_samples_per_second': 200.697, 'train_steps_per_second': 12.531, 'train_loss': 0.5292507445258934, 'epoch': 2.9921011058451814}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-12 16:13:36,134] Trial 10 finished with value: 0.7657465660033187 and parameters: {'learning_rate': 3.42766133793997e-05, 'weight_decay': 0.0, 'per_device_train_batch_size': 8, 'warmup_steps': 0, 'num_train_epochs': 3, 'adam_beta1': 0.852483227758041, 'adam_beta2': 0.9636878178188408, 'adam_epsilon': 1.821466157321173e-07, 'gradient_accumulation_steps': 2, 'lr_scheduler_type': 'cosine_with_restarts', 'label_smoothing_factor': 0.19946308457920844}. Best is trial 10 with value: 0.7657465660033187.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.49644458293914795, 'eval_accuracy': 0.7659952606635071, 'eval_f1': 0.7657465660033187, 'eval_precision': 0.7660767731900293, 'eval_recall': 0.7672944222641227, 'eval_runtime': 0.8126, 'eval_samples_per_second': 2077.17, 'eval_steps_per_second': 33.225, 'epoch': 2.9921011058451814}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ai4bharat/indic-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ai4bharat/indic-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.6202, 'grad_norm': 5.109050273895264, 'learning_rate': 2.690717746027847e-05, 'epoch': 1.0}\n",
            "{'eval_loss': 0.5457545518875122, 'eval_accuracy': 0.7345971563981043, 'eval_f1': 0.7292140222723529, 'eval_precision': 0.7383158591766913, 'eval_recall': 0.7283972206513447, 'eval_runtime': 0.8349, 'eval_samples_per_second': 2021.716, 'eval_steps_per_second': 32.338, 'epoch': 1.0}\n",
            "{'loss': 0.5399, 'grad_norm': 7.3010945320129395, 'learning_rate': 8.951947893291144e-06, 'epoch': 2.0}\n",
            "{'eval_loss': 0.48387280106544495, 'eval_accuracy': 0.7772511848341233, 'eval_f1': 0.7760815111167559, 'eval_precision': 0.7762893105741897, 'eval_recall': 0.7759090210369108, 'eval_runtime': 0.8506, 'eval_samples_per_second': 1984.514, 'eval_steps_per_second': 31.743, 'epoch': 2.0}\n",
            "{'loss': 0.4507, 'grad_norm': 6.869204044342041, 'learning_rate': 3.93241204656166e-10, 'epoch': 2.9921011058451814}\n",
            "{'eval_loss': 0.48600077629089355, 'eval_accuracy': 0.7813981042654028, 'eval_f1': 0.7809182432011784, 'eval_precision': 0.7807086304456544, 'eval_recall': 0.7818032201972396, 'eval_runtime': 0.838, 'eval_samples_per_second': 2014.37, 'eval_steps_per_second': 32.22, 'epoch': 2.9921011058451814}\n",
            "{'train_runtime': 75.3685, 'train_samples_per_second': 201.45, 'train_steps_per_second': 12.578, 'train_loss': 0.5372019256720563, 'epoch': 2.9921011058451814}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-12 16:14:53,264] Trial 11 finished with value: 0.7809182432011784 and parameters: {'learning_rate': 3.5807791573164564e-05, 'weight_decay': 0.0, 'per_device_train_batch_size': 8, 'warmup_steps': 0, 'num_train_epochs': 3, 'adam_beta1': 0.8572608615370939, 'adam_beta2': 0.9629580402507818, 'adam_epsilon': 1.9807032678600395e-07, 'gradient_accumulation_steps': 2, 'lr_scheduler_type': 'cosine_with_restarts', 'label_smoothing_factor': 0.19281713973831405}. Best is trial 11 with value: 0.7809182432011784.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.48600077629089355, 'eval_accuracy': 0.7813981042654028, 'eval_f1': 0.7809182432011784, 'eval_precision': 0.7807086304456544, 'eval_recall': 0.7818032201972396, 'eval_runtime': 0.8047, 'eval_samples_per_second': 2097.607, 'eval_steps_per_second': 33.552, 'epoch': 2.9921011058451814}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ai4bharat/indic-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ai4bharat/indic-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.6321, 'grad_norm': 4.121572494506836, 'learning_rate': 2.8035249732571094e-05, 'epoch': 1.0}\n",
            "{'eval_loss': 0.5955010652542114, 'eval_accuracy': 0.6949052132701422, 'eval_f1': 0.6947422645297829, 'eval_precision': 0.7013511207190807, 'eval_recall': 0.6996567416974222, 'eval_runtime': 0.8241, 'eval_samples_per_second': 2048.408, 'eval_steps_per_second': 32.765, 'epoch': 1.0}\n",
            "{'loss': 0.5587, 'grad_norm': 9.233365058898926, 'learning_rate': 9.327254601560264e-06, 'epoch': 2.0}\n",
            "{'eval_loss': 0.5112212896347046, 'eval_accuracy': 0.7464454976303317, 'eval_f1': 0.7458746989036718, 'eval_precision': 0.74570775505158, 'eval_recall': 0.7466502699950782, 'eval_runtime': 0.8164, 'eval_samples_per_second': 2067.59, 'eval_steps_per_second': 33.072, 'epoch': 2.0}\n",
            "{'loss': 0.4782, 'grad_norm': 9.2858247756958, 'learning_rate': 4.097276793133626e-10, 'epoch': 2.9921011058451814}\n",
            "{'eval_loss': 0.5091999173164368, 'eval_accuracy': 0.7654028436018957, 'eval_f1': 0.7640883892106776, 'eval_precision': 0.7644094265625552, 'eval_recall': 0.7638456212002194, 'eval_runtime': 0.8315, 'eval_samples_per_second': 2030.153, 'eval_steps_per_second': 32.473, 'epoch': 2.9921011058451814}\n",
            "{'train_runtime': 76.3519, 'train_samples_per_second': 198.856, 'train_steps_per_second': 12.416, 'train_loss': 0.5566030413792606, 'epoch': 2.9921011058451814}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-12 16:16:11,419] Trial 12 finished with value: 0.7640883892106776 and parameters: {'learning_rate': 3.730901840624104e-05, 'weight_decay': 0.0, 'per_device_train_batch_size': 8, 'warmup_steps': 0, 'num_train_epochs': 3, 'adam_beta1': 0.8501390412776609, 'adam_beta2': 0.9651367119374921, 'adam_epsilon': 1.9343007734846487e-07, 'gradient_accumulation_steps': 2, 'lr_scheduler_type': 'cosine_with_restarts', 'label_smoothing_factor': 0.19703754432519552}. Best is trial 11 with value: 0.7809182432011784.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.5091999173164368, 'eval_accuracy': 0.7654028436018957, 'eval_f1': 0.7640883892106776, 'eval_precision': 0.7644094265625552, 'eval_recall': 0.7638456212002194, 'eval_runtime': 0.8284, 'eval_samples_per_second': 2037.692, 'eval_steps_per_second': 32.593, 'epoch': 2.9921011058451814}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ai4bharat/indic-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ai4bharat/indic-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.6248, 'grad_norm': 4.805906295776367, 'learning_rate': 2.4348329106819502e-05, 'epoch': 1.0}\n",
            "{'eval_loss': 0.5775752663612366, 'eval_accuracy': 0.6925355450236966, 'eval_f1': 0.6922969932932579, 'eval_precision': 0.6929627258380648, 'eval_recall': 0.6938196582365774, 'eval_runtime': 0.8306, 'eval_samples_per_second': 2032.183, 'eval_steps_per_second': 32.505, 'epoch': 1.0}\n",
            "{'loss': 0.5493, 'grad_norm': 9.482950210571289, 'learning_rate': 8.147166652998388e-06, 'epoch': 2.0}\n",
            "{'eval_loss': 0.5151545405387878, 'eval_accuracy': 0.7618483412322274, 'eval_f1': 0.761364595465978, 'eval_precision': 0.7612205484743144, 'eval_recall': 0.7622844587476572, 'eval_runtime': 0.8215, 'eval_samples_per_second': 2054.833, 'eval_steps_per_second': 32.868, 'epoch': 2.0}\n",
            "{'loss': 0.4526, 'grad_norm': 4.938157558441162, 'learning_rate': 8.006460565892724e-10, 'epoch': 2.9921011058451814}\n",
            "{'eval_loss': 0.5096556544303894, 'eval_accuracy': 0.7624407582938388, 'eval_f1': 0.761130867245482, 'eval_precision': 0.7614170736118647, 'eval_recall': 0.7609101562995797, 'eval_runtime': 0.8453, 'eval_samples_per_second': 1996.812, 'eval_steps_per_second': 31.94, 'epoch': 2.9921011058451814}\n",
            "{'train_runtime': 77.3153, 'train_samples_per_second': 196.378, 'train_steps_per_second': 12.261, 'train_loss': 0.54251039078467, 'epoch': 2.9921011058451814}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-12 16:17:31,480] Trial 13 finished with value: 0.761364595465978 and parameters: {'learning_rate': 3.2402502830290764e-05, 'weight_decay': 0.0, 'per_device_train_batch_size': 8, 'warmup_steps': 0, 'num_train_epochs': 3, 'adam_beta1': 0.850509135691507, 'adam_beta2': 0.9831971696255822, 'adam_epsilon': 4.008756453420887e-07, 'gradient_accumulation_steps': 2, 'lr_scheduler_type': 'cosine_with_restarts', 'label_smoothing_factor': 0.19741708704992933}. Best is trial 11 with value: 0.7809182432011784.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.5151545405387878, 'eval_accuracy': 0.7618483412322274, 'eval_f1': 0.761364595465978, 'eval_precision': 0.7612205484743144, 'eval_recall': 0.7622844587476572, 'eval_runtime': 0.7997, 'eval_samples_per_second': 2110.731, 'eval_steps_per_second': 33.762, 'epoch': 2.9921011058451814}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ai4bharat/indic-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ai4bharat/indic-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.6114, 'grad_norm': 7.035349369049072, 'learning_rate': 2.4004110128055163e-05, 'epoch': 1.0}\n",
            "{'eval_loss': 0.5240118503570557, 'eval_accuracy': 0.7505924170616114, 'eval_f1': 0.7488896733348104, 'eval_precision': 0.749668372986854, 'eval_recall': 0.7484448311702232, 'eval_runtime': 0.826, 'eval_samples_per_second': 2043.462, 'eval_steps_per_second': 32.686, 'epoch': 1.0}\n",
            "{'loss': 0.5141, 'grad_norm': 5.735294818878174, 'learning_rate': 8.001370042685057e-06, 'epoch': 2.0}\n",
            "{'eval_loss': 0.49309754371643066, 'eval_accuracy': 0.768957345971564, 'eval_f1': 0.7681436275421365, 'eval_precision': 0.7679324181863989, 'eval_recall': 0.7684614158770362, 'eval_runtime': 0.8298, 'eval_samples_per_second': 2034.177, 'eval_steps_per_second': 32.537, 'epoch': 2.0}\n",
            "{'loss': 0.4176, 'grad_norm': 7.450030326843262, 'learning_rate': 3.5148421684215647e-10, 'epoch': 2.9921011058451814}\n",
            "{'eval_loss': 0.4963224232196808, 'eval_accuracy': 0.7725118483412322, 'eval_f1': 0.7721199643937444, 'eval_precision': 0.7720562733179156, 'eval_recall': 0.7732379806709191, 'eval_runtime': 0.8502, 'eval_samples_per_second': 1985.36, 'eval_steps_per_second': 31.756, 'epoch': 2.9921011058451814}\n",
            "{'train_runtime': 76.3125, 'train_samples_per_second': 198.958, 'train_steps_per_second': 12.423, 'train_loss': 0.5146554552553072, 'epoch': 2.9921011058451814}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-12 16:18:49,620] Trial 14 finished with value: 0.7721199643937444 and parameters: {'learning_rate': 3.2005480170740215e-05, 'weight_decay': 0.0, 'per_device_train_batch_size': 8, 'warmup_steps': 0, 'num_train_epochs': 3, 'adam_beta1': 0.8669537418598141, 'adam_beta2': 0.963235135135547, 'adam_epsilon': 1.3338395781997984e-08, 'gradient_accumulation_steps': 2, 'lr_scheduler_type': 'cosine_with_restarts', 'label_smoothing_factor': 0.06559401942402085}. Best is trial 11 with value: 0.7809182432011784.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.4963224232196808, 'eval_accuracy': 0.7725118483412322, 'eval_f1': 0.7721199643937444, 'eval_precision': 0.7720562733179156, 'eval_recall': 0.7732379806709191, 'eval_runtime': 0.8142, 'eval_samples_per_second': 2073.27, 'eval_steps_per_second': 33.162, 'epoch': 2.9921011058451814}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ai4bharat/indic-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ai4bharat/indic-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.6408, 'grad_norm': 3.5908122062683105, 'learning_rate': 4.4958768312966687e-05, 'epoch': 1.0}\n",
            "{'eval_loss': 0.5415602922439575, 'eval_accuracy': 0.7322274881516587, 'eval_f1': 0.7305799598858724, 'eval_precision': 0.7310513066868918, 'eval_recall': 0.7302771028096693, 'eval_runtime': 0.8477, 'eval_samples_per_second': 1991.356, 'eval_steps_per_second': 31.852, 'epoch': 1.0}\n",
            "{'loss': 0.5464, 'grad_norm': 4.728912353515625, 'learning_rate': 3.268738033395316e-09, 'epoch': 1.9952606635071088}\n",
            "{'eval_loss': 0.5018904805183411, 'eval_accuracy': 0.7713270142180095, 'eval_f1': 0.7704556727994227, 'eval_precision': 0.7702847518824097, 'eval_recall': 0.7706811717038953, 'eval_runtime': 0.8395, 'eval_samples_per_second': 2010.767, 'eval_steps_per_second': 32.163, 'epoch': 1.9952606635071088}\n",
            "{'train_runtime': 50.3173, 'train_samples_per_second': 201.164, 'train_steps_per_second': 12.56, 'train_loss': 0.5937478270711778, 'epoch': 1.9952606635071088}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-12 16:19:41,746] Trial 15 finished with value: 0.7704556727994227 and parameters: {'learning_rate': 4.833001032897873e-05, 'weight_decay': 0.0, 'per_device_train_batch_size': 8, 'warmup_steps': 250, 'num_train_epochs': 2, 'adam_beta1': 0.8685200783878227, 'adam_beta2': 0.977325837514244, 'adam_epsilon': 1.0565130123820612e-08, 'gradient_accumulation_steps': 2, 'lr_scheduler_type': 'cosine_with_restarts', 'label_smoothing_factor': 0.06601431275567494}. Best is trial 11 with value: 0.7809182432011784.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.5018904805183411, 'eval_accuracy': 0.7713270142180095, 'eval_f1': 0.7704556727994227, 'eval_precision': 0.7702847518824097, 'eval_recall': 0.7706811717038953, 'eval_runtime': 0.7967, 'eval_samples_per_second': 2118.609, 'eval_steps_per_second': 33.888, 'epoch': 1.9952606635071088}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ai4bharat/indic-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ai4bharat/indic-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.6166, 'grad_norm': 5.94743013381958, 'learning_rate': 2.4852161246145507e-05, 'epoch': 1.0}\n",
            "{'eval_loss': 0.5631246566772461, 'eval_accuracy': 0.7085308056872038, 'eval_f1': 0.7081620834481246, 'eval_precision': 0.7084134112563472, 'eval_recall': 0.7093657054776071, 'eval_runtime': 0.8238, 'eval_samples_per_second': 2049.095, 'eval_steps_per_second': 32.776, 'epoch': 1.0}\n",
            "{'loss': 0.5396, 'grad_norm': 7.325517177581787, 'learning_rate': 1.4543105471919187e-05, 'epoch': 2.0}\n",
            "{'eval_loss': 0.5062466263771057, 'eval_accuracy': 0.7630331753554502, 'eval_f1': 0.7629769422059713, 'eval_precision': 0.7646304691618345, 'eval_recall': 0.7654843481829451, 'eval_runtime': 0.8215, 'eval_samples_per_second': 2054.766, 'eval_steps_per_second': 32.867, 'epoch': 2.0}\n",
            "{'loss': 0.4617, 'grad_norm': 8.600879669189453, 'learning_rate': 4.259576973213945e-06, 'epoch': 3.0}\n",
            "{'eval_loss': 0.5014758706092834, 'eval_accuracy': 0.764218009478673, 'eval_f1': 0.7637882538253825, 'eval_precision': 0.7637011645656365, 'eval_recall': 0.7648257548086483, 'eval_runtime': 0.8254, 'eval_samples_per_second': 2045.142, 'eval_steps_per_second': 32.713, 'epoch': 3.0}\n",
            "{'loss': 0.3634, 'grad_norm': 15.874632835388184, 'learning_rate': 7.187025093486879e-10, 'epoch': 3.9889415481832544}\n",
            "{'eval_loss': 0.5330986976623535, 'eval_accuracy': 0.7659952606635071, 'eval_f1': 0.765187567154292, 'eval_precision': 0.7649710374177423, 'eval_recall': 0.7655259509763964, 'eval_runtime': 0.8214, 'eval_samples_per_second': 2055.146, 'eval_steps_per_second': 32.873, 'epoch': 3.9889415481832544}\n",
            "{'train_runtime': 100.2654, 'train_samples_per_second': 201.904, 'train_steps_per_second': 12.607, 'train_loss': 0.49572735194918477, 'epoch': 3.9889415481832544}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-12 16:21:23,879] Trial 16 finished with value: 0.765187567154292 and parameters: {'learning_rate': 2.9086210943838374e-05, 'weight_decay': 0.0, 'per_device_train_batch_size': 8, 'warmup_steps': 0, 'num_train_epochs': 4, 'adam_beta1': 0.8652279978806596, 'adam_beta2': 0.9568054820956922, 'adam_epsilon': 1.1174014585621843e-08, 'gradient_accumulation_steps': 2, 'lr_scheduler_type': 'cosine_with_restarts', 'label_smoothing_factor': 0.055251396360931085}. Best is trial 11 with value: 0.7809182432011784.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.5330986976623535, 'eval_accuracy': 0.7659952606635071, 'eval_f1': 0.765187567154292, 'eval_precision': 0.7649710374177423, 'eval_recall': 0.7655259509763964, 'eval_runtime': 0.7932, 'eval_samples_per_second': 2128.001, 'eval_steps_per_second': 34.038, 'epoch': 3.9889415481832544}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ai4bharat/indic-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ai4bharat/indic-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.6178, 'grad_norm': 2.116870880126953, 'learning_rate': 3.134726210975741e-05, 'epoch': 1.0}\n",
            "{'eval_loss': 0.5559106469154358, 'eval_accuracy': 0.731042654028436, 'eval_f1': 0.7265653544026306, 'eval_precision': 0.7329485994571032, 'eval_recall': 0.7257106673793201, 'eval_runtime': 0.8249, 'eval_samples_per_second': 2046.404, 'eval_steps_per_second': 32.733, 'epoch': 1.0}\n",
            "{'loss': 0.5355, 'grad_norm': 2.4959030151367188, 'learning_rate': 1.0469136995349065e-05, 'epoch': 2.0}\n",
            "{'eval_loss': 0.4904743432998657, 'eval_accuracy': 0.7701421800947867, 'eval_f1': 0.7700957045291157, 'eval_precision': 0.7718781239147582, 'eval_recall': 0.7727063110732534, 'eval_runtime': 0.83, 'eval_samples_per_second': 2033.652, 'eval_steps_per_second': 32.529, 'epoch': 2.0}\n",
            "{'loss': 0.4475, 'grad_norm': 3.625965118408203, 'learning_rate': 1.0288329192498171e-09, 'epoch': 2.9921011058451814}\n",
            "{'eval_loss': 0.48473572731018066, 'eval_accuracy': 0.7671800947867299, 'eval_f1': 0.767014514355434, 'eval_precision': 0.7677143242271747, 'eval_recall': 0.7688866105287504, 'eval_runtime': 0.8222, 'eval_samples_per_second': 2052.904, 'eval_steps_per_second': 32.837, 'epoch': 2.9921011058451814}\n",
            "{'train_runtime': 75.6011, 'train_samples_per_second': 200.83, 'train_steps_per_second': 12.54, 'train_loss': 0.5338764914983436, 'epoch': 2.9921011058451814}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-12 16:22:41,824] Trial 17 finished with value: 0.7700957045291157 and parameters: {'learning_rate': 4.1637326885617954e-05, 'weight_decay': 0.0, 'per_device_train_batch_size': 8, 'warmup_steps': 0, 'num_train_epochs': 3, 'adam_beta1': 0.863272144390903, 'adam_beta2': 0.9676016394062075, 'adam_epsilon': 2.028712698627598e-08, 'gradient_accumulation_steps': 2, 'lr_scheduler_type': 'cosine_with_restarts', 'label_smoothing_factor': 0.05042925228069978}. Best is trial 11 with value: 0.7809182432011784.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.4904743432998657, 'eval_accuracy': 0.7701421800947867, 'eval_f1': 0.7700957045291157, 'eval_precision': 0.7718781239147582, 'eval_recall': 0.7727063110732534, 'eval_runtime': 0.8059, 'eval_samples_per_second': 2094.643, 'eval_steps_per_second': 33.504, 'epoch': 2.9921011058451814}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ai4bharat/indic-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ai4bharat/indic-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.6239, 'grad_norm': 3.871460437774658, 'learning_rate': 2.3795752816387e-05, 'epoch': 1.0}\n",
            "{'eval_loss': 0.5794200301170349, 'eval_accuracy': 0.7002369668246445, 'eval_f1': 0.6974503205695866, 'eval_precision': 0.6991177951008896, 'eval_recall': 0.6969342266886855, 'eval_runtime': 0.8266, 'eval_samples_per_second': 2042.113, 'eval_steps_per_second': 32.664, 'epoch': 1.0}\n",
            "{'loss': 0.5583, 'grad_norm': 10.639233589172363, 'learning_rate': 1.3959520839027507e-05, 'epoch': 2.0}\n",
            "{'eval_loss': 0.509932816028595, 'eval_accuracy': 0.7476303317535545, 'eval_f1': 0.7469425675675676, 'eval_precision': 0.7611393424956351, 'eval_recall': 0.754592877883814, 'eval_runtime': 0.8161, 'eval_samples_per_second': 2068.318, 'eval_steps_per_second': 33.083, 'epoch': 2.0}\n",
            "{'loss': 0.4993, 'grad_norm': 6.219370365142822, 'learning_rate': 4.0785121162326904e-06, 'epoch': 3.0}\n",
            "{'eval_loss': 0.491237610578537, 'eval_accuracy': 0.7606635071090048, 'eval_f1': 0.7605878858773422, 'eval_precision': 0.7620118970161901, 'eval_recall': 0.762943052121954, 'eval_runtime': 0.8353, 'eval_samples_per_second': 2020.712, 'eval_steps_per_second': 32.322, 'epoch': 3.0}\n",
            "{'loss': 0.4416, 'grad_norm': 8.242392539978027, 'learning_rate': 3.870869581245089e-10, 'epoch': 3.9889415481832544}\n",
            "{'eval_loss': 0.49412107467651367, 'eval_accuracy': 0.7624407582938388, 'eval_f1': 0.7620879829631028, 'eval_precision': 0.7621442370330436, 'eval_recall': 0.7633217080555701, 'eval_runtime': 0.8342, 'eval_samples_per_second': 2023.597, 'eval_steps_per_second': 32.368, 'epoch': 3.9889415481832544}\n",
            "{'train_runtime': 100.3766, 'train_samples_per_second': 201.68, 'train_steps_per_second': 12.593, 'train_loss': 0.5310529757149612, 'epoch': 3.9889415481832544}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-12 16:24:24,038] Trial 18 finished with value: 0.7620879829631028 and parameters: {'learning_rate': 2.784982276308928e-05, 'weight_decay': 0.0, 'per_device_train_batch_size': 8, 'warmup_steps': 0, 'num_train_epochs': 4, 'adam_beta1': 0.8857215859381466, 'adam_beta2': 0.9848136839886907, 'adam_epsilon': 8.10382673640342e-08, 'gradient_accumulation_steps': 2, 'lr_scheduler_type': 'cosine_with_restarts', 'label_smoothing_factor': 0.08104686556132909}. Best is trial 11 with value: 0.7809182432011784.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.49412107467651367, 'eval_accuracy': 0.7624407582938388, 'eval_f1': 0.7620879829631028, 'eval_precision': 0.7621442370330436, 'eval_recall': 0.7633217080555701, 'eval_runtime': 0.7947, 'eval_samples_per_second': 2124.165, 'eval_steps_per_second': 33.977, 'epoch': 3.9889415481832544}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ai4bharat/indic-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ai4bharat/indic-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.6584, 'grad_norm': 2.628298282623291, 'learning_rate': 2.4494491884488994e-05, 'epoch': 1.0}\n",
            "{'eval_loss': 0.6074706315994263, 'eval_accuracy': 0.683649289099526, 'eval_f1': 0.683348790152601, 'eval_precision': 0.683855787710705, 'eval_recall': 0.6846917233005259, 'eval_runtime': 0.8118, 'eval_samples_per_second': 2079.26, 'eval_steps_per_second': 33.258, 'epoch': 1.0}\n",
            "{'loss': 0.5885, 'grad_norm': 4.741590976715088, 'learning_rate': 1.1744578003686706e-06, 'epoch': 1.9905362776025237}\n",
            "{'eval_loss': 0.5393214225769043, 'eval_accuracy': 0.6984597156398105, 'eval_f1': 0.6983147675102486, 'eval_precision': 0.7047604832977967, 'eval_recall': 0.7031471455547768, 'eval_runtime': 0.8001, 'eval_samples_per_second': 2109.795, 'eval_steps_per_second': 33.747, 'epoch': 1.9905362776025237}\n",
            "{'train_runtime': 28.1006, 'train_samples_per_second': 360.206, 'train_steps_per_second': 11.245, 'train_loss': 0.6236842191672023, 'epoch': 1.9905362776025237}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-12 16:24:53,969] Trial 19 finished with value: 0.6983147675102486 and parameters: {'learning_rate': 3.875710741216613e-05, 'weight_decay': 0.1, 'per_device_train_batch_size': 16, 'warmup_steps': 250, 'num_train_epochs': 2, 'adam_beta1': 0.9055396010752869, 'adam_beta2': 0.9746960378175081, 'adam_epsilon': 1.1593428291654288e-07, 'gradient_accumulation_steps': 2, 'lr_scheduler_type': 'linear', 'label_smoothing_factor': 0.03549299472243646}. Best is trial 11 with value: 0.7809182432011784.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.5393214225769043, 'eval_accuracy': 0.6984597156398105, 'eval_f1': 0.6983147675102486, 'eval_precision': 0.7047604832977967, 'eval_recall': 0.7031471455547768, 'eval_runtime': 0.8007, 'eval_samples_per_second': 2108.113, 'eval_steps_per_second': 33.72, 'epoch': 1.9905362776025237}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ai4bharat/indic-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ai4bharat/indic-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.6136, 'grad_norm': 7.760701656341553, 'learning_rate': 2.3183897077754377e-05, 'epoch': 1.0}\n",
            "{'eval_loss': 0.5396987795829773, 'eval_accuracy': 0.7328199052132701, 'eval_f1': 0.7303670048731629, 'eval_precision': 0.7321843995894629, 'eval_recall': 0.7297066509469219, 'eval_runtime': 0.8305, 'eval_samples_per_second': 2032.436, 'eval_steps_per_second': 32.509, 'epoch': 1.0}\n",
            "{'loss': 0.5145, 'grad_norm': 6.758963584899902, 'learning_rate': 7.727965692584796e-06, 'epoch': 2.0}\n",
            "{'eval_loss': 0.4872657060623169, 'eval_accuracy': 0.7659952606635071, 'eval_f1': 0.7658946132517637, 'eval_precision': 0.7670569163253036, 'eval_recall': 0.7680982728494529, 'eval_runtime': 0.8325, 'eval_samples_per_second': 2027.625, 'eval_steps_per_second': 32.432, 'epoch': 2.0}\n",
            "{'loss': 0.4119, 'grad_norm': 6.790521144866943, 'learning_rate': 3.394741094026083e-10, 'epoch': 2.9921011058451814}\n",
            "{'eval_loss': 0.49226856231689453, 'eval_accuracy': 0.7742890995260664, 'eval_f1': 0.7734123613307736, 'eval_precision': 0.7732524731279717, 'eval_recall': 0.7736166366045352, 'eval_runtime': 0.8466, 'eval_samples_per_second': 1993.884, 'eval_steps_per_second': 31.893, 'epoch': 2.9921011058451814}\n",
            "{'train_runtime': 75.5492, 'train_samples_per_second': 200.968, 'train_steps_per_second': 12.548, 'train_loss': 0.5136383797046001, 'epoch': 2.9921011058451814}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-12 16:26:11,316] Trial 20 finished with value: 0.7734123613307736 and parameters: {'learning_rate': 3.091186277033917e-05, 'weight_decay': 0.0, 'per_device_train_batch_size': 8, 'warmup_steps': 0, 'num_train_epochs': 3, 'adam_beta1': 0.8615532577939053, 'adam_beta2': 0.9601249328817801, 'adam_epsilon': 3.679714935421026e-08, 'gradient_accumulation_steps': 2, 'lr_scheduler_type': 'cosine_with_restarts', 'label_smoothing_factor': 0.1560580640629845}. Best is trial 11 with value: 0.7809182432011784.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.49226856231689453, 'eval_accuracy': 0.7742890995260664, 'eval_f1': 0.7734123613307736, 'eval_precision': 0.7732524731279717, 'eval_recall': 0.7736166366045352, 'eval_runtime': 0.8052, 'eval_samples_per_second': 2096.465, 'eval_steps_per_second': 33.533, 'epoch': 2.9921011058451814}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ai4bharat/indic-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ai4bharat/indic-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.6176, 'grad_norm': 3.6213600635528564, 'learning_rate': 2.3434247415814412e-05, 'epoch': 1.0}\n",
            "{'eval_loss': 0.5360457301139832, 'eval_accuracy': 0.7381516587677726, 'eval_f1': 0.7379029311749993, 'eval_precision': 0.7383600702502138, 'eval_recall': 0.7394438200108027, 'eval_runtime': 0.8257, 'eval_samples_per_second': 2044.345, 'eval_steps_per_second': 32.7, 'epoch': 1.0}\n",
            "{'loss': 0.5201, 'grad_norm': 6.760727882385254, 'learning_rate': 7.796513108613965e-06, 'epoch': 2.0}\n",
            "{'eval_loss': 0.4925355017185211, 'eval_accuracy': 0.7588862559241706, 'eval_f1': 0.7588719541621698, 'eval_precision': 0.761356981981982, 'eval_recall': 0.7618409306615408, 'eval_runtime': 0.8484, 'eval_samples_per_second': 1989.557, 'eval_steps_per_second': 31.823, 'epoch': 2.0}\n",
            "{'loss': 0.4291, 'grad_norm': 5.81725549697876, 'learning_rate': 7.705883101996567e-10, 'epoch': 2.9921011058451814}\n",
            "{'eval_loss': 0.5017616748809814, 'eval_accuracy': 0.7624407582938388, 'eval_f1': 0.7620879829631028, 'eval_precision': 0.7621442370330436, 'eval_recall': 0.7633217080555701, 'eval_runtime': 0.8389, 'eval_samples_per_second': 2012.26, 'eval_steps_per_second': 32.187, 'epoch': 2.9921011058451814}\n",
            "{'train_runtime': 76.7158, 'train_samples_per_second': 197.912, 'train_steps_per_second': 12.357, 'train_loss': 0.5225635480277145, 'epoch': 2.9921011058451814}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-12 16:27:29,855] Trial 21 finished with value: 0.7620879829631028 and parameters: {'learning_rate': 3.118605243445585e-05, 'weight_decay': 0.0, 'per_device_train_batch_size': 8, 'warmup_steps': 0, 'num_train_epochs': 3, 'adam_beta1': 0.8730736283509758, 'adam_beta2': 0.9592384623264576, 'adam_epsilon': 1.7860172786251214e-08, 'gradient_accumulation_steps': 2, 'lr_scheduler_type': 'cosine_with_restarts', 'label_smoothing_factor': 0.17007084212003318}. Best is trial 11 with value: 0.7809182432011784.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.5017616748809814, 'eval_accuracy': 0.7624407582938388, 'eval_f1': 0.7620879829631028, 'eval_precision': 0.7621442370330436, 'eval_recall': 0.7633217080555701, 'eval_runtime': 0.8334, 'eval_samples_per_second': 2025.456, 'eval_steps_per_second': 32.398, 'epoch': 2.9921011058451814}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ai4bharat/indic-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ai4bharat/indic-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.6212, 'grad_norm': 5.998795509338379, 'learning_rate': 2.6713245670928008e-05, 'epoch': 1.0}\n",
            "{'eval_loss': 0.5544937252998352, 'eval_accuracy': 0.7399289099526066, 'eval_f1': 0.7334585053099175, 'eval_precision': 0.7463636363636363, 'eval_recall': 0.7327485907935134, 'eval_runtime': 0.8281, 'eval_samples_per_second': 2038.365, 'eval_steps_per_second': 32.604, 'epoch': 1.0}\n",
            "{'loss': 0.5455, 'grad_norm': 13.108197212219238, 'learning_rate': 8.9215009444895e-06, 'epoch': 2.0}\n",
            "{'eval_loss': 0.5150092840194702, 'eval_accuracy': 0.7529620853080569, 'eval_f1': 0.7509692935858396, 'eval_precision': 0.7523312011371712, 'eval_recall': 0.7503430467629502, 'eval_runtime': 0.8553, 'eval_samples_per_second': 1973.546, 'eval_steps_per_second': 31.567, 'epoch': 2.0}\n",
            "{'loss': 0.458, 'grad_norm': 7.286264896392822, 'learning_rate': 8.767421674667935e-10, 'epoch': 2.9921011058451814}\n",
            "{'eval_loss': 0.5141251087188721, 'eval_accuracy': 0.7606635071090048, 'eval_f1': 0.759642808302705, 'eval_precision': 0.759566584499251, 'eval_recall': 0.7597276497806335, 'eval_runtime': 0.8408, 'eval_samples_per_second': 2007.717, 'eval_steps_per_second': 32.114, 'epoch': 2.9921011058451814}\n",
            "{'train_runtime': 77.9948, 'train_samples_per_second': 194.667, 'train_steps_per_second': 12.155, 'train_loss': 0.5418546169619017, 'epoch': 2.9921011058451814}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-12 16:28:49,714] Trial 22 finished with value: 0.759642808302705 and parameters: {'learning_rate': 3.548214636040046e-05, 'weight_decay': 0.0, 'per_device_train_batch_size': 8, 'warmup_steps': 0, 'num_train_epochs': 3, 'adam_beta1': 0.8585359305563025, 'adam_beta2': 0.9559919968021677, 'adam_epsilon': 3.5411921692550533e-08, 'gradient_accumulation_steps': 2, 'lr_scheduler_type': 'cosine_with_restarts', 'label_smoothing_factor': 0.17574426833019086}. Best is trial 11 with value: 0.7809182432011784.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.5141251087188721, 'eval_accuracy': 0.7606635071090048, 'eval_f1': 0.759642808302705, 'eval_precision': 0.759566584499251, 'eval_recall': 0.7597276497806335, 'eval_runtime': 0.8053, 'eval_samples_per_second': 2095.986, 'eval_steps_per_second': 33.526, 'epoch': 2.9921011058451814}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ai4bharat/indic-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ai4bharat/indic-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.6254, 'grad_norm': 3.1795766353607178, 'learning_rate': 1.997474723109981e-05, 'epoch': 1.0}\n",
            "{'eval_loss': 0.5612427592277527, 'eval_accuracy': 0.691350710900474, 'eval_f1': 0.6913454029856508, 'eval_precision': 0.6940321426357638, 'eval_recall': 0.6942370964352752, 'eval_runtime': 0.8326, 'eval_samples_per_second': 2027.335, 'eval_steps_per_second': 32.428, 'epoch': 1.0}\n",
            "{'loss': 0.5487, 'grad_norm': 8.478343963623047, 'learning_rate': 6.645546403313329e-06, 'epoch': 2.0}\n",
            "{'eval_loss': 0.5188645720481873, 'eval_accuracy': 0.7434834123222749, 'eval_f1': 0.7434076776468201, 'eval_precision': 0.7495148523278576, 'eval_recall': 0.748055598255221, 'eval_runtime': 0.8385, 'eval_samples_per_second': 2013.023, 'eval_steps_per_second': 32.199, 'epoch': 2.0}\n",
            "{'loss': 0.4703, 'grad_norm': 6.562790393829346, 'learning_rate': 6.568295726489215e-10, 'epoch': 2.9921011058451814}\n",
            "{'eval_loss': 0.5172664523124695, 'eval_accuracy': 0.7511848341232228, 'eval_f1': 0.750679428098783, 'eval_precision': 0.7505516345377296, 'eval_recall': 0.7515720919999943, 'eval_runtime': 0.8292, 'eval_samples_per_second': 2035.779, 'eval_steps_per_second': 32.563, 'epoch': 2.9921011058451814}\n",
            "{'train_runtime': 76.9472, 'train_samples_per_second': 197.317, 'train_steps_per_second': 12.32, 'train_loss': 0.5483865858633307, 'epoch': 2.9921011058451814}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-12 16:30:08,684] Trial 23 finished with value: 0.750679428098783 and parameters: {'learning_rate': 2.6582185613253307e-05, 'weight_decay': 0.0, 'per_device_train_batch_size': 8, 'warmup_steps': 0, 'num_train_epochs': 3, 'adam_beta1': 0.8851614941012822, 'adam_beta2': 0.963107913626336, 'adam_epsilon': 3.5729367116141164e-08, 'gradient_accumulation_steps': 2, 'lr_scheduler_type': 'cosine_with_restarts', 'label_smoothing_factor': 0.14874643463836482}. Best is trial 11 with value: 0.7809182432011784.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.5172664523124695, 'eval_accuracy': 0.7511848341232228, 'eval_f1': 0.750679428098783, 'eval_precision': 0.7505516345377296, 'eval_recall': 0.7515720919999943, 'eval_runtime': 0.8011, 'eval_samples_per_second': 2107.19, 'eval_steps_per_second': 33.705, 'epoch': 2.9921011058451814}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ai4bharat/indic-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ai4bharat/indic-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.6233, 'grad_norm': 7.0629563331604, 'learning_rate': 3.583567041633149e-05, 'epoch': 1.0}\n",
            "{'eval_loss': 0.5985558032989502, 'eval_accuracy': 0.7091232227488151, 'eval_f1': 0.6943588586213165, 'eval_precision': 0.7281373219930264, 'eval_recall': 0.6974609603617046, 'eval_runtime': 0.8375, 'eval_samples_per_second': 2015.641, 'eval_steps_per_second': 32.241, 'epoch': 1.0}\n",
            "{'loss': 0.5475, 'grad_norm': 5.2226057052612305, 'learning_rate': 2.1031684424265065e-05, 'epoch': 2.0}\n",
            "{'eval_loss': 0.48677483201026917, 'eval_accuracy': 0.7659952606635071, 'eval_f1': 0.7658827763586409, 'eval_precision': 0.7669453434016541, 'eval_recall': 0.7680178877909198, 'eval_runtime': 0.8351, 'eval_samples_per_second': 2021.346, 'eval_steps_per_second': 32.332, 'epoch': 2.0}\n",
            "{'loss': 0.4703, 'grad_norm': 9.7666015625, 'learning_rate': 6.166393944430293e-06, 'epoch': 3.0}\n",
            "{'eval_loss': 0.4785652458667755, 'eval_accuracy': 0.7796208530805687, 'eval_f1': 0.7785014448257179, 'eval_precision': 0.7786528734398976, 'eval_recall': 0.7783699320393689, 'eval_runtime': 0.8365, 'eval_samples_per_second': 2017.846, 'eval_steps_per_second': 32.276, 'epoch': 3.0}\n",
            "{'loss': 0.3951, 'grad_norm': 11.100394248962402, 'learning_rate': 1.0342192033241443e-09, 'epoch': 3.9889415481832544}\n",
            "{'eval_loss': 0.4886228144168854, 'eval_accuracy': 0.7808056872037915, 'eval_f1': 0.780033697924591, 'eval_precision': 0.7798115940193135, 'eval_recall': 0.7803640455966616, 'eval_runtime': 0.8196, 'eval_samples_per_second': 2059.455, 'eval_steps_per_second': 32.942, 'epoch': 3.9889415481832544}\n",
            "{'train_runtime': 101.0827, 'train_samples_per_second': 200.272, 'train_steps_per_second': 12.505, 'train_loss': 0.5094280001483386, 'epoch': 3.9889415481832544}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-12 16:31:51,969] Trial 24 finished with value: 0.780033697924591 and parameters: {'learning_rate': 4.185531220325835e-05, 'weight_decay': 0.0, 'per_device_train_batch_size': 8, 'warmup_steps': 0, 'num_train_epochs': 4, 'adam_beta1': 0.8588274117472381, 'adam_beta2': 0.9662715039220985, 'adam_epsilon': 1.5193077479182037e-08, 'gradient_accumulation_steps': 2, 'lr_scheduler_type': 'cosine_with_restarts', 'label_smoothing_factor': 0.1793927888115259}. Best is trial 11 with value: 0.7809182432011784.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.4886228144168854, 'eval_accuracy': 0.7808056872037915, 'eval_f1': 0.780033697924591, 'eval_precision': 0.7798115940193135, 'eval_recall': 0.7803640455966616, 'eval_runtime': 0.7987, 'eval_samples_per_second': 2113.445, 'eval_steps_per_second': 33.805, 'epoch': 3.9889415481832544}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ai4bharat/indic-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ai4bharat/indic-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.6248, 'grad_norm': 6.757620334625244, 'learning_rate': 3.902735207046871e-05, 'epoch': 1.0}\n",
            "{'eval_loss': 0.5723260641098022, 'eval_accuracy': 0.7203791469194313, 'eval_f1': 0.7130372519669044, 'eval_precision': 0.7262683161235863, 'eval_recall': 0.712827904051266, 'eval_runtime': 0.8208, 'eval_samples_per_second': 2056.585, 'eval_steps_per_second': 32.896, 'epoch': 1.0}\n",
            "{'loss': 0.588, 'grad_norm': 1.695119023323059, 'learning_rate': 2.292824387270955e-05, 'epoch': 2.0}\n",
            "{'eval_loss': 0.5560380220413208, 'eval_accuracy': 0.7292654028436019, 'eval_f1': 0.7241892192900504, 'eval_precision': 0.7320198256467941, 'eval_recall': 0.7234027700409118, 'eval_runtime': 0.8205, 'eval_samples_per_second': 2057.232, 'eval_steps_per_second': 32.906, 'epoch': 2.0}\n",
            "{'loss': 0.5282, 'grad_norm': 8.531274795532227, 'learning_rate': 6.722456524213358e-06, 'epoch': 3.0}\n",
            "{'eval_loss': 0.5035688877105713, 'eval_accuracy': 0.764218009478673, 'eval_f1': 0.7635155303990313, 'eval_precision': 0.763270431761005, 'eval_recall': 0.7640219042233181, 'eval_runtime': 0.8257, 'eval_samples_per_second': 2044.267, 'eval_steps_per_second': 32.699, 'epoch': 3.0}\n",
            "{'loss': 0.4683, 'grad_norm': 9.400689125061035, 'learning_rate': 1.1274812627131739e-09, 'epoch': 3.9889415481832544}\n",
            "{'eval_loss': 0.5090819597244263, 'eval_accuracy': 0.7742890995260664, 'eval_f1': 0.7726575217531935, 'eval_precision': 0.7736899594291713, 'eval_recall': 0.7720893204924077, 'eval_runtime': 0.8175, 'eval_samples_per_second': 2064.727, 'eval_steps_per_second': 33.026, 'epoch': 3.9889415481832544}\n",
            "{'train_runtime': 100.9055, 'train_samples_per_second': 200.623, 'train_steps_per_second': 12.527, 'train_loss': 0.5525888974153543, 'epoch': 3.9889415481832544}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-12 16:33:34,719] Trial 25 finished with value: 0.7726575217531935 and parameters: {'learning_rate': 4.56296693220395e-05, 'weight_decay': 0.0, 'per_device_train_batch_size': 8, 'warmup_steps': 0, 'num_train_epochs': 4, 'adam_beta1': 0.8596803095999973, 'adam_beta2': 0.9717892764413999, 'adam_epsilon': 2.7711712710899696e-08, 'gradient_accumulation_steps': 2, 'lr_scheduler_type': 'cosine_with_restarts', 'label_smoothing_factor': 0.18413926095867109}. Best is trial 11 with value: 0.7809182432011784.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.5090819597244263, 'eval_accuracy': 0.7742890995260664, 'eval_f1': 0.7726575217531935, 'eval_precision': 0.7736899594291713, 'eval_recall': 0.7720893204924077, 'eval_runtime': 0.8006, 'eval_samples_per_second': 2108.52, 'eval_steps_per_second': 33.726, 'epoch': 3.9889415481832544}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ai4bharat/indic-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ai4bharat/indic-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.6208, 'grad_norm': 5.196330547332764, 'learning_rate': 3.520554239828305e-05, 'epoch': 1.0}\n",
            "{'eval_loss': 0.5679700374603271, 'eval_accuracy': 0.7227488151658767, 'eval_f1': 0.717504087333771, 'eval_precision': 0.7252929703241614, 'eval_recall': 0.7168161311658513, 'eval_runtime': 0.8222, 'eval_samples_per_second': 2052.909, 'eval_steps_per_second': 32.837, 'epoch': 1.0}\n",
            "{'loss': 0.5431, 'grad_norm': 2.5839827060699463, 'learning_rate': 2.063181099465161e-05, 'epoch': 2.0}\n",
            "{'eval_loss': 0.4878176152706146, 'eval_accuracy': 0.784952606635071, 'eval_f1': 0.784837751503517, 'eval_precision': 0.7858027673690893, 'eval_recall': 0.7869817102837875, 'eval_runtime': 0.8316, 'eval_samples_per_second': 2029.774, 'eval_steps_per_second': 32.467, 'epoch': 2.0}\n",
            "{'loss': 0.4597, 'grad_norm': 7.235661506652832, 'learning_rate': 6.027935492331093e-06, 'epoch': 3.0}\n",
            "{'eval_loss': 0.48679232597351074, 'eval_accuracy': 0.7784360189573459, 'eval_f1': 0.7778043701984061, 'eval_precision': 0.7775400107135777, 'eval_recall': 0.7783854449454016, 'eval_runtime': 0.8432, 'eval_samples_per_second': 2001.987, 'eval_steps_per_second': 32.022, 'epoch': 3.0}\n",
            "{'loss': 0.3637, 'grad_norm': 6.836355686187744, 'learning_rate': 5.721045192462249e-10, 'epoch': 3.9889415481832544}\n",
            "{'eval_loss': 0.5005754828453064, 'eval_accuracy': 0.7784360189573459, 'eval_f1': 0.7773105923785444, 'eval_precision': 0.7774614006581334, 'eval_recall': 0.7771796690674064, 'eval_runtime': 0.8346, 'eval_samples_per_second': 2022.424, 'eval_steps_per_second': 32.349, 'epoch': 3.9889415481832544}\n",
            "{'train_runtime': 100.3576, 'train_samples_per_second': 201.719, 'train_steps_per_second': 12.595, 'train_loss': 0.497229853762856, 'epoch': 3.9889415481832544}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-12 16:35:16,881] Trial 26 finished with value: 0.784837751503517 and parameters: {'learning_rate': 4.1161318222053906e-05, 'weight_decay': 0.0, 'per_device_train_batch_size': 8, 'warmup_steps': 0, 'num_train_epochs': 4, 'adam_beta1': 0.8600049278155624, 'adam_beta2': 0.9667180237875557, 'adam_epsilon': 1.573419909436933e-08, 'gradient_accumulation_steps': 2, 'lr_scheduler_type': 'cosine_with_restarts', 'label_smoothing_factor': 0.15169409960372646}. Best is trial 26 with value: 0.784837751503517.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.4878176152706146, 'eval_accuracy': 0.784952606635071, 'eval_f1': 0.784837751503517, 'eval_precision': 0.7858027673690893, 'eval_recall': 0.7869817102837875, 'eval_runtime': 0.7995, 'eval_samples_per_second': 2111.304, 'eval_steps_per_second': 33.771, 'epoch': 3.9889415481832544}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ai4bharat/indic-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ai4bharat/indic-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.6425, 'grad_norm': 4.141180992126465, 'learning_rate': 4.292388806953179e-05, 'epoch': 1.0}\n",
            "{'eval_loss': 0.5869409441947937, 'eval_accuracy': 0.7162322274881516, 'eval_f1': 0.710816553605655, 'eval_precision': 0.7185599507374718, 'eval_recall': 0.7102294922907908, 'eval_runtime': 0.8178, 'eval_samples_per_second': 2064.125, 'eval_steps_per_second': 33.016, 'epoch': 1.0}\n",
            "{'loss': 0.593, 'grad_norm': 5.263251304626465, 'learning_rate': 2.9931425430495586e-05, 'epoch': 2.0}\n",
            "{'eval_loss': 0.5631958246231079, 'eval_accuracy': 0.7239336492890995, 'eval_f1': 0.7211717332123412, 'eval_precision': 0.7507567503033802, 'eval_recall': 0.7340030207858839, 'eval_runtime': 0.8255, 'eval_samples_per_second': 2044.731, 'eval_steps_per_second': 32.706, 'epoch': 2.0}\n",
            "{'loss': 0.5175, 'grad_norm': 7.606295585632324, 'learning_rate': 9.586866209967065e-06, 'epoch': 3.0}\n",
            "{'eval_loss': 0.4874754250049591, 'eval_accuracy': 0.7701421800947867, 'eval_f1': 0.7701263676018234, 'eval_precision': 0.7749284244904884, 'eval_recall': 0.7742336271853807, 'eval_runtime': 0.8285, 'eval_samples_per_second': 2037.463, 'eval_steps_per_second': 32.59, 'epoch': 3.0}\n",
            "{'loss': 0.4399, 'grad_norm': 7.701761245727539, 'learning_rate': 9.365112676860023e-10, 'epoch': 3.9889415481832544}\n",
            "{'eval_loss': 0.4782182574272156, 'eval_accuracy': 0.7808056872037915, 'eval_f1': 0.7798013501869305, 'eval_precision': 0.7798013501869305, 'eval_recall': 0.7798013501869305, 'eval_runtime': 0.82, 'eval_samples_per_second': 2058.424, 'eval_steps_per_second': 32.925, 'epoch': 3.9889415481832544}\n",
            "{'train_runtime': 100.5818, 'train_samples_per_second': 201.269, 'train_steps_per_second': 12.567, 'train_loss': 0.5485751598696166, 'epoch': 3.9889415481832544}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-12 16:36:59,269] Trial 27 finished with value: 0.7798013501869305 and parameters: {'learning_rate': 4.336204742178506e-05, 'weight_decay': 0.0, 'per_device_train_batch_size': 8, 'warmup_steps': 250, 'num_train_epochs': 4, 'adam_beta1': 0.8839089378202836, 'adam_beta2': 0.9674682583416865, 'adam_epsilon': 1.609639344064455e-08, 'gradient_accumulation_steps': 2, 'lr_scheduler_type': 'cosine', 'label_smoothing_factor': 0.14124721754699007}. Best is trial 26 with value: 0.784837751503517.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.4782182574272156, 'eval_accuracy': 0.7808056872037915, 'eval_f1': 0.7798013501869305, 'eval_precision': 0.7798013501869305, 'eval_recall': 0.7798013501869305, 'eval_runtime': 0.8111, 'eval_samples_per_second': 2081.097, 'eval_steps_per_second': 33.288, 'epoch': 3.9889415481832544}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ai4bharat/indic-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ai4bharat/indic-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.6714, 'grad_norm': 4.062751770019531, 'learning_rate': 1.2400719197755078e-05, 'epoch': 1.0}\n",
            "{'eval_loss': 0.6151564717292786, 'eval_accuracy': 0.6848341232227488, 'eval_f1': 0.6845573554434314, 'eval_precision': 0.6851269221535701, 'eval_recall': 0.6859623713310214, 'eval_runtime': 0.7983, 'eval_samples_per_second': 2114.532, 'eval_steps_per_second': 33.822, 'epoch': 1.0}\n",
            "{'loss': 0.6011, 'grad_norm': 14.319256782531738, 'learning_rate': 2.4801438395510156e-05, 'epoch': 2.0}\n",
            "{'eval_loss': 0.551771342754364, 'eval_accuracy': 0.6806872037914692, 'eval_f1': 0.6752204179150083, 'eval_precision': 0.7146881287726359, 'eval_recall': 0.6926082413018431, 'eval_runtime': 0.8076, 'eval_samples_per_second': 2090.058, 'eval_steps_per_second': 33.431, 'epoch': 2.0}\n",
            "{'loss': 0.5558, 'grad_norm': 22.884658813476562, 'learning_rate': 3.7202157593265234e-05, 'epoch': 3.0}\n",
            "{'eval_loss': 0.5488858819007874, 'eval_accuracy': 0.7008293838862559, 'eval_f1': 0.6953193831615028, 'eval_precision': 0.7392857142857143, 'eval_recall': 0.7131642520593383, 'eval_runtime': 0.7983, 'eval_samples_per_second': 2114.584, 'eval_steps_per_second': 33.823, 'epoch': 3.0}\n",
            "{'loss': 0.4999, 'grad_norm': 25.427074432373047, 'learning_rate': 8.656496089274301e-07, 'epoch': 4.0}\n",
            "{'eval_loss': 0.49307385087013245, 'eval_accuracy': 0.7659952606635071, 'eval_f1': 0.7655323891003296, 'eval_precision': 0.7653958079358214, 'eval_recall': 0.7664905716787926, 'eval_runtime': 0.8154, 'eval_samples_per_second': 2070.164, 'eval_steps_per_second': 33.113, 'epoch': 4.0}\n",
            "{'train_runtime': 36.8556, 'train_samples_per_second': 549.278, 'train_steps_per_second': 17.257, 'train_loss': 0.58204478137898, 'epoch': 4.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-12 16:37:37,965] Trial 28 finished with value: 0.7655323891003296 and parameters: {'learning_rate': 3.924278227137683e-05, 'weight_decay': 0.01, 'per_device_train_batch_size': 32, 'warmup_steps': 500, 'num_train_epochs': 4, 'adam_beta1': 0.8565261683220612, 'adam_beta2': 0.9728497696840638, 'adam_epsilon': 3.1440021302335226e-07, 'gradient_accumulation_steps': 1, 'lr_scheduler_type': 'linear', 'label_smoothing_factor': 0.18019558166101035}. Best is trial 26 with value: 0.784837751503517.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.49307385087013245, 'eval_accuracy': 0.7659952606635071, 'eval_f1': 0.7655323891003296, 'eval_precision': 0.7653958079358214, 'eval_recall': 0.7664905716787926, 'eval_runtime': 0.812, 'eval_samples_per_second': 2078.855, 'eval_steps_per_second': 33.252, 'epoch': 4.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ai4bharat/indic-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ai4bharat/indic-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.6654, 'grad_norm': 1.4896259307861328, 'learning_rate': 1.571468001117588e-05, 'epoch': 1.0}\n",
            "{'eval_loss': 0.6111540794372559, 'eval_accuracy': 0.6860189573459715, 'eval_f1': 0.685944448422912, 'eval_precision': 0.6875122122649798, 'eval_recall': 0.6881172550053802, 'eval_runtime': 0.8133, 'eval_samples_per_second': 2075.596, 'eval_steps_per_second': 33.2, 'epoch': 1.0}\n",
            "{'loss': 0.6034, 'grad_norm': 5.265953540802002, 'learning_rate': 3.142936002235176e-05, 'epoch': 2.0}\n",
            "{'eval_loss': 0.5853397250175476, 'eval_accuracy': 0.7014218009478673, 'eval_f1': 0.7014213817936785, 'eval_precision': 0.7044906959006698, 'eval_recall': 0.7045552943432893, 'eval_runtime': 0.8117, 'eval_samples_per_second': 2079.519, 'eval_steps_per_second': 33.262, 'epoch': 2.0}\n",
            "{'loss': 0.5501, 'grad_norm': 8.029162406921387, 'learning_rate': 4.724350003359838e-05, 'epoch': 3.0}\n",
            "{'eval_loss': 0.5581828355789185, 'eval_accuracy': 0.6860189573459715, 'eval_f1': 0.676604973973395, 'eval_precision': 0.7399087493742742, 'eval_recall': 0.7007377091950635, 'eval_runtime': 0.8134, 'eval_samples_per_second': 2075.307, 'eval_steps_per_second': 33.195, 'epoch': 3.0}\n",
            "{'loss': 0.4891, 'grad_norm': 6.164804935455322, 'learning_rate': 2.8163611234536248e-08, 'epoch': 3.9779179810725553}\n",
            "{'eval_loss': 0.48437535762786865, 'eval_accuracy': 0.7659952606635071, 'eval_f1': 0.7659951785376489, 'eval_precision': 0.7694218838127467, 'eval_recall': 0.7694648188445141, 'eval_runtime': 0.7995, 'eval_samples_per_second': 2111.22, 'eval_steps_per_second': 33.77, 'epoch': 3.9779179810725553}\n",
            "{'train_runtime': 57.0356, 'train_samples_per_second': 354.936, 'train_steps_per_second': 11.081, 'train_loss': 0.5775640946400317, 'epoch': 3.9779179810725553}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-12 16:38:36,821] Trial 29 finished with value: 0.7659951785376489 and parameters: {'learning_rate': 4.9730000035366715e-05, 'weight_decay': 0.1, 'per_device_train_batch_size': 16, 'warmup_steps': 500, 'num_train_epochs': 4, 'adam_beta1': 0.8722128034843306, 'adam_beta2': 0.9536665840146046, 'adam_epsilon': 1.2410864128067605e-07, 'gradient_accumulation_steps': 2, 'lr_scheduler_type': 'cosine', 'label_smoothing_factor': 0.10140446355444149}. Best is trial 26 with value: 0.784837751503517.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.48437535762786865, 'eval_accuracy': 0.7659952606635071, 'eval_f1': 0.7659951785376489, 'eval_precision': 0.7694218838127467, 'eval_recall': 0.7694648188445141, 'eval_runtime': 0.7954, 'eval_samples_per_second': 2122.09, 'eval_steps_per_second': 33.943, 'epoch': 3.9779179810725553}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ai4bharat/indic-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ai4bharat/indic-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.6404, 'grad_norm': 2.49338698387146, 'learning_rate': 3.505634142992875e-05, 'epoch': 1.0}\n",
            "{'eval_loss': 0.5900595188140869, 'eval_accuracy': 0.7138625592417062, 'eval_f1': 0.7066322097038666, 'eval_precision': 0.7188314561063682, 'eval_recall': 0.7064824203518045, 'eval_runtime': 0.8205, 'eval_samples_per_second': 2057.193, 'eval_steps_per_second': 32.905, 'epoch': 1.0}\n",
            "{'loss': 0.5777, 'grad_norm': 6.415389060974121, 'learning_rate': 2.061641024681601e-05, 'epoch': 2.0}\n",
            "{'eval_loss': 0.518600583076477, 'eval_accuracy': 0.7494075829383886, 'eval_f1': 0.7484341964381029, 'eval_precision': 0.7482902179056719, 'eval_recall': 0.7486211141933219, 'eval_runtime': 0.8225, 'eval_samples_per_second': 2052.33, 'eval_steps_per_second': 32.828, 'epoch': 2.0}\n",
            "{'loss': 0.513, 'grad_norm': 10.221381187438965, 'learning_rate': 6.044637449731956e-06, 'epoch': 3.0}\n",
            "{'eval_loss': 0.49520179629325867, 'eval_accuracy': 0.7665876777251185, 'eval_f1': 0.765480160006319, 'eval_precision': 0.7655246841737733, 'eval_recall': 0.7654378094648471, 'eval_runtime': 0.842, 'eval_samples_per_second': 2004.806, 'eval_steps_per_second': 32.067, 'epoch': 3.0}\n",
            "{'loss': 0.4461, 'grad_norm': 8.917128562927246, 'learning_rate': 1.0137983696762727e-09, 'epoch': 3.9889415481832544}\n",
            "{'eval_loss': 0.49440813064575195, 'eval_accuracy': 0.7760663507109005, 'eval_f1': 0.7743784715002822, 'eval_precision': 0.775569799053115, 'eval_recall': 0.773754137362552, 'eval_runtime': 0.8254, 'eval_samples_per_second': 2044.999, 'eval_steps_per_second': 32.71, 'epoch': 3.9889415481832544}\n",
            "{'train_runtime': 101.3507, 'train_samples_per_second': 199.742, 'train_steps_per_second': 12.472, 'train_loss': 0.544611701482459, 'epoch': 3.9889415481832544}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-12 16:40:20,053] Trial 30 finished with value: 0.7743784715002822 and parameters: {'learning_rate': 4.1028871962122556e-05, 'weight_decay': 0.1, 'per_device_train_batch_size': 8, 'warmup_steps': 0, 'num_train_epochs': 4, 'adam_beta1': 0.894054259980831, 'adam_beta2': 0.9674707681412987, 'adam_epsilon': 5.820127418578835e-07, 'gradient_accumulation_steps': 2, 'lr_scheduler_type': 'cosine_with_restarts', 'label_smoothing_factor': 0.18735500283742462}. Best is trial 26 with value: 0.784837751503517.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.49440813064575195, 'eval_accuracy': 0.7760663507109005, 'eval_f1': 0.7743784715002822, 'eval_precision': 0.775569799053115, 'eval_recall': 0.773754137362552, 'eval_runtime': 0.7975, 'eval_samples_per_second': 2116.6, 'eval_steps_per_second': 33.856, 'epoch': 3.9889415481832544}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ai4bharat/indic-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ai4bharat/indic-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.6442, 'grad_norm': 2.691969871520996, 'learning_rate': 4.2322100733632325e-05, 'epoch': 1.0}\n",
            "{'eval_loss': 0.5854297876358032, 'eval_accuracy': 0.7014218009478673, 'eval_f1': 0.6995282878927226, 'eval_precision': 0.7000058040194959, 'eval_recall': 0.6992498804801104, 'eval_runtime': 0.8373, 'eval_samples_per_second': 2016.032, 'eval_steps_per_second': 32.247, 'epoch': 1.0}\n",
            "{'loss': 0.57, 'grad_norm': 3.593366861343384, 'learning_rate': 2.9502625777085772e-05, 'epoch': 2.0}\n",
            "{'eval_loss': 0.5273114442825317, 'eval_accuracy': 0.7505924170616114, 'eval_f1': 0.7480063671433277, 'eval_precision': 0.7506034260874539, 'eval_recall': 0.7471586702336949, 'eval_runtime': 0.8368, 'eval_samples_per_second': 2017.093, 'eval_steps_per_second': 32.264, 'epoch': 2.0}\n",
            "{'loss': 0.509, 'grad_norm': 6.238471031188965, 'learning_rate': 9.449524107177272e-06, 'epoch': 3.0}\n",
            "{'eval_loss': 0.49094700813293457, 'eval_accuracy': 0.7665876777251185, 'eval_f1': 0.7665863670311035, 'eval_precision': 0.7704409305476175, 'eval_recall': 0.770260912976828, 'eval_runtime': 0.8563, 'eval_samples_per_second': 1971.169, 'eval_steps_per_second': 31.529, 'epoch': 3.0}\n",
            "{'loss': 0.4291, 'grad_norm': 5.371413230895996, 'learning_rate': 9.230947430393345e-10, 'epoch': 3.9889415481832544}\n",
            "{'eval_loss': 0.4834212064743042, 'eval_accuracy': 0.7772511848341233, 'eval_f1': 0.7768222716384867, 'eval_precision': 0.7766832807384906, 'eval_recall': 0.7778382624417032, 'eval_runtime': 0.8354, 'eval_samples_per_second': 2020.497, 'eval_steps_per_second': 32.318, 'epoch': 3.9889415481832544}\n",
            "{'train_runtime': 100.638, 'train_samples_per_second': 201.157, 'train_steps_per_second': 12.56, 'train_loss': 0.5384054063241693, 'epoch': 3.9889415481832544}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-12 16:42:02,561] Trial 31 finished with value: 0.7768222716384867 and parameters: {'learning_rate': 4.274083975665805e-05, 'weight_decay': 0.0, 'per_device_train_batch_size': 8, 'warmup_steps': 250, 'num_train_epochs': 4, 'adam_beta1': 0.8833107946155683, 'adam_beta2': 0.9680008813406903, 'adam_epsilon': 1.425956539181473e-08, 'gradient_accumulation_steps': 2, 'lr_scheduler_type': 'cosine', 'label_smoothing_factor': 0.13953971721649405}. Best is trial 26 with value: 0.784837751503517.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.4834212064743042, 'eval_accuracy': 0.7772511848341233, 'eval_f1': 0.7768222716384867, 'eval_precision': 0.7766832807384906, 'eval_recall': 0.7778382624417032, 'eval_runtime': 0.8244, 'eval_samples_per_second': 2047.554, 'eval_steps_per_second': 32.751, 'epoch': 3.9889415481832544}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ai4bharat/indic-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ai4bharat/indic-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.6427, 'grad_norm': 8.020462036132812, 'learning_rate': 4.3446625242958716e-05, 'epoch': 1.0}\n",
            "{'eval_loss': 0.5904233455657959, 'eval_accuracy': 0.6682464454976303, 'eval_f1': 0.6398477826528917, 'eval_precision': 0.7005090566016711, 'eval_recall': 0.6522570573145468, 'eval_runtime': 0.8185, 'eval_samples_per_second': 2062.328, 'eval_steps_per_second': 32.987, 'epoch': 1.0}\n",
            "{'loss': 0.5814, 'grad_norm': 3.2089085578918457, 'learning_rate': 3.0295937347515565e-05, 'epoch': 2.0}\n",
            "{'eval_loss': 0.5126543641090393, 'eval_accuracy': 0.7731042654028436, 'eval_f1': 0.7721188724941602, 'eval_precision': 0.77205745538609, 'eval_recall': 0.7721852184569735, 'eval_runtime': 0.8283, 'eval_samples_per_second': 2038.009, 'eval_steps_per_second': 32.598, 'epoch': 2.0}\n",
            "{'loss': 0.5038, 'grad_norm': 11.52712345123291, 'learning_rate': 9.703617314538542e-06, 'epoch': 3.0}\n",
            "{'eval_loss': 0.4876725375652313, 'eval_accuracy': 0.7790284360189573, 'eval_f1': 0.7782963342470866, 'eval_precision': 0.7780526102519079, 'eval_recall': 0.7786992287265173, 'eval_runtime': 0.8391, 'eval_samples_per_second': 2011.594, 'eval_steps_per_second': 32.176, 'epoch': 3.0}\n",
            "{'loss': 0.4263, 'grad_norm': 9.878439903259277, 'learning_rate': 9.479163214909986e-10, 'epoch': 3.9889415481832544}\n",
            "{'eval_loss': 0.4883270859718323, 'eval_accuracy': 0.7837677725118484, 'eval_f1': 0.7820234823952793, 'eval_precision': 0.7835238095238095, 'eval_recall': 0.7812898840339761, 'eval_runtime': 0.8269, 'eval_samples_per_second': 2041.463, 'eval_steps_per_second': 32.654, 'epoch': 3.9889415481832544}\n",
            "{'train_runtime': 100.6774, 'train_samples_per_second': 201.078, 'train_steps_per_second': 12.555, 'train_loss': 0.5388940859444534, 'epoch': 3.9889415481832544}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-12 16:43:45,170] Trial 32 finished with value: 0.7820234823952793 and parameters: {'learning_rate': 4.38901206025405e-05, 'weight_decay': 0.0, 'per_device_train_batch_size': 8, 'warmup_steps': 250, 'num_train_epochs': 4, 'adam_beta1': 0.8800935438521875, 'adam_beta2': 0.9659975702326824, 'adam_epsilon': 1.5890991561512696e-08, 'gradient_accumulation_steps': 2, 'lr_scheduler_type': 'cosine', 'label_smoothing_factor': 0.16140977357221242}. Best is trial 26 with value: 0.784837751503517.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.4883270859718323, 'eval_accuracy': 0.7837677725118484, 'eval_f1': 0.7820234823952793, 'eval_precision': 0.7835238095238095, 'eval_recall': 0.7812898840339761, 'eval_runtime': 0.804, 'eval_samples_per_second': 2099.547, 'eval_steps_per_second': 33.583, 'epoch': 3.9889415481832544}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ai4bharat/indic-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ai4bharat/indic-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.648, 'grad_norm': 9.361392974853516, 'learning_rate': 3.583782808328119e-05, 'epoch': 1.0}\n",
            "{'eval_loss': 0.5910580158233643, 'eval_accuracy': 0.707345971563981, 'eval_f1': 0.7009955436477259, 'eval_precision': 0.7103315770415968, 'eval_recall': 0.7006996320620742, 'eval_runtime': 0.8197, 'eval_samples_per_second': 2059.18, 'eval_steps_per_second': 32.937, 'epoch': 1.0}\n",
            "{'loss': 0.5903, 'grad_norm': 3.6594417095184326, 'learning_rate': 2.4938320022014914e-05, 'epoch': 2.0}\n",
            "{'eval_loss': 0.5157023668289185, 'eval_accuracy': 0.7470379146919431, 'eval_f1': 0.7469824156050491, 'eval_precision': 0.7486756866185302, 'eval_recall': 0.7494559905907174, 'eval_runtime': 0.8275, 'eval_samples_per_second': 2039.817, 'eval_steps_per_second': 32.627, 'epoch': 2.0}\n",
            "{'loss': 0.5277, 'grad_norm': 4.301440238952637, 'learning_rate': 7.957726653660202e-06, 'epoch': 3.0}\n",
            "{'eval_loss': 0.4964108467102051, 'eval_accuracy': 0.7648104265402843, 'eval_f1': 0.7648103439985989, 'eval_precision': 0.7683984698972597, 'eval_recall': 0.7683549409310846, 'eval_runtime': 0.8236, 'eval_samples_per_second': 2049.632, 'eval_steps_per_second': 32.784, 'epoch': 3.0}\n",
            "{'loss': 0.4615, 'grad_norm': 14.633809089660645, 'learning_rate': 7.8190796125037e-10, 'epoch': 3.9889415481832544}\n",
            "{'eval_loss': 0.4968096613883972, 'eval_accuracy': 0.7802132701421801, 'eval_f1': 0.7790780903678445, 'eval_precision': 0.7792584434654919, 'eval_recall': 0.7789248709960837, 'eval_runtime': 0.8243, 'eval_samples_per_second': 2047.827, 'eval_steps_per_second': 32.756, 'epoch': 3.9889415481832544}\n",
            "{'train_runtime': 100.2673, 'train_samples_per_second': 201.9, 'train_steps_per_second': 12.606, 'train_loss': 0.5572025323215919, 'epoch': 3.9889415481832544}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-12 16:45:27,414] Trial 33 finished with value: 0.7790780903678445 and parameters: {'learning_rate': 3.620365420587516e-05, 'weight_decay': 0.0, 'per_device_train_batch_size': 8, 'warmup_steps': 250, 'num_train_epochs': 4, 'adam_beta1': 0.8700165530468561, 'adam_beta2': 0.9502868118867783, 'adam_epsilon': 2.4332951895623625e-08, 'gradient_accumulation_steps': 2, 'lr_scheduler_type': 'cosine', 'label_smoothing_factor': 0.1619754306632154}. Best is trial 26 with value: 0.784837751503517.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.4968096613883972, 'eval_accuracy': 0.7802132701421801, 'eval_f1': 0.7790780903678445, 'eval_precision': 0.7792584434654919, 'eval_recall': 0.7789248709960837, 'eval_runtime': 0.8028, 'eval_samples_per_second': 2102.612, 'eval_steps_per_second': 33.632, 'epoch': 3.9889415481832544}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ai4bharat/indic-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ai4bharat/indic-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.6555, 'grad_norm': 1.229231595993042, 'learning_rate': 2.9106312723790394e-05, 'epoch': 1.0}\n",
            "{'eval_loss': 0.6137180328369141, 'eval_accuracy': 0.6540284360189573, 'eval_f1': 0.649776107843402, 'eval_precision': 0.6785516932068141, 'eval_recall': 0.6645009709668912, 'eval_runtime': 0.8361, 'eval_samples_per_second': 2018.823, 'eval_steps_per_second': 32.292, 'epoch': 1.0}\n",
            "{'loss': 0.5901, 'grad_norm': 1.5206847190856934, 'learning_rate': 4.274464212351453e-05, 'epoch': 2.0}\n",
            "{'eval_loss': 0.5347189903259277, 'eval_accuracy': 0.7280805687203792, 'eval_f1': 0.7260591892215835, 'eval_precision': 0.7270278402252651, 'eval_recall': 0.7255886795273359, 'eval_runtime': 0.8313, 'eval_samples_per_second': 2030.671, 'eval_steps_per_second': 32.481, 'epoch': 2.0}\n",
            "{'loss': 0.5357, 'grad_norm': 3.545498847961426, 'learning_rate': 1.6671914198310495e-05, 'epoch': 3.0}\n",
            "{'eval_loss': 0.5017962455749512, 'eval_accuracy': 0.7671800947867299, 'eval_f1': 0.7665014820157103, 'eval_precision': 0.7662518631007622, 'eval_recall': 0.767037754182491, 'eval_runtime': 0.8362, 'eval_samples_per_second': 2018.705, 'eval_steps_per_second': 32.29, 'epoch': 3.0}\n",
            "{'loss': 0.4559, 'grad_norm': 6.651536464691162, 'learning_rate': 3.1148227729865728e-09, 'epoch': 3.9794628751974725}\n",
            "{'eval_loss': 0.4967586398124695, 'eval_accuracy': 0.7754739336492891, 'eval_f1': 0.772437066017766, 'eval_precision': 0.7773679777063632, 'eval_recall': 0.7711895719425119, 'eval_runtime': 0.8191, 'eval_samples_per_second': 2060.856, 'eval_steps_per_second': 32.964, 'epoch': 3.9794628751974725}\n",
            "{'train_runtime': 98.7736, 'train_samples_per_second': 204.953, 'train_steps_per_second': 6.398, 'train_loss': 0.5599381410622899, 'epoch': 3.9794628751974725}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-12 16:47:08,000] Trial 34 finished with value: 0.772437066017766 and parameters: {'learning_rate': 4.605429228447847e-05, 'weight_decay': 0.0, 'per_device_train_batch_size': 8, 'warmup_steps': 250, 'num_train_epochs': 4, 'adam_beta1': 0.8566583185507971, 'adam_beta2': 0.9653794939162958, 'adam_epsilon': 1.899544265637705e-08, 'gradient_accumulation_steps': 4, 'lr_scheduler_type': 'cosine', 'label_smoothing_factor': 0.1032934468376483}. Best is trial 26 with value: 0.784837751503517.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.4967586398124695, 'eval_accuracy': 0.7754739336492891, 'eval_f1': 0.772437066017766, 'eval_precision': 0.7773679777063632, 'eval_recall': 0.7711895719425119, 'eval_runtime': 0.8036, 'eval_samples_per_second': 2100.439, 'eval_steps_per_second': 33.597, 'epoch': 3.9794628751974725}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ai4bharat/indic-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ai4bharat/indic-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.6315, 'grad_norm': 3.320995807647705, 'learning_rate': 1.979737222765219e-05, 'epoch': 1.0}\n",
            "{'eval_loss': 0.549763560295105, 'eval_accuracy': 0.7186018957345972, 'eval_f1': 0.7148744974831931, 'eval_precision': 0.7188429609131788, 'eval_recall': 0.7141373343468431, 'eval_runtime': 0.818, 'eval_samples_per_second': 2063.613, 'eval_steps_per_second': 33.008, 'epoch': 1.0}\n",
            "{'loss': 0.564, 'grad_norm': 9.879278182983398, 'learning_rate': 1.4836291598272157e-05, 'epoch': 2.0}\n",
            "{'eval_loss': 0.5080326795578003, 'eval_accuracy': 0.7505924170616114, 'eval_f1': 0.750244519615165, 'eval_precision': 0.7503650147409799, 'eval_recall': 0.7514994633944776, 'eval_runtime': 0.8241, 'eval_samples_per_second': 2048.318, 'eval_steps_per_second': 32.763, 'epoch': 2.0}\n",
            "{'loss': 0.4977, 'grad_norm': 4.541494369506836, 'learning_rate': 9.87521096889212e-06, 'epoch': 3.0}\n",
            "{'eval_loss': 0.4941796362400055, 'eval_accuracy': 0.7606635071090048, 'eval_f1': 0.7606631711203296, 'eval_precision': 0.7639850502811883, 'eval_recall': 0.7640684429414162, 'eval_runtime': 0.8258, 'eval_samples_per_second': 2044.061, 'eval_steps_per_second': 32.695, 'epoch': 3.0}\n",
            "{'loss': 0.428, 'grad_norm': 9.450616836547852, 'learning_rate': 4.929780436134735e-06, 'epoch': 4.0}\n",
            "{'eval_loss': 0.49828165769577026, 'eval_accuracy': 0.7606635071090048, 'eval_f1': 0.7606581311769991, 'eval_precision': 0.7635121525210055, 'eval_recall': 0.7638272877658172, 'eval_runtime': 0.8208, 'eval_samples_per_second': 2056.536, 'eval_steps_per_second': 32.895, 'epoch': 4.0}\n",
            "{'loss': 0.3516, 'grad_norm': 20.978544235229492, 'learning_rate': 4.6950289867949866e-08, 'epoch': 4.9857819905213265}\n",
            "{'eval_loss': 0.5259973406791687, 'eval_accuracy': 0.7648104265402843, 'eval_f1': 0.7644393123726868, 'eval_precision': 0.7644474584267013, 'eval_recall': 0.7656218489409621, 'eval_runtime': 0.8151, 'eval_samples_per_second': 2070.962, 'eval_steps_per_second': 33.126, 'epoch': 4.9857819905213265}\n",
            "{'train_runtime': 126.7467, 'train_samples_per_second': 199.65, 'train_steps_per_second': 12.466, 'train_loss': 0.49500567279284513, 'epoch': 4.9857819905213265}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-12 16:49:16,544] Trial 35 finished with value: 0.7644393123726868 and parameters: {'learning_rate': 2.3162143001521933e-05, 'weight_decay': 0.0, 'per_device_train_batch_size': 8, 'warmup_steps': 100, 'num_train_epochs': 5, 'adam_beta1': 0.8789416044389076, 'adam_beta2': 0.9761065169653476, 'adam_epsilon': 1.45022502173426e-07, 'gradient_accumulation_steps': 2, 'lr_scheduler_type': 'linear', 'label_smoothing_factor': 0.17408228559600772}. Best is trial 26 with value: 0.784837751503517.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.5259973406791687, 'eval_accuracy': 0.7648104265402843, 'eval_f1': 0.7644393123726868, 'eval_precision': 0.7644474584267013, 'eval_recall': 0.7656218489409621, 'eval_runtime': 0.7996, 'eval_samples_per_second': 2111.116, 'eval_steps_per_second': 33.768, 'epoch': 4.9857819905213265}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ai4bharat/indic-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ai4bharat/indic-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.6855, 'grad_norm': 0.3891807794570923, 'learning_rate': 7.853277019495099e-06, 'epoch': 1.0}\n",
            "{'eval_loss': 0.670242965221405, 'eval_accuracy': 0.6765402843601895, 'eval_f1': 0.6756181447281543, 'eval_precision': 0.6754882621649019, 'eval_recall': 0.6760228293566234, 'eval_runtime': 0.7957, 'eval_samples_per_second': 2121.526, 'eval_steps_per_second': 33.934, 'epoch': 1.0}\n",
            "{'loss': 0.6224, 'grad_norm': 6.567438125610352, 'learning_rate': 1.5805962608857223e-05, 'epoch': 2.0}\n",
            "{'eval_loss': 0.5966561436653137, 'eval_accuracy': 0.6954976303317536, 'eval_f1': 0.6953590669820171, 'eval_precision': 0.7016670764132349, 'eval_recall': 0.7001312955956038, 'eval_runtime': 0.8037, 'eval_samples_per_second': 2100.379, 'eval_steps_per_second': 33.596, 'epoch': 2.0}\n",
            "{'loss': 0.5787, 'grad_norm': 9.313718795776367, 'learning_rate': 2.3659239628352322e-05, 'epoch': 3.0}\n",
            "{'eval_loss': 0.5663836598396301, 'eval_accuracy': 0.6712085308056872, 'eval_f1': 0.6647100570877618, 'eval_precision': 0.7077592365788936, 'eval_recall': 0.6837292179944069, 'eval_runtime': 0.8101, 'eval_samples_per_second': 2083.789, 'eval_steps_per_second': 33.331, 'epoch': 3.0}\n",
            "{'loss': 0.5386, 'grad_norm': 8.62896728515625, 'learning_rate': 3.151251664784742e-05, 'epoch': 4.0}\n",
            "{'eval_loss': 0.5307334661483765, 'eval_accuracy': 0.7411137440758294, 'eval_f1': 0.7410656711650283, 'eval_precision': 0.746529430429954, 'eval_recall': 0.745433917135697, 'eval_runtime': 0.7935, 'eval_samples_per_second': 2127.151, 'eval_steps_per_second': 34.024, 'epoch': 4.0}\n",
            "{'loss': 0.5044, 'grad_norm': 5.395706653594971, 'learning_rate': 3.886875081800739e-05, 'epoch': 4.943396226415095}\n",
            "{'eval_loss': 0.5279014110565186, 'eval_accuracy': 0.7553317535545023, 'eval_f1': 0.7553247980267178, 'eval_precision': 0.758086959460982, 'eval_recall': 0.7584309118627193, 'eval_runtime': 0.8031, 'eval_samples_per_second': 2101.932, 'eval_steps_per_second': 33.621, 'epoch': 4.943396226415095}\n",
            "{'train_runtime': 44.5305, 'train_samples_per_second': 568.262, 'train_steps_per_second': 8.87, 'train_loss': 0.5869439426856705, 'epoch': 4.943396226415095}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-12 16:50:02,920] Trial 36 finished with value: 0.7553247980267178 and parameters: {'learning_rate': 4.9704284933513284e-05, 'weight_decay': 0.01, 'per_device_train_batch_size': 32, 'warmup_steps': 500, 'num_train_epochs': 5, 'adam_beta1': 0.8902773466777361, 'adam_beta2': 0.970305396759612, 'adam_epsilon': 1.2744841040413767e-08, 'gradient_accumulation_steps': 2, 'lr_scheduler_type': 'polynomial', 'label_smoothing_factor': 0.13883880671396231}. Best is trial 26 with value: 0.784837751503517.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.5279014110565186, 'eval_accuracy': 0.7553317535545023, 'eval_f1': 0.7553247980267178, 'eval_precision': 0.758086959460982, 'eval_recall': 0.7584309118627193, 'eval_runtime': 0.7961, 'eval_samples_per_second': 2120.456, 'eval_steps_per_second': 33.917, 'epoch': 4.943396226415095}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ai4bharat/indic-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ai4bharat/indic-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.6476, 'grad_norm': 3.750575542449951, 'learning_rate': 4.345273289663733e-05, 'epoch': 1.0}\n",
            "{'eval_loss': 0.6011258959770203, 'eval_accuracy': 0.6943127962085308, 'eval_f1': 0.6923155181768854, 'eval_precision': 0.7131821528188218, 'eval_recall': 0.7029602855502921, 'eval_runtime': 0.8011, 'eval_samples_per_second': 2107.162, 'eval_steps_per_second': 33.705, 'epoch': 1.0}\n",
            "{'loss': 0.5632, 'grad_norm': 9.99544620513916, 'learning_rate': 3.0391822110446448e-05, 'epoch': 2.0}\n",
            "{'eval_loss': 0.505931556224823, 'eval_accuracy': 0.7636255924170616, 'eval_f1': 0.7634421971560885, 'eval_precision': 0.7640627544802565, 'eval_recall': 0.7652354365543297, 'eval_runtime': 0.8006, 'eval_samples_per_second': 2108.549, 'eval_steps_per_second': 33.727, 'epoch': 2.0}\n",
            "{'loss': 0.4772, 'grad_norm': 40.368255615234375, 'learning_rate': 9.859421964465888e-06, 'epoch': 3.0}\n",
            "{'eval_loss': 0.5234618782997131, 'eval_accuracy': 0.7701421800947867, 'eval_f1': 0.7701341127681589, 'eval_precision': 0.7728687967081944, 'eval_recall': 0.7732690064829844, 'eval_runtime': 0.8072, 'eval_samples_per_second': 2091.11, 'eval_steps_per_second': 33.448, 'epoch': 3.0}\n",
            "{'loss': 0.3655, 'grad_norm': 32.85593795776367, 'learning_rate': 9.40538928974029e-10, 'epoch': 4.0}\n",
            "{'eval_loss': 0.5245245099067688, 'eval_accuracy': 0.7766587677725119, 'eval_f1': 0.7758240906601004, 'eval_precision': 0.7756371265241756, 'eval_recall': 0.7760775476069932, 'eval_runtime': 0.8154, 'eval_samples_per_second': 2070.171, 'eval_steps_per_second': 33.113, 'epoch': 4.0}\n",
            "{'train_runtime': 59.1585, 'train_samples_per_second': 342.199, 'train_steps_per_second': 21.434, 'train_loss': 0.5134067655737844, 'epoch': 4.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-12 16:51:04,016] Trial 37 finished with value: 0.7758240906601004 and parameters: {'learning_rate': 4.389278829154606e-05, 'weight_decay': 0.1, 'per_device_train_batch_size': 16, 'warmup_steps': 250, 'num_train_epochs': 4, 'adam_beta1': 0.8754105872867004, 'adam_beta2': 0.9806044223794713, 'adam_epsilon': 5.00680829720073e-08, 'gradient_accumulation_steps': 1, 'lr_scheduler_type': 'cosine', 'label_smoothing_factor': 0.1647075689249505}. Best is trial 26 with value: 0.784837751503517.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.5245245099067688, 'eval_accuracy': 0.7766587677725119, 'eval_f1': 0.7758240906601004, 'eval_precision': 0.7756371265241756, 'eval_recall': 0.7760775476069932, 'eval_runtime': 0.801, 'eval_samples_per_second': 2107.245, 'eval_steps_per_second': 33.706, 'epoch': 4.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ai4bharat/indic-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ai4bharat/indic-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.6448, 'grad_norm': 1.8678524494171143, 'learning_rate': 9.111952984203908e-06, 'epoch': 1.0}\n",
            "{'eval_loss': 0.6020362377166748, 'eval_accuracy': 0.6771327014218009, 'eval_f1': 0.6766232373881973, 'eval_precision': 0.6767574231013316, 'eval_recall': 0.6775423890157344, 'eval_runtime': 0.8224, 'eval_samples_per_second': 2052.492, 'eval_steps_per_second': 32.83, 'epoch': 1.0}\n",
            "{'loss': 0.5724, 'grad_norm': 6.673666477203369, 'learning_rate': 6.5934711201481544e-06, 'epoch': 2.0}\n",
            "{'eval_loss': 0.5563668608665466, 'eval_accuracy': 0.6854265402843602, 'eval_f1': 0.6853681267474372, 'eval_precision': 0.6903373676933886, 'eval_recall': 0.6894915574534578, 'eval_runtime': 0.8219, 'eval_samples_per_second': 2053.726, 'eval_steps_per_second': 32.85, 'epoch': 2.0}\n",
            "{'loss': 0.5272, 'grad_norm': 7.951581954956055, 'learning_rate': 3.4804563360748595e-06, 'epoch': 3.0}\n",
            "{'eval_loss': 0.5481971502304077, 'eval_accuracy': 0.7180094786729858, 'eval_f1': 0.7166246975756678, 'eval_precision': 0.7167008883526316, 'eval_recall': 0.71655664255585, 'eval_runtime': 0.8551, 'eval_samples_per_second': 1974.107, 'eval_steps_per_second': 31.576, 'epoch': 3.0}\n",
            "{'loss': 0.4892, 'grad_norm': 10.59173583984375, 'learning_rate': 9.502330967624932e-07, 'epoch': 4.0}\n",
            "{'eval_loss': 0.5417979955673218, 'eval_accuracy': 0.7257109004739336, 'eval_f1': 0.7217658030084626, 'eval_precision': 0.7265442452284557, 'eval_recall': 0.7209573719444864, 'eval_runtime': 0.8404, 'eval_samples_per_second': 2008.468, 'eval_steps_per_second': 32.126, 'epoch': 4.0}\n",
            "{'loss': 0.4663, 'grad_norm': 15.872382164001465, 'learning_rate': 3.5844435813036956e-10, 'epoch': 4.973143759873618}\n",
            "{'eval_loss': 0.5411630868911743, 'eval_accuracy': 0.7280805687203792, 'eval_f1': 0.725521162685655, 'eval_precision': 0.7274269005847953, 'eval_recall': 0.7248652140005387, 'eval_runtime': 0.8445, 'eval_samples_per_second': 1998.914, 'eval_steps_per_second': 31.973, 'epoch': 4.973143759873618}\n",
            "{'train_runtime': 122.4288, 'train_samples_per_second': 206.692, 'train_steps_per_second': 6.453, 'train_loss': 0.5404683270031893, 'epoch': 4.973143759873618}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-12 16:53:08,304] Trial 38 finished with value: 0.725521162685655 and parameters: {'learning_rate': 1.0073927456223013e-05, 'weight_decay': 0.0, 'per_device_train_batch_size': 8, 'warmup_steps': 0, 'num_train_epochs': 5, 'adam_beta1': 0.9077661467242895, 'adam_beta2': 0.9612283970926411, 'adam_epsilon': 1.031201514571245e-08, 'gradient_accumulation_steps': 4, 'lr_scheduler_type': 'cosine_with_restarts', 'label_smoothing_factor': 0.18625736199806875}. Best is trial 26 with value: 0.784837751503517.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.5411630868911743, 'eval_accuracy': 0.7280805687203792, 'eval_f1': 0.725521162685655, 'eval_precision': 0.7274269005847953, 'eval_recall': 0.7248652140005387, 'eval_runtime': 0.8043, 'eval_samples_per_second': 2098.768, 'eval_steps_per_second': 33.57, 'epoch': 4.973143759873618}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ai4bharat/indic-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ai4bharat/indic-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.6707, 'grad_norm': 1.4574757814407349, 'learning_rate': 2.7598702901921102e-05, 'epoch': 1.0}\n",
            "{'eval_loss': 0.6208125948905945, 'eval_accuracy': 0.6777251184834123, 'eval_f1': 0.6770355155701554, 'eval_precision': 0.6769945005934864, 'eval_recall': 0.6776954026797839, 'eval_runtime': 0.804, 'eval_samples_per_second': 2099.614, 'eval_steps_per_second': 33.584, 'epoch': 1.0}\n",
            "{'loss': 0.5941, 'grad_norm': 6.262739658355713, 'learning_rate': 2.5419932932498898e-05, 'epoch': 2.0}\n",
            "{'eval_loss': 0.5596458911895752, 'eval_accuracy': 0.693127962085308, 'eval_f1': 0.6930435026820569, 'eval_precision': 0.6945178813624833, 'eval_recall': 0.6951784477786224, 'eval_runtime': 0.7936, 'eval_samples_per_second': 2126.956, 'eval_steps_per_second': 34.021, 'epoch': 2.0}\n",
            "{'loss': 0.5485, 'grad_norm': 7.512438774108887, 'learning_rate': 1.2679329737165061e-05, 'epoch': 3.0}\n",
            "{'eval_loss': 0.5321617126464844, 'eval_accuracy': 0.7428909952606635, 'eval_f1': 0.7385299555289062, 'eval_precision': 0.745378524653453, 'eval_recall': 0.7375329120404126, 'eval_runtime': 0.8027, 'eval_samples_per_second': 2103.032, 'eval_steps_per_second': 33.639, 'epoch': 3.0}\n",
            "{'loss': 0.4872, 'grad_norm': 7.234081745147705, 'learning_rate': 5.838203745063467e-07, 'epoch': 3.9559748427672954}\n",
            "{'eval_loss': 0.5253416895866394, 'eval_accuracy': 0.735781990521327, 'eval_f1': 0.7346136491821771, 'eval_precision': 0.7345776394275572, 'eval_recall': 0.7346517423108871, 'eval_runtime': 0.8005, 'eval_samples_per_second': 2108.658, 'eval_steps_per_second': 33.729, 'epoch': 3.9559748427672954}\n",
            "{'train_runtime': 35.5354, 'train_samples_per_second': 569.686, 'train_steps_per_second': 8.893, 'train_loss': 0.5762396583074256, 'epoch': 3.9559748427672954}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-12 16:53:45,749] Trial 39 finished with value: 0.7385299555289062 and parameters: {'learning_rate': 3.493506696445709e-05, 'weight_decay': 0.01, 'per_device_train_batch_size': 32, 'warmup_steps': 100, 'num_train_epochs': 4, 'adam_beta1': 0.9494869929534815, 'adam_beta2': 0.9534889654587171, 'adam_epsilon': 2.67483679715371e-08, 'gradient_accumulation_steps': 2, 'lr_scheduler_type': 'polynomial', 'label_smoothing_factor': 0.1554568684903373}. Best is trial 26 with value: 0.784837751503517.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.5321617126464844, 'eval_accuracy': 0.7428909952606635, 'eval_f1': 0.7385299555289062, 'eval_precision': 0.745378524653453, 'eval_recall': 0.7375329120404126, 'eval_runtime': 0.8085, 'eval_samples_per_second': 2087.73, 'eval_steps_per_second': 33.394, 'epoch': 3.9559748427672954}\n",
            "\n",
            "Best Hyperparameters:\n",
            "learning_rate: 4.1161318222053906e-05\n",
            "weight_decay: 0.0\n",
            "per_device_train_batch_size: 8\n",
            "warmup_steps: 0\n",
            "num_train_epochs: 4\n",
            "adam_beta1: 0.8600049278155624\n",
            "adam_beta2: 0.9667180237875557\n",
            "adam_epsilon: 1.573419909436933e-08\n",
            "gradient_accumulation_steps: 2\n",
            "lr_scheduler_type: cosine_with_restarts\n",
            "label_smoothing_factor: 0.15169409960372646\n",
            "Best F1: 0.7848\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ai4bharat/indic-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ai4bharat/indic-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1264' max='1264' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1264/1264 01:41, Epoch 3/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.620800</td>\n",
              "      <td>0.567970</td>\n",
              "      <td>0.722749</td>\n",
              "      <td>0.717504</td>\n",
              "      <td>0.725293</td>\n",
              "      <td>0.716816</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.543100</td>\n",
              "      <td>0.487818</td>\n",
              "      <td>0.784953</td>\n",
              "      <td>0.784838</td>\n",
              "      <td>0.785803</td>\n",
              "      <td>0.786982</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.363700</td>\n",
              "      <td>0.500575</td>\n",
              "      <td>0.778436</td>\n",
              "      <td>0.777311</td>\n",
              "      <td>0.777461</td>\n",
              "      <td>0.777180</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='27' max='27' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [27/27 00:00]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation Set Results:\n",
            "eval_loss: 0.4878\n",
            "eval_accuracy: 0.7850\n",
            "eval_f1: 0.7848\n",
            "eval_precision: 0.7858\n",
            "eval_recall: 0.7870\n",
            "eval_runtime: 0.8184\n",
            "eval_samples_per_second: 2062.5850\n",
            "eval_steps_per_second: 32.9920\n",
            "epoch: 3.9889\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here I made some dictionaries with the best paramaters found from the optuna search for fast rerunning of the code but result metrics may vary from original optuna search results due to randomness."
      ],
      "metadata": {
        "id": "yxryDSNtvNrK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Best hyperparameters found for each model to run code without having to search for hyperparameters again\n",
        "xlm_best_params = {\n",
        "    \"learning_rate\": 3.5377e-5,\n",
        "    \"weight_decay\": 0.01,\n",
        "    \"per_device_train_batch_size\": 32,\n",
        "    \"warmup_steps\": 100,\n",
        "    \"num_train_epochs\": 3,\n",
        "    \"adam_beta1\": 0.8814,\n",
        "    \"adam_beta2\": 0.9763,\n",
        "    \"adam_epsilon\": 1.279e-8,\n",
        "    \"gradient_accumulation_steps\": 1,\n",
        "    \"lr_scheduler_type\": \"polynomial\",\n",
        "    \"label_smoothing_factor\": 0.1538\n",
        "}\n",
        "\n",
        "distil_best_params = {\n",
        "    \"learning_rate\": 2.1513e-5,\n",
        "    \"weight_decay\": 0.0,\n",
        "    \"per_device_train_batch_size\": 8,\n",
        "    \"warmup_steps\": 250,\n",
        "    \"num_train_epochs\": 4,\n",
        "    \"adam_beta1\": 0.9172,\n",
        "    \"adam_beta2\": 0.9604,\n",
        "    \"adam_epsilon\": 4.4486e-8,\n",
        "    \"gradient_accumulation_steps\": 1,\n",
        "    \"lr_scheduler_type\": \"cosine\",\n",
        "    \"label_smoothing_factor\": 0.0297\n",
        "}\n",
        "\n",
        "indic_best_params = {\n",
        "    \"learning_rate\": 4.1161e-5,\n",
        "    \"weight_decay\": 0.0,\n",
        "    \"per_device_train_batch_size\": 8,\n",
        "    \"warmup_steps\": 0,\n",
        "    \"num_train_epochs\": 4,\n",
        "    \"adam_beta1\": 0.8600,\n",
        "    \"adam_beta2\": 0.9667,\n",
        "    \"adam_epsilon\": 1.5734e-8,\n",
        "    \"gradient_accumulation_steps\": 2,\n",
        "    \"lr_scheduler_type\": \"cosine_with_restarts\",\n",
        "    \"label_smoothing_factor\": 0.1517\n",
        "}\n",
        "\n",
        "run_optuna_search(\"xlm-roberta-base\", raw_train_ds, raw_val_ds, le.classes_.tolist(), predefined_params=xlm_best_params)\n",
        "run_optuna_search(\"distilbert-base-multilingual-cased\", raw_train_ds, raw_val_ds, le.classes_.tolist(), predefined_params=distil_best_params)\n",
        "run_optuna_search(\"ai4bharat/indic-bert\", raw_train_ds, raw_val_ds, le.classes_.tolist(), predefined_params=indic_best_params)"
      ],
      "metadata": {
        "id": "qczhasqfvMDP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing"
      ],
      "metadata": {
        "id": "cn4S_bDVLUkc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate Fine-Tuned Models on Test Set\n",
        "\n",
        "This function evaluates the performance of a fine-tuned model on the test set. It performs the following steps:\n",
        "\n",
        "- **Model and Tokeniser Loading**: Loads the best checkpoint saved after Optuna tuning.\n",
        "- **Tokenisation**: Applies padding and truncation to the test set to match model input requirements.\n",
        "- **Evaluation Configuration**: Sets up evaluation-only arguments using HuggingFace `TrainingArguments`.\n",
        "- **Prediction and Metric Calculation**:\n",
        "  - Evaluates the model using the `WeightedTrainer` to ensure consistency.\n",
        "  - Computes and prints key performance metrics using `classification_report`.\n",
        "  - Generates a **confusion matrix** to visually assess misclassifications across classes."
      ],
      "metadata": {
        "id": "tF9ly-WRUflD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining function for testing of each model\n",
        "def evaluate_on_test_set(model_name, raw_test_ds, label_list):\n",
        "    \"\"\"\n",
        "    Evaluates the fine-tuned HuggingFace models on the test set.\n",
        "\n",
        "    Loads the model, tokenizes the test set, computes evaluation metrics,\n",
        "    prints classification report, and plots a confusion matrix.\n",
        "    \"\"\"\n",
        "    # Load trained model and tokenizer from disk\n",
        "    tokenizer = AutoTokenizer.from_pretrained(f\"{model_name}_best\", local_files_only=True)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(f\"{model_name}_best\", local_files_only=True)\n",
        "\n",
        "    # Tokenize test set\n",
        "    def tokenize(batch):\n",
        "        return tokenizer(batch[\"text\"], padding=True, truncation=True)\n",
        "\n",
        "    test_ds = raw_test_ds.map(tokenize, batched=True)\n",
        "\n",
        "    # Define evaluation-only training args\n",
        "    eval_args = TrainingArguments(\n",
        "        output_dir=f\"{model_name}_test_results\",\n",
        "        per_device_eval_batch_size=64,\n",
        "        report_to=\"none\"\n",
        "    )\n",
        "\n",
        "    # Create trainer for evaluation\n",
        "    trainer = WeightedTrainer(\n",
        "        model=model,\n",
        "        args=eval_args,\n",
        "        tokenizer=tokenizer,\n",
        "        compute_metrics=compute_metrics\n",
        "    )\n",
        "\n",
        "    print(f\"\\nEvaluating {model_name} on test set...\")\n",
        "    results = trainer.evaluate(test_ds)\n",
        "\n",
        "    # Generate predictions\n",
        "    preds = trainer.predict(test_ds)\n",
        "    pred_labels = preds.predictions.argmax(-1)\n",
        "    true_labels = preds.label_ids\n",
        "\n",
        "    # Print classification metrics\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(true_labels, pred_labels, target_names=label_list))\n",
        "\n",
        "    # Plot and show confusion matrix\n",
        "    cm = confusion_matrix(true_labels, pred_labels)\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=label_list)\n",
        "    disp.plot(cmap=plt.cm.Blues, values_format='d')\n",
        "    plt.title(f\"Confusion Matrix for {model_name}\")\n",
        "    plt.show()\n",
        "\n",
        "    return {\n",
        "        \"metrics\": results,\n",
        "        \"predictions\": pred_labels,\n",
        "        \"true_labels\": true_labels\n",
        "    }"
      ],
      "metadata": {
        "id": "F375XxSGEXqc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run Evaluation Across All Fine-Tuned Models\n",
        "\n",
        "This runs final evaluation for each fine-tuned model on the **test set**. It does the following:\n",
        "\n",
        "- Iterates over all model names defined in a list.\n",
        "- Calls the `evaluate_on_test_set` function to load each model, run predictions, and generate evaluation metrics.\n",
        "- Stores all evaluation results (including metrics, predictions, and true labels) in a list for comparative analysis.\n"
      ],
      "metadata": {
        "id": "zz1cB41bUpMr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run evaluation for all models on the test set\n",
        "test_results_all = []\n",
        "for model_name in models:\n",
        "    result = evaluate_on_test_set(model_name, raw_test_ds, le.classes_.tolist())\n",
        "    test_results_all.append(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "9dde239a7515471a93b93b9a580ac9c5",
            "9acb2f82b60846d8bff044c7893d8809",
            "efeb561e951f4aeab7d2f041e33a1ec0",
            "aed27a59e77e469bb444f0a27f1535ec",
            "c408aa81b5764fbc9f1b6e16c72fb448",
            "b0a1f48bcab342399e218f9dfa173249",
            "cf4b1826400a445e8a33f89164d2d653",
            "2899d221428a436981376c723b8487c2",
            "96ab344a737a47abb747ddf757a3ffb1",
            "7c501cccaa624a2fa94105b9995c9e01",
            "b4d0e187d33e42b892f1fc7d44b9cac6",
            "9fead0a95ed74611872b697c28d88bfc",
            "a346bf17b3fe49c598ee46c88811a3c8",
            "40fdd61cf0f5487786dd19d3d9e0e117",
            "0215ff33f3b74e28b418da2fe3bf5520",
            "a3259a9775d5423895f0149c04b1b4ad",
            "6f9bf9b1bbd14e7eba16d23602b45f68",
            "8697a51cc2b44d9abbed16d19e8aa63c",
            "da73d3e2918b43a58a4404a80e1c39ae",
            "1617c4d458d64b44a30616b04c2e65a9",
            "920ecceade15462c84bb6c19773cb056",
            "d26fd431ae324527883afb923f715861",
            "36d66960fdfe433192080abf64e55b1d",
            "50461cdb92f447a7bbde9d7b641b3d0c",
            "e01dcd02ec8e439797681deecf2cd041",
            "c5292c4224dd4c1ab85fe50c1fd19935",
            "745c583c800940e4b6664d66baa49ca7",
            "84ad0bc703e24b03b19286523c6bb92a",
            "ca5a9561bdfc45639e7871236367ede3",
            "08d0f8a7787740fa9fc6e8340c55f18b",
            "6d03d33892ae4dd88f4e1aea80624afc",
            "7c87cb9dc6544246b4c28bfc9c51d81d",
            "4584a0e176f94ee68a5bfd10e2e6e287"
          ]
        },
        "id": "1w8uXBUfFNsa",
        "outputId": "1a7ca104-63be-434b-85c2-a40046082966"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/1688 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9dde239a7515471a93b93b9a580ac9c5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-24-6c3ee72e9cc0>:21: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedTrainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = WeightedTrainer(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating xlm-roberta-base on test set...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         HOF       0.76      0.76      0.76       787\n",
            "         NOT       0.79      0.79      0.79       901\n",
            "\n",
            "    accuracy                           0.78      1688\n",
            "   macro avg       0.77      0.77      0.77      1688\n",
            "weighted avg       0.78      0.78      0.78      1688\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/1688 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9fead0a95ed74611872b697c28d88bfc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-24-6c3ee72e9cc0>:21: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedTrainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = WeightedTrainer(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating distilbert-base-multilingual-cased on test set...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         HOF       0.72      0.78      0.75       787\n",
            "         NOT       0.80      0.74      0.77       901\n",
            "\n",
            "    accuracy                           0.76      1688\n",
            "   macro avg       0.76      0.76      0.76      1688\n",
            "weighted avg       0.76      0.76      0.76      1688\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/1688 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "36d66960fdfe433192080abf64e55b1d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating ai4bharat/indic-bert on test set...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-24-6c3ee72e9cc0>:21: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedTrainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = WeightedTrainer(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         HOF       0.73      0.78      0.75       787\n",
            "         NOT       0.79      0.75      0.77       901\n",
            "\n",
            "    accuracy                           0.76      1688\n",
            "   macro avg       0.76      0.76      0.76      1688\n",
            "weighted avg       0.76      0.76      0.76      1688\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visual Model Comparison"
      ],
      "metadata": {
        "id": "Jre35Yc-hkmY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This graph was created using Excel, hence why the code is absent."
      ],
      "metadata": {
        "id": "_Am4syTOS6w8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://i.imgur.com/WoPFmgm.png\" width=\"1500\">\n",
        "\n",
        "*Figure 1. Comparison of three multilingual transformer models (XLM-RoBERTa, DistilBERT, IndicBERT) on validation F1, test F1, and overall accuracy. XLM-RoBERTa shows the highest validation performance and maintains strong generalisation on the test set.*\n"
      ],
      "metadata": {
        "id": "IHMXdOx7P2_9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Potential Improvements"
      ],
      "metadata": {
        "id": "C8yb3kjxTbBm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing Enhancements\n",
        "\n",
        "- **Emoji & Emoticon Handling**:  \n",
        "  The current pipeline removes all non-alphanumeric characters, including emojis, which often carry sentiment cues. Integrating an emoji lexicon or translating emojis to textual equivalents could help preserve emotional context.\n",
        "\n",
        "- **Slang Normalisation**:  \n",
        "  Incorporating a Hinglish or slang normaliser, especially for code-mixed Hindi-English tweets, would improve consistency in informal expressions.\n",
        "\n",
        "- **Spelling Correction**:  \n",
        "  Tweets often contain typographical or phonetic spelling errors. Adding a lightweight spell-correction mechanism could reduce vocabulary fragmentation.\n",
        "\n",
        "- **Stopword Retention for Transformers**:  \n",
        "  Since transformer models rely on contextual attention, removing stopwords may be unnecessary or even harmful. Retaining them might preserve important sentence structure cues.\n",
        "\n",
        "- **Language Detection Refinement**:  \n",
        "  The current use of Devanagari script detection can be extended using a full language identification model (e.g., `langdetect`) to better distinguish code-switched or Romanised Hindi tweets.\n",
        "\n",
        "---\n",
        "\n",
        "### Modelling Enhancements\n",
        "\n",
        "- **Data Augmentation Diversification**:  \n",
        "  Beyond back-translation, using transformer-based paraphrasing models like PEGASUS or T5 can further enrich the training corpus with semantically diverse samples.\n",
        "\n",
        "- **Model Ensembling**:  \n",
        "  Combining predictions from multiple models (e.g., XLM-RoBERTa + IndicBERT) using majority voting or weighted averaging could increase robustness across language and style variance.\n",
        "\n",
        "- **Domain-Adaptive Pretraining**:  \n",
        "  Fine-tuning on a larger corpus of Hindi-English social media data before task-specific training can help the model better understand domain-specific syntax and slang.\n",
        "\n",
        "- **Class-Specific Loss Functions**:  \n",
        "  Techniques like **focal loss** or enhanced **label smoothing** can further mitigate class imbalance and penalise misclassifications of underrepresented labels.\n",
        "\n",
        "- **Token-Level Feature Integration**:  \n",
        "  Including additional cues such as hashtag density, punctuation frequency, or uppercase ratio (via multi-modal input or concatenated embeddings) could help capture latent patterns missed by text alone.\n",
        "\n",
        "---\n",
        "\n",
        "Implementing these improvements could enhance generalisation, robustness to noisy or informal text, and classification performance across diverse linguistic inputs.\n"
      ],
      "metadata": {
        "id": "Bqj5Ad8aTe9d"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}